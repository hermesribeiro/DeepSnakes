{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWCiggNd_h_f"
   },
   "source": [
    "# Deep Snakes\n",
    "by Hermes, Channel and Jeanne. Rio de Janeiro, Brazil, 2018.\n",
    "\n",
    "## Welcome to project Deep Snakes\n",
    "\n",
    "In this series, we will try different machine learning approaches to identify two classes of snakes; python snakes (family pythonidae) and rattlesnakes (genera crotalus and sistrurus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D66nXmaf_m_Q"
   },
   "source": [
    "## Convolutional Neural Networks\n",
    "In the previous notebook, we developed and tuned a shallow neural network, finishing with an error analysis, showing that most errors had no apparend causes. In this notebook, we will try to address this issues by building a deeper architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2YrSUAu_qty"
   },
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 704,
     "status": "error",
     "timestamp": 1526604174353,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "m2ue5xaa_r2X",
    "outputId": "7c9dbedd-13f0-4a70-a0f3-f01cdc88680c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import tensorflow as tf\n",
    "from supporting_functions import snake_data\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1526579723555,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "Mnwm5S0DPXbz",
    "outputId": "819dac35-8908-4753-a48f-d395d267af76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "Images train: (484, 128, 128, 3)\n",
      "Labels train: (484,)\n",
      "Images dev: (121, 128, 128, 3)\n",
      "Labels dev: (121,)\n"
     ]
    }
   ],
   "source": [
    "images_train_orig, labels_train_orig, images_dev_orig, labels_dev_orig = snake_data()\n",
    "print(\"Shapes\")\n",
    "print(\"Images train:\", images_train_orig.shape)\n",
    "print(\"Labels train:\", labels_train_orig.shape)\n",
    "print(\"Images dev:\", images_dev_orig.shape)\n",
    "print(\"Labels dev:\", labels_dev_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bZDVM6B_yUV"
   },
   "source": [
    "This time we don't need to linearize the dataset, we will only normalize it and set the vector dimensions straight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 636,
     "status": "ok",
     "timestamp": 1526580013463,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "72jzB6FbLf6a",
    "outputId": "69e57e51-87e0-4ead-dc2a-3a7dff47575c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "Images train: (484, 128, 128, 3)\n",
      "Labels train: (484, 1)\n",
      "Images dev: (121, 128, 128, 3)\n",
      "Labels dev: (121, 1)\n"
     ]
    }
   ],
   "source": [
    "images_train = images_train_orig/255\n",
    "labels_train = (labels_train_orig).reshape(-1,1)\n",
    "images_dev = images_dev_orig/255\n",
    "labels_dev = (labels_dev_orig).reshape(-1,1)\n",
    "imsize = images_train.shape[1:3] # image_shape\n",
    "dsize = images_train.shape[0] # amount of images\n",
    "dsize_dev = images_dev.shape[0] # amount of dev images\n",
    "print(\"Shapes\")\n",
    "print(\"Images train:\", images_train.shape)\n",
    "print(\"Labels train:\", labels_train.shape)\n",
    "print(\"Images dev:\", images_dev.shape)\n",
    "print(\"Labels dev:\", labels_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RPDWqb0Q8jx"
   },
   "source": [
    "## Building the model\n",
    "### Convolutional Neural Networks\n",
    "Convolutional Neural Networks (CNN) are made by (as the name suggests), convolutional layers.  The first operation is convolving the image with filters in order to generate a 3D affine transformation matrix, followed by an activation function to compute an activation matrix. Optinionally,  these layers ends with a pooling operation to reduce the matrix size along the first two axes. Using a sequence of convolutional layers  has the incredible advantage of exponencially increasing the NN's expressive power, which brings about two major advantages: less parameters and better accuracy, compared to a shalow (but wide) fully connected model.\n",
    "\n",
    "Typically, as we go deeper into the model architecture, the height and width of the activation matrix decreases while the lenght increases. The last layer is usually a 1D flattened layer. So, we will start with two layers, followed by a fully connected layer and increase the model's complexity as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Te0etsI0Tw73"
   },
   "outputs": [],
   "source": [
    "def model_builder(cparams,hparams):\n",
    "    # Builds the models placeholders for inputs and outputs, parameter variables and feed forward computation graph.\n",
    "    # Argument: cparams - constant params, meaning, input/output dimensions.\n",
    "    #           hparams - hyperparameters, meaning, hyperparameters. All of them.\n",
    "    # Returns: tf objects regarding X, Y, logits and costs. Will handle weights on the next fcn.\n",
    "    #\n",
    "    # getting win, hin, nout (inputs width and height, output # of classes)\n",
    "    win = cparams[\"win\"]\n",
    "    hin = cparams[\"hin\"]\n",
    "    nout = cparams[\"nout\"]\n",
    "    # getting f1,c1,f2,c2. Stride on filters will be 1 for what it's worth\n",
    "    f1 = hparams[\"f1\"] # filter size of conv 1\n",
    "    c1 = hparams[\"c1\"] # num channels(filters) conv 1\n",
    "    f2 = hparams[\"f2\"] # there are 2 typoe of people in the world\n",
    "    c2 = hparams[\"c2\"] # those who can extrapolate from incomplete information.\n",
    "    # getting p1, sp2 (pooling layers sizes, will consider size=stride)\n",
    "    p1 = hparams[\"p1\"]\n",
    "    p2 = hparams[\"p2\"]\n",
    "    # planting the seed\n",
    "    tf.set_random_seed(2001) # a space odissey\n",
    "    # creating placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32,shape=[None,win,hin,3],name=\"X\")\n",
    "    Y = tf.placeholder(dtype=tf.float32,shape=[None,nout],name=\"Y\")\n",
    "    # first layer\n",
    "    conv_1 = tf.layers.conv2d(X,kernel_size=f1,filters=c1,name=\"conv_1\",padding=\"SAME\")\n",
    "    act_1 = tf.nn.relu(conv_1)\n",
    "    pool_1 = tf.layers.max_pooling2d(act_1,pool_size=p1,strides=p1)\n",
    "    # second layer\n",
    "    conv_2 = tf.layers.conv2d(pool_1,kernel_size=f2,filters=c2,name=\"conv_2\",padding=\"SAME\")\n",
    "    act_2 = tf.nn.relu(conv_2)\n",
    "    pool_2 = tf.layers.max_pooling2d(act_2,pool_size=p2,strides=p2)\n",
    "    # flatten\n",
    "    flat = tf.layers.flatten(pool_2)\n",
    "    # logits (activation is already None, brilliant!)\n",
    "    logits = tf.layers.dense(flat,nout,name=\"logits\")\n",
    "    # cost\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    # If we need the parameter values later, we will return it. For now, let's do all things within TemsorFlow\n",
    "    return X, Y,logits, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training the model. For minibatch split, we will try a different approach. Previously, when the number of samples was not divisible by the (mini)batch size, we split the data into n chunks, where n-1 were equally sized and the last one contained the remaining data. Now, we will use numpy `array_split`:\n",
    ">For an array of length l that should be split into n sections, it returns l % n sub-arrays of size l//n + 1 and the rest of size l//n.\n",
    "\n",
    "Therefore, `batch_size` will no longer be constant, but bounded below by its value and upper bounded by `batch_size+1`. This may bring about some computational efficiency issue, but will minimize some effect cused by the last mini_batch being smaller than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_trainer(hparams, \n",
    "                  images_train = images_train, # Problematic if used incorrectly.\n",
    "                  labels_train = labels_train, # watch out!\n",
    "                  images_dev = images_dev, \n",
    "                  labels_dev = labels_dev, \n",
    "                  print_every=5):\n",
    "    # Since CNN has a ton of hyperparameters, there will be no default value.\n",
    "    tf.set_random_seed(42) # the answer to life the universe and everything\n",
    "    np.random.seed(42) # Is this a valid turing test question tho? \n",
    "    # number of train and dev samples\n",
    "    n_train = images_train.shape[0]\n",
    "    n_dev = images_dev.shape[0]\n",
    "    # Retrieving hyperparameters\n",
    "    epochs = hparams[\"epochs\"]\n",
    "    learning_rate = hparams[\"learning_rate\"]\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    num_batches = n_train//batch_size\n",
    "    # determining cparams\n",
    "    cparams = {\"hin\":images_train.shape[1], \n",
    "               \"win\":images_train.shape[2], \n",
    "               \"nout\":labels_train.shape[1]}\n",
    "    # building the model\n",
    "    X,Y,logits,cost = model_builder(cparams,hparams)\n",
    "    # bulding the floppy disc icon\n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    # defining the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    # I'm sessy and I know it!\n",
    "    with tf.Session() as sess: \n",
    "        # Performance variables container\n",
    "        costs_train = []\n",
    "        costs_dev = []\n",
    "        cost_dev_best = float(\"inf\")\n",
    "        # indices\n",
    "        indices = np.arange(n_train) \n",
    "        # note to self: don't use n or m, use explicit names. Almost screwed everything up.\n",
    "        # Starting the global variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)        \n",
    "        for epoch in range(epochs+1): # epochs+1 to facilitate print_every\n",
    "            # everyday I'm shufflin\n",
    "            indices = np.random.permutation(indices)\n",
    "            splits = np.array_split(indices,num_batches)\n",
    "            cost_train = 0.\n",
    "            # running mini batches\n",
    "            for split in splits:\n",
    "                _,temp_cost = sess.run([optimizer,cost],feed_dict={X:images_train[split],Y:labels_train[split]})\n",
    "                cost_train += temp_cost/num_batches\n",
    "            # storing train cost\n",
    "            costs_train.append(cost_train)\n",
    "            # computing dev cost\n",
    "            dev_splits = np.array_split(np.arange(images_dev.shape[0]),images_dev.shape[0]/batch_size)\n",
    "            cost_dev=0.\n",
    "            # will divide into samples to minimize mamory usage\n",
    "            samples = np.array_split(np.arange(n_dev),n_dev/16)\n",
    "            for sample in samples:\n",
    "                cost_dev += sess.run(cost, feed_dict={X:images_dev[sample],Y:labels_dev[sample]})/(n_dev/16)\n",
    "            costs_dev.append(cost_dev)\n",
    "            # saver\n",
    "            if cost_dev < cost_dev_best:\n",
    "                saver.save(sess,\"./save_best\")\n",
    "                cost_dev_best = cost_dev\n",
    "            # print output\n",
    "            if not epoch%print_every:\n",
    "                print(\"Epoch {}: train cost = {:6.4f}, dev cost = {:6.4f}\".format(epoch,cost_train,cost_dev))\n",
    "        return costs_train, costs_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Lola run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train cost = 0.6937, dev cost = 0.6451\n",
      "Epoch 1: train cost = 0.6919, dev cost = 0.6436\n",
      "Epoch 2: train cost = 0.6907, dev cost = 0.6424\n",
      "Epoch 3: train cost = 0.6893, dev cost = 0.6417\n",
      "Epoch 4: train cost = 0.6884, dev cost = 0.6411\n",
      "Epoch 5: train cost = 0.6874, dev cost = 0.6404\n",
      "Epoch 6: train cost = 0.6864, dev cost = 0.6394\n",
      "Epoch 7: train cost = 0.6859, dev cost = 0.6387\n",
      "Epoch 8: train cost = 0.6849, dev cost = 0.6383\n",
      "Epoch 9: train cost = 0.6837, dev cost = 0.6378\n",
      "Epoch 10: train cost = 0.6830, dev cost = 0.6372\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "# f1 = filter 1 size, c1 = number of channels (filters), p1 = size of pooling\n",
    "hparams = {\"f1\":5, \"c1\":10, \"p1\":4,\n",
    "           \"f2\":3, \"c2\":20, \"p2\":2,\n",
    "           \"epochs\":10,\n",
    "           \"learning_rate\":1.e-5, \n",
    "           \"batch_size\":4}\n",
    "costs_train, costs_dev = model_trainer(hparams, print_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using saved values to predict and calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metrics(hparams,\n",
    "            images_train = images_train, # same as before: tread lightly\n",
    "            labels_train = labels_train,\n",
    "            images_dev = images_dev,\n",
    "            labels_dev = labels_dev):\n",
    "    # Function to calculate the predicted values and the accuracies on the dataset.\n",
    "    # determining cparams\n",
    "    cparams = {\"hin\":images_train.shape[1], \n",
    "               \"win\":images_train.shape[2], \n",
    "               \"nout\":labels_train.shape[1]}\n",
    "    # This function uses a tensorflow graph instead of numpy\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        # building the same model used in prediction\n",
    "        X,Y,logits,cost = model_builder(cparams,hparams)\n",
    "        # loading the weights and biases that rendered the smallest dev error\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,\"./save_best\")\n",
    "        # Building the graphs to calculate pred and accuracy\n",
    "        pred = tf.sigmoid(logits)\n",
    "        n_train = images_train.shape[0]\n",
    "        n_dev = images_dev.shape[0]\n",
    "        # iterating over the train set\n",
    "        cost_train = 0.\n",
    "        splits = np.array_split(np.arange(n_train),n_train//16)\n",
    "        pred_train = np.zeros([n_train,1])\n",
    "        for split in splits:\n",
    "            # calculating the train cost again\n",
    "            cost_train += sess.run(cost, feed_dict={X:images_train[split],Y:labels_train[split]})/(n_train/16)\n",
    "            pred_train[split] = sess.run(pred, feed_dict={X:images_train[split],Y:labels_train[split]})\n",
    "        # iterating on the dev set\n",
    "        cost_dev = 0.\n",
    "        pred_dev = np.zeros([n_dev,1])\n",
    "        splits = np.array_split(np.arange(n_dev),n_dev//16)\n",
    "        for split in splits:\n",
    "            # calculating the dev cost again.\n",
    "            cost_dev += sess.run(cost, feed_dict={X:images_dev[split],Y:labels_dev[split]})/(n_dev/16)\n",
    "            pred_dev[split] = sess.run(pred,feed_dict={X:images_dev[split],Y:labels_dev[split]})\n",
    "    # using numpy for less verbosity\n",
    "    acc_train = np.mean(np.equal(np.greater(pred_train,0.5).astype(int),labels_train))\n",
    "    acc_dev = np.mean(np.equal(np.greater(pred_dev,0.5).astype(int),labels_dev))\n",
    "    return cost_train,cost_dev,pred_train,pred_dev,acc_train,acc_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save_best\n",
      "At dev cost = 0.6372: Accuracy train = 0.6736, Accuracy dev = 0.5702\n"
     ]
    }
   ],
   "source": [
    "cost_train,cost_dev,pred_train,pred_dev,acc_train,acc_dev = metrics(hparams)\n",
    "print(\"At dev cost = {:6.4f}: Accuracy train = {:6.4f}, Accuracy dev = {:6.4f}\".format(cost_dev,acc_train,acc_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8HeV97/HPT0fL0S5Zizd5A9ushgBiKwkhoWBTEmia\npAYCpGl7gd6QQGiSa+59pSl5tffFTbpAAnFKCUlIUpKUkJimxJjQsKRNwTY4BtuAjTG2vMmWbcna\nt9/9Y0bS0WaNbY2OLH3fr9d5nZln5sx5xou+euZ55hlzd0REREaSke4KiIjIiUGBISIikSgwREQk\nEgWGiIhEosAQEZFIFBgiIhKJAkNERCJRYIiISCQKDBERiSQz3RUYTeXl5T537tx0V0NE5ISxdu3a\n/e5eEWXfCRUYc+fOZc2aNemuhojICcPM3o26ry5JiYhIJAoMERGJRIEhIiKRTKg+DBE5cXV0dFBT\nU0Nra2u6qzIhJZNJqqqqyMrKOuZjKDBEZFyoqamhsLCQuXPnYmbprs6E4u7U1dVRU1PDvHnzjvk4\nuiQlIuNCa2srZWVlCosYmBllZWXH3XpTYIjIuKGwiM9o/NnqkhTw9Wc3M604yVlVxcyvKCAzoRwV\nERlo0gdGe2c33/7NO9S3dACQm5XgjBlFnFVVwllVxSyqKmZeWT4ZGfrNR2Qiq6ur4/LLLwdgz549\nJBIJKiqCG6BffvllsrOzRzzGpz71KZYtW8Ypp5wSa13Txdw93XUYNdXV1X4sd3p3dzvv1DWxvuYQ\n62vqea2mntd31dPa0Q1AYU4mZ84s5qyq4t4gqSrNVfNZZBRt2rSJ0047Ld3VAOCv//qvKSgo4POf\n/3y/cnfH3cnIGLurEJ2dnWRmZg67Ppyh6jrUn7GZrXX36ih1mfQtDICMDOPkigJOrijgI+dUAdDZ\n1c2WfY2s31HP+p2HeK2mnu/85zbau4IQKc3LYlFVCWelBMnUohyFiMgEs2XLFq655hrOOeccXn31\nVZ555hnuueceXnnlFVpaWli6dCl/9Vd/BcB73/teHnjgAc4880zKy8u57bbb+OUvf0leXh4rVqyg\nsrKy37EbGxu5/fbb2bhxIx0dHXzlK1/hwx/+MA8//DC/+MUvqK+vJyMjg7vvvpu/+Zu/oaCggLff\nfptNmzbx1a9+lUcffRSAW2+9lc985jND1nXmzJmj9mehwBhGZiKDU6cVceq0Iv74/FkAtHV28dae\nRtbvPBQGST3Ln3+bru6glVZRmMPZVcUsmtl3Oau8ICedpyFyQrrn3zawcVfDqB7z9BlFfPnDZxzT\nZ9944w0effRRqquDX8TvvfdepkyZQmdnJx/4wAf42Mc+xumnn97vM/X19bz//e/n3nvv5a677uKR\nRx5h2bJl/fb5yle+wpIlS/jud7/LwYMHufDCC7niiisAePXVV1m3bh2lpaX86le/Ys2aNWzcuJHZ\ns2fz0ksv8cMf/pDVq1fT2dnJBRdcwGWXXUZubu6guo4mBcZRyMlMsCgMgk9cGJS1dnSxYVcDr4WX\ns9bvrOfZN2rpudI3sySXRTOLOWtWMWfNLGHRzGKK8479xhkRGXsnn3xyvx/Ajz32GN/+9rfp7Oxk\n165dbNy4cVBg5ObmctVVVwFw3nnn8eKLLw467qpVq/jlL3/JvffeCwRDi7dv3w7AlVdeSWlpae++\nF198MbNnzwbgN7/5DR/96EfJzc0F4A//8A958cUXufLKKwfVdTQpMI5TMivBeXNKOW9O319sY1sn\nr+8M+kLW76xnfc0hVm7Y07t9blkei6pKOLuqmNOnFzGnPJ9pRUkS6lgXATjmlkBc8vPze5c3b97M\n/fffz8svv0xJSQk33njjkPc3pHaSJxIJOjs7B+3j7vz85z/n5JNP7lf+wgsv9PvOgXWIWtfRpsCI\nQUFOJhedVMZFJ5X1lh1qbue1nfW9neprtx3g3363q3d7diKDqtJcZk3JY/aUPOaU5TGr5700j/wc\n/VWJjAcNDQ0UFhZSVFTE7t27efrpp1myZMkxHWvx4sV84xvf4L777gOCy1DnnHPOiJ973/vex623\n3soXvvAFurq6WLFiBT/+8Y+PqQ5HQz+FxkhJXjbvW1DB+xb0Padk3+E23txzmO0HmsNXE9sPNPPK\n9oMcbu3/20h5QTazwzCZXZbfGyqzp+RRUZCjYb8iY+Tcc8/l9NNP59RTT2XOnDlccsklx3ysL3/5\ny9x5550sWrSI7u5u5s+fz4oVK0b83AUXXMD111/P+eefD8Bf/MVfsGjRIrZs2XLMdYlCw2rHqUPN\n7Ww/0My7dWGY9LwfaGZ3fQvdKX9tOZkZKWGS1y9MqkrzSGYl0nciIhGNp2G1E9W4HlZrZkuA+4EE\n8LC73zvEPpcB9wFZwH53f39YfgfwPwAD/tnd74uzruNNSV42JXnZnFVVMmhbe2c3Ow+18G5dEzvC\nEOkJlt9uraO5vavf/tOKkv3CpGf55IoCinPVAS8i0cQWGGaWAB4ErgBqgNVm9qS7b0zZpwT4JrDE\n3bebWWVYfiZBWFwAtAMrzewX7h5ve+sEkZ2ZwbzyfOaVD+7ccnfqmtp5t665X5jsONDMi5v3sbeh\nrd/+lYU5LJxayPzKAhZOLWTB1AIWVBZQkjfyXa0iMrnE2cK4ANji7lsBzOxHwLXAxpR9bgCecPft\nAO5eG5afBrzk7s3hZ58H/gj4aoz1nRDMjPKCHMoLcvqN3OrR2tFFzcFmtu1vZsu+RjbvbWRz7WF+\nsmZHv5ZJRWEOC8IQmV9Z0Ltcmq8gEZms4gyMmcCOlPUa4MIB+ywEsszsOaAQuN/dHwVeB/7WzMqA\nFuAPgInROZFmyawE8ysLmV9ZyO8ztbe8u9vZVd/SGyCb9zbyVm0j/7pmB00pQVJekM2Cyr6WyIKp\nhSyoLKBMNyiKTHjpHiWVCZwHXA7kAr81s/92901m9v+AVUATsA7oGuoAZnYLcAvQe1OLHL2MDKOq\nNOgk/8CpfdMXuDu761t5a+9httQ28tbew2yubeRnr+zkcFvfSK6y/OygJTK1oN8lrrL8bE2XIjJB\nxBkYO4FZKetVYVmqGqDO3ZuAJjN7ATgbeMvdvw18G8DM/m+47yDu/hDwEASjpEb1DAQzY0ZJLjNK\ncrnslP5BsqehNWyRNLI5DJIV63b1GxJcmpfVr0WycGohc8vzqSjMIUvTyIucUOIMjNXAAjObRxAU\n1xH0WaRaATxgZplANsElq38EMLNKd681s9kE/RcXxVhXOUpmxvTiXKYX53Lpwr57S9yd2sNtwSWt\nMES21B7mF+t3904hH3w+aJVUFiapLMphamGSqUU5VBYlmVqUpLIwh6lFScoLsvV8EhkziUSCRYsW\n0dHRQWZmJjfffDOf+9znxnR22vEstsBw904zux14mmBY7SPuvsHMbgu3fyu89LQSWA90Ewy9fT08\nxE/DPowO4NPufiiuusroMTOmhj/037ugvLfc3dnX2MaWvY28e6CZ2oY29h5upbahlb0NbWzc1cD+\nxrZ+95cEx4PygpwgTHpCpTAZfkdfWVlBjqZWkeOWm5vLunXrAKitreWGG26goaGBe+65J/bvHmo6\n8q6uLhKJke+jirrf8dKNezJudHU7dY1t7G1oY29DK3sPB2FS29BK7eGwrKGNuqY2Bv6zzbBgZFfQ\nOunfaplaFKxXFiYpy8/WXfHj1Hi4ca+goIDGxsbe9a1bt3L++eezf/9+uru7WbZsGc899xxtbW18\n+tOf5tZbb+W6667jpptu4uqrrwbgT/7kT/jQhz7Exz72sX7H/trXvsZPfvIT2tra+MhHPsI999zD\ntm3bWLx4MRdeeCFr167lqaee4owzzuDWW2/lV7/6FQ8++CBtbW18/vOfp7Ozk/PPP5/ly5eTk5PD\n3LlzWbp0Kc888wxf/OIXue6660Y8v3F9457I0UhkGJVFSSqLkiyieNj9Orq62R8GS21DK3sPh+9h\noNQcbObV7Qepa2of9NnsRAZTi3PCy2nJlPckM0pymVacVEf9ePDLZbDntdE95rRFcNWge4eP6KST\nTqKrq4va2lpWrFhBcXExq1evpq2tjUsuuYQrr7ySpUuX8pOf/ISrr76a9vZ2nn32WZYvX97vOKtW\nrWLz5s28/PLLuDvXXHMNL7zwArNnz2bz5s1873vf46KLgqvuTU1NXHjhhfz93/89ra2tLFiwgGef\nfZaFCxdy8803s3z5cu68804AysrKeOWVV0bnzycCBYaccLISGb39J0fS3tnNvsagZVLb0Mqe+lZ2\n97wfauWV7QfZU7+bjq7+zZXsRAbTwhCZXpxkekkuM4qTTEsJlykKlUln1apVrF+/nscffxwInnex\nefNmrrrqKu644w7a2tpYuXIll156ae+046mfXbVqVe/Ego2NjWzevJnZs2czZ86c3rCAoB/lox/9\nKABvvvkm8+bNY+HChQB88pOf5MEHH+wNjKVLl8Z+3qkUGDJhZWdmMLMkl5klwwdLd3dwZ/zu+hZ2\nHWplT30Lu+tbw1cLq7cdZG/DbjoHdK5kZ2b0tUyKg5bJ9JJcphclmV4SlJXkZSlUjtVRtgTisnXr\nVhKJBJWVlbg73/jGN1i8ePGg/S677DKefvppfvzjHw95acjdufvuu7n11lv7lW/btm3QdOTJZDJy\nf0ScU5kPRYEhk1pGhlFRmENFYQ5nVQ29T3e3s7+xrTdEdh1qZU9DK7sOtbCnvpWX3jnAnobW3icv\n9khmBS2haWGIzCwJWkUzSoLLX9OLkxQmNZfXeLVv3z5uu+02br/9dsyMxYsXs3z5cj74wQ+SlZXF\nW2+9xcyZM8nPz2fp0qU8/PDDrFmzhu9+97uDjrV48WK+9KUv8YlPfIKCggJ27txJVtbIf/ennHIK\n27ZtY8uWLcyfP5/vf//7vP/974/hbKNRYIiMICOlb+XsWYMng4Sgw35/Y1tviOyqD1oru+pb2X2o\nhd++XcfehtZBo8AKk5nMCENketga6ulP6Wm5ZGdqSOdYaWlp4T3veU/vsNqbbrqJu+66C4A///M/\nZ9u2bZx77rm4OxUVFfz85z8Hgqfj3XTTTVx77bX9HpzU48orr2TTpk1cfPHFQNC5/oMf/GDElkQy\nmeQ73/kOH//4x3s7vW+77bZRPuvoNEpKZIx0dnWz93Abuw8FQbLrUAu7D7Ww81BPy6WFg80d/T7T\nM6x4RtiPktpC6SkrnyDPQxkPo6QmOo2SEjlBZCZG7lNpae/qvey1KwyR3eHyW3sP89yb+2jp6D9L\nTlbCmBb2pQRB0hcs04qCvpSi3CzysxPqU5HjosAQGUdysxOcVFHASRUFQ253d+pbOoJAOdQShEtv\na6WVl985wN6G1kGd9BAMWy5KZlKUm0VRMoui3EyKe5ezKEqG6ynbi5JZvWU5mRkKnElOgSFyAjGz\n3odrnT6jaMh9urqdfYfb2FXfwt76VhpaO6hv6aChpTNluYOG1k5qGxqD9dYOWju6j/jd2YmMIEQi\nhkxwKS1JcW700WLurlCKyWh0PygwRCaYREZwiWpacfKoPtfW2cXh1s5+gdLQ0tEbKAMDp76lgx0H\nmnuXh2rVAORmJcL7WcJLZT33tITDj6eXJCnMySSZTFJXV0dZWZlCY5S5O3V1dSSTR/dvYiAFhogA\nkJOZIKcgQfkxPNvE3Wnt6O4Nl/qWDmob2tjde19L0C/zm837qT08eLRYfnaC+RV5fPKsAqbm7yQz\nw0gMeGUoRI5LMpmkqmqYseMRKTBE5LiZGbnZCXKzEyO2bDq6uqkNR4ulhsnu+ha+91ozu+pb2Xe4\nbdDneoYgBy2VvmldeqZ0mVGcS252/BPwTWYKDBEZU1kRRou1d3azt6F1UKD0rL9WUz/kXGEleVnM\nKM5lbnkec8vymVuez7zyfOaW5VNeoOlcjpcCQ0TGnezMDGZNyWPWlLxh92nt6GJvQ+ugMKk52MKm\n3YdZtWFvv36VwpxM5oRB0hMiPYFSqmlcIlFgiMgJKZmVYE5ZPnPKhp5PqaOrm50HW3inrolt+4PX\nO3XNrK+p56nXdvfrRylKZgYhEgbJvPJ85pTlMa88n5K8wXduT1YKDBGZkLISGUEAlOfDKf23tXd2\ns+NgcxAi+5t4t66ZbXVNrH33IE/+ble/562U5GUNaJXk9YZL0SSbC0yBISKTTnZmBidXFHDyEDdI\ntnV2seNAM+/sDwMlbKG8tLWOn726s9++ZfnZKa2SPCoLk5TmZzMlP4vSvGxK87Ipzs2aEFO3gAJD\nRKSfnMwE8ysLmV9ZOGhba0dXb2tk2/4mttUFLZT/3LKfn77SOuTxMgxK8rIpzctiSn5w0+WUvOxB\nwRKsB9sKk5njMmQUGCIiESWzEpwyrZBTpg0Ok5b2Luqa2jjY1MHB5nYONrdzoKmdg03tHGhu7y3f\ncaCZ9TWHONjUQXvX0HfXJzKMktysIETysinNHzlsinPjvzymwBARGQW52QmqsvOoKo22v7vT1N7F\nwaaUcGlu50BTR2/IHArLt+1v5pXthzjY1D7kHfWleVm8+ldXjvIZDabAEBFJAzOjICeTgpzMIw4f\nTuXuHG7r5FBTR9hqCQJl4MO74qLAEBE5QZhZMMljMovZZdFCZjTpUV4iIhKJAkNERCJRYIiISCQK\nDBERiSTWwDCzJWb2ppltMbNlw+xzmZmtM7MNZvZ8SvnnwrLXzewxMzu+J3+IiMhxiS0wzCwBPAhc\nBZwOXG9mpw/YpwT4JnCNu58BfDwsnwl8Fqh29zOBBHBdXHUVEZGRxdnCuADY4u5b3b0d+BFw7YB9\nbgCecPftAO5em7ItE8g1s0wgD9gVY11FRGQEcQbGTGBHynpNWJZqIVBqZs+Z2VozuxnA3XcCfwds\nB3YD9e6+Ksa6iojICNLd6Z0JnAdcDSwGvmRmC82slKA1Mg+YAeSb2Y1DHcDMbjGzNWa2Zt++fWNV\nbxGRSSfOwNgJzEpZrwrLUtUAT7t7k7vvB14AzgZ+H3jH3fe5ewfwBPB7Q32Juz/k7tXuXl1RUTHq\nJyEiIoE4A2M1sMDM5plZNkGn9ZMD9lkBvNfMMs0sD7gQ2ERwKeoiM8uz4LmJl4flIiKSJrHNJeXu\nnWZ2O/A0wSinR9x9g5ndFm7/lrtvMrOVwHqgG3jY3V8HMLPHgVeATuBV4KG46ioiIiMz97GZ5XAs\nVFdX+5o1a9JdDRGRE4aZrXX36ij7prvTW0REThAKDBERiUSBISIikSgwREQkEgWGiIhEosAQEZFI\nFBgiIhKJAkNERCJRYIiISCQKDBERiUSBISIikSgwREQkEgWGiIhEMmJgmNklZpYfLt9oZv9gZnPi\nr5qIiIwnUVoYy4FmMzsb+EvgbeDRWGslIiLjTpTA6PTgoRnXAg+4+4NAYbzVEhGR8SbKE/cOm9nd\nwI3ApWaWAWTFWy0RERlvorQwlgJtwJ+5+x6gCvharLUSEZFxJ1ILA7jf3bvMbCFwKvBYvNUSEZHx\nJkoL4wUgx8xmAquAm4DvxlkpEREZf6IEhrl7M/BHwDfd/ePAmfFWS0RExptIgWFmFwOfAP79KD4n\nIiITSJQf/HcCdwM/c/cNZnYS8Ot4qyUiIuPNiJ3e7v488LyZFZhZgbtvBT4bf9VERGQ8iTI1yCIz\nexXYAGw0s7Vmdkb8VRMRkfEkyiWpfwLucvc57j6bYHqQf463WiIiMt5ECYx8d+/ts3D354D8KAc3\nsyVm9qaZbTGzZcPsc5mZrTOzDWb2fFh2SljW82owszujfKeIiMQjyo17W83sS8D3w/Ubga0jfcjM\nEsCDwBVADbDazJ50940p+5QA3wSWuPt2M6sEcPc3gfekHGcn8LPIZyUiIqMuSgvjT4EK4InwVRGW\njeQCYIu7b3X3duBHBBMYproBeMLdtwO4e+0Qx7kceNvd343wnSIiEpMoo6QOcmyjomYCO1LWa4AL\nB+yzEMgys+cIZsC9390HTp1+HZqKREQk7YYNDDP7N8CH2+7u14zS959H0IrIBX5rZv/t7m+FdcgG\nriG4D2S4et4C3AIwe/bsUaiSiIgM5UgtjL87zmPvBGalrFeFZalqgDp3bwKazOwF4GzgrXD7VcAr\n7r53uC9x94eAhwCqq6uHDTgRETk+wwZGeMPe8VgNLDCzeQRBcR1Bn0WqFcADZpYJZBNcsvrHlO3X\no8tRIiLjQpRRUsfE3TvN7HbgaSABPBJOLXJbuP1b7r7JzFYC64Fu4GF3fx0gfI74FcCtcdVRRESi\ns+DpqxNDdXW1r1mzJt3VEBE5YZjZWnevjrKvZp0VEZFIRrwkNcxoqXpgDfBP7t4aR8VERGR8idLC\n2Ao0Eswf9c9AA8FjWxeiOaVERCaNKJ3ev+fu56es/5uZrXb3881sQ1wVExGR8SVKC6PAzHrviAuX\nC8LV9lhqJSIi406UFsZfAr8xs7cBA+YB/zMc9vq9OCsnIiLjR5S5pJ4yswXAqWHRmykd3ffFVjMR\nERlXooySyiK4ee7SsOg5M/snd++ItWYiIjKuRLkktRzIInhuBcBNYdmfx1UpEREZf6IExvnufnbK\n+n+Y2e/iqpCIiIxPUUZJdZnZyT0rZnYS0BVflUREZDyK0sL4AvBrM9tKMEpqDvCpWGslIiLjTpRR\nUs+Go6ROCYvedPe2eKslIiLjzZGeuPdHw2yab2a4+xMx1UlERMahI7UwPnyEbQ4oMEREJpEjPXFP\n/RQiItJLz8MQEZFIFBgiIhKJAkNERCKJch8GZvZ7wNzU/d390ZjqJCIi41CUyQe/D5wMrKPvDm8H\nFBgiIpNIlBZGNXC6uw98rreIiEwiUfowXgemxV0REREZ36K0MMqBjWb2MtA7JYi7XxNbrUREZNyJ\nEhh/HXclRERk/Isy+eDzY1EREREZ30bswzCzi8xstZk1mlm7mXWZWUOUg5vZEjN708y2mNmyYfa5\nzMzWmdkGM3s+pbzEzB43szfMbJOZXRz9tEREZLRFuST1AHAd8K8EI6ZuBhaO9CEzSwAPAlcANcBq\nM3vS3Tem7FNC8OjXJe6+3cwqUw5xP7DS3T9mZtlAXsRzEhGRGES609vdtwAJd+9y9+8ASyJ87AJg\ni7tvdfd24EfAtQP2uQF4wt23h99TC2BmxcClwLfD8nZ3PxSlriIiEo8ogdEc/oa/zsy+amafi/i5\nmcCOlPWasCzVQqDUzJ4zs7VmdnNYPg/YB3zHzF41s4fNLD/Cd4qISEyi/OC/KdzvdqAJmAV8dJS+\nPxM4D7gaWAx8ycwWhuXnAsvd/Zzwe4frA7nFzNaY2Zp9+/aNUrVERGSgKKOk3jWzXGC6u99zFMfe\nSRAuParCslQ1QJ27NwFNZvYCcDbwIlDj7i+F+z3OMIHh7g8BDwFUV1frbnQRkZhEGSX1YYJ5pFaG\n6+8xsycjHHs1sMDM5oWXtK4DBn5uBfBeM8s0szzgQmCTu+8BdphZz3PELwc2IiIiaRP1xr0LgOcA\n3H2dmc0b6UPu3mlmtwNPAwngEXffYGa3hdu/5e6bzGwlsB7oBh5299fDQ3wG+GEYNlsBPQFQRCSN\nogRGh7vXm1lqWaRLP+7+FPDUgLJvDVj/GvC1IT67jmAYr4iIjANRAmODmd0AJMxsAfBZ4L/irZaI\niIw3UUZJfQY4g2DiwceABuDOOCslIiLjT5RRUs3A/wlfIiIySQ0bGCONhNL05iIik8uRWhgXE9yp\n/RjwEmBH2FdERCa4IwXGNIKJA68nmPPp34HH3H3DWFRMRETGl2E7vcOJBle6+yeBi4AtwHPhvRUi\nIjLJHLHT28xyCOZ5uh6YC3wd+Fn81RIRkfHmSJ3ejwJnEtx4d0/KHdgiIjIJHamFcSPBLLF3AJ9N\nudPbAHf3opjrJiIi48iwgeHukR6uJCIik4NCQUREIlFgiIhIJAoMERGJRIEhIiKRKDBERCQSBYaI\niESiwBARkUgUGCIiEokCQ0REIlFgiIhIJAoMERGJRIEhIiKRKDBERCQSBYaIiESiwBARkUhiDQwz\nW2Jmb5rZFjNbNsw+l5nZOjPbYGbPp5RvM7PXwm1r4qyniIiM7IjP9D4eZpYAHgSuAGqA1Wb2pLtv\nTNmnBPgmsMTdt5tZ5YDDfMDd98dVRxERiS7OFsYFwBZ33+ru7cCPgGsH7HMD8IS7bwdw99oY6yMi\nIschzsCYCexIWa8Jy1ItBErN7DkzW2tmN6dsc+BXYfktMdZTREQiiO2S1FF8/3nA5UAu8Fsz+293\nfwt4r7vvDC9TPWNmb7j7CwMPEIbJLQCzZ88ew6qLiEwucbYwdgKzUtarwrJUNcDT7t4U9lW8AJwN\n4O47w/da4GcEl7gGcfeH3L3a3asrKipG+RRERKRHnIGxGlhgZvPMLBu4DnhywD4rgPeaWaaZ5QEX\nApvMLN/MCgHMLB+4Eng9xrqKiMgIYrsk5e6dZnY78DSQAB5x9w1mdlu4/VvuvsnMVgLrgW7gYXd/\n3cxOAn5mZj11/Bd3XxlXXUVEZGTm7umuw6iprq72NWt0y4aISFRmttbdq6Psqzu9RUQkEgWGiIhE\nosAQEZFIFBgiIhKJAkNERCJRYIiISCQKDBERiUSBISIikSgwREQkEgWGiIhEosAQEZFI0v08jPFh\n3b9A4TSoOC14DyY9FBGRFAqMrk548rPQ3RGsJ0ug8nSoPDV4rwjf88vSW08RkTRTYCQy4a5NsG8T\n1Ka8Xv8ptD7St19+BVSe1j9EKk+FZHH66i4iMoYUGAAFFcFr3qV9Ze5weHf/ENm3CV75PnQ09e1X\nNDMMkNP6QqTiVMjOH/vzEBGJkQJjOGZQNCN4zb+8r7y7G+p39AVIT5i8/Bvoauvbr2ROGCCn9b3K\nFkBWcuzPRURkFCgwjlZGBpTOCV6nLOkr7+6Cg9ugdmP/VsmWZ6C7M9jHMmDKyX39I5WnQfkpUDoX\nsvPScTYiIpEpMEZLRgLKTg5ep324r7yzHQ683f+yVu0meOPfwbv79iuYBlPmQem8we95UzRyS0TS\nToERt8zsvktSqTpaYf9bwevgO3BgW/C+9Tn43b/03zenKGiFDAyS0rlQXBWElYhIzBQY6ZKVhOln\nBa+BOlo9UTBJAAALM0lEQVTg4LthkLzT9753A7zxVN8QYICMLCiZPUzrZC5k5Y7ZKYnIxKbAGI+y\ncsN+jlMHb+vugoZdg8Pk4DuwYzW01fffv3D60Je5SufqUpeIHBUFxokmIwEls4JX6jBgCIYCtxwc\nHCQHt8Hbv4bDPxxwrCzIL4e88uDGxLzy4H6Tfss928uDe04UMCKTlgJjIjELWg15U6DqvMHbUy91\nHdwGjbXQtA+a66BpPxxcGyy3NQx9/IwsyCsbECrl/UMlv6IvgJIlChiRCUSBMZkc6VJXqs62IECa\n9wfv/ZaPJWDKU4ImXE6WQG4J5JYOWC6GRNbon7uIHDcFhgyWmQPFM4NXFP0CZh801aUs7w8DZh/s\nHCFgemQXhCFSGgRJsnhwuPTbnhI2GjEmEhsFhhy/ow6Ydmith9ZD0HIo6HfpWW4N13uXD0Hd233L\nnS1HPnZOMeQWDxEo4XpPS6fnsll+RRBQunQmMqJYA8PMlgD3AwngYXe/d4h9LgPuA7KA/e7+/pRt\nCWANsNPdPxRnXWUMZWb3zd91tDpaB4TLCIFT+0bf9q72oY+ZyBmmL6ZscL+MAkYmsdgCI/xh/yBw\nBVADrDazJ919Y8o+JcA3gSXuvt3MKgcc5g5gE1AUVz3lBJOVhKxpwXNLjoY7dDSnXDob5rJZ036o\n2xy8dzQPfayegOnXLzNwAEDKwICcQgWMTAhxtjAuALa4+1YAM/sRcC2wMWWfG4An3H07gLvX9mww\nsyrgauBvgbtirKdMBmbBDMLZ+cE8YFG0Nw/RLzOg479pXxgwdf1nMU6VyIGCqVA0PZjMsnDGgOUZ\nwf0ymdmjd74iMYgzMGYCO1LWa4ALB+yzEMgys+eAQuB+d3803HYf8MWwXGTsZedB9uzgTvooegNm\nQIulaV8whLlhJ+x5Dd56eujWS35FEBw9syT3hEnR9GAa/cLpkFRjW9In3Z3emcB5wOVALvBbM/tv\ngiCpdfe1YR/HsMzsFuAWgNmzI/7HFolD1IBxDzr9D+8OQqRhd3D3/uFdwXv9TqhZHYTOoO8oDAKk\nMAyRgS2VohnBZbCMjHjOUSa1OANjJzArZb0qLEtVA9S5exPQZGYvAGcD5wLXmNkfAEmgyMx+4O43\nDvwSd38IeAigurraR/80REaZWThqq2TwpJSpOlrDUNnVP1x6guWdF4Jy7+r/uYysMFDCYBnYiZ+X\n0teSN0VDkSWyOANjNbDAzOYRBMV1BH0WqVYAD5hZJpBNcMnqH939X4G7oXcU1eeHCguRCS0rGcz9\nNWXe8Pt0dwWXvHrDJLXVsjOYsLJ5fzBSbEgWDDceNEVMarCkBE1emfpaJrHYAsPdO83sduBpgmG1\nj7j7BjO7Ldz+LXffZGYrgfVAN8HQ29fjqpPIhJORCEaMFU4Leg2H09UJLQdSRoXt79+R37O+fzM0\n/TbYN/V5LalyioefGiY1dJLFwRDk7PxglgGNFDvhmfvEuYpTXV3ta9asSXc1RE583V3BvSz9AmV/\n3+iwodZ7niw5FMvoC4/s/HA5ZT2nYITthYO3ZeYohEaBma119+oo+6a701tExqOMRHijYhlUnDLy\n/u7BzZGprZa2w9DeGL6aglfb4b7l9kZo3JOyLdyXiL/EWiIIjpwhgiZZHL6KUpaLg4eR9a6XBNs1\nd1lkCgwROX4W9oXklgLzj/047sGsyoOCJnV9hG31NVC7IZx+poERAygrb4gwGS5sSgZvy0xOmpaO\nAkNExg+zcHhyHjBw4odj0N0N7YeD4GitD15tKcsDX20NQQvpwNvhZw4d+VIbQCK7f9ikzl/W+148\nRFlJ8LkTaAi0AkNEJq6MjL4f5P1G+UfU0+KJEjQth/om1Tz4bt/yEQPHwtZKxIDpt1wMibH9Ea7A\nEBEZTr8Wz/Sj/7x7cKksdXLM1vr+k2f2vtf3TZjZU9bVduTjZxcGAVI8C/70l8d0ikdDgSEiEhez\noFM+pwCKq47+8z2tmyMFTMuhMWtpKDBERMarrNzgdbSzM8fkxOltERGRtFJgiIhIJAoMERGJRIEh\nIiKRKDBERCQSBYaIiESiwBARkUgUGCIiEsmEeh6Gme0D3j3Gj5cD+0exOicCnfPEN9nOF3TOR2uO\nu1dE2XFCBcbxMLM1UR8iMlHonCe+yXa+oHOOky5JiYhIJAoMERGJRIHR56F0VyANdM4T32Q7X9A5\nx0Z9GCIiEolaGCIiEsmkDwwzW2Jmb5rZFjNblu76xM3MZpnZr81so5ltMLM70l2nsWJmCTN71cx+\nke66jAUzKzGzx83sDTPbZGYXp7tOcTOzz4X/rl83s8fMLJnuOo02M3vEzGrN7PWUsilm9oyZbQ7f\nS+P47kkdGGaWAB4ErgJOB643s9PTW6vYdQJ/6e6nAxcBn54E59zjDmBTuisxhu4HVrr7qcDZTPBz\nN7OZwGeBanc/E0gA16W3VrH4LrBkQNky4Fl3XwA8G66PukkdGMAFwBZ33+ru7cCPgGvTXKdYuftu\nd38lXD5M8ENkZnprFT8zqwKuBh5Od13GgpkVA5cC3wZw93Z3P5TeWo2JTCDXzDKBPGBXmusz6tz9\nBeDAgOJrge+Fy98D/jCO757sgTET2JGyXsMk+OHZw8zmAucAL6W3JmPiPuCLQHe6KzJG5gH7gO+E\nl+EeNrP8dFcqTu6+E/g7YDuwG6h391XprdWYmeruu8PlPcDUOL5ksgfGpGVmBcBPgTvdvSHd9YmT\nmX0IqHX3temuyxjKBM4Flrv7OUATMV2mGC/C6/bXEoTlDCDfzG5Mb63GngdDX2MZ/jrZA2MnMCtl\nvSosm9DMLIsgLH7o7k+kuz5j4BLgGjPbRnDZ8YNm9oP0Vil2NUCNu/e0Hh8nCJCJ7PeBd9x9n7t3\nAE8Av5fmOo2VvWY2HSB8r43jSyZ7YKwGFpjZPDPLJuggezLNdYqVmRnBde1N7v4P6a7PWHD3u929\nyt3nEvwd/4e7T+jfPN19D7DDzE4Jiy4HNqaxSmNhO3CRmeWF/84vZ4J39Kd4EvhkuPxJYEUcX5IZ\nx0FPFO7eaWa3A08TjKh4xN03pLlacbsEuAl4zczWhWX/292fSmOdJB6fAX4Y/jK0FfhUmusTK3d/\nycweB14hGA34KhPwrm8zewy4DCg3sxrgy8C9wE/M7M8IZuz+41i+W3d6i4hIFJP9kpSIiESkwBAR\nkUgUGCIiEokCQ0REIlFgiIhIJAoMkRGYWZeZrUt5jdod02Y2N3XWUZHxbFLfhyESUYu7vyfdlRBJ\nN7UwRI6RmW0zs6+a2Wtm9rKZzQ/L55rZf5jZejN71sxmh+VTzexnZva78NUzbUXCzP45fI7DKjPL\nDff/bPjckvVm9qM0naZILwWGyMhyB1ySWpqyrd7dFwEPEMyIC/AN4HvufhbwQ+DrYfnXgefd/WyC\neZ16ZhVYADzo7mcAh4CPhuXLgHPC49wW18mJRKU7vUVGYGaN7l4wRPk24IPuvjWc0HGPu5eZ2X5g\nurt3hOW73b3czPYBVe7elnKMucAz4YNvMLP/BWS5+9+Y2UqgEfg58HN3b4z5VEWOSC0MkePjwywf\njbaU5S76+havJngi5LnA6vChQCJpo8AQOT5LU95/Gy7/F32PBv0E8GK4/CzwF9D7fPHi4Q5qZhnA\nLHf/NfC/gGJgUCtHZCzpNxaRkeWmzOwLwXOye4bWlprZeoJWwvVh2WcInnT3BYKn3vXMEnsH8FA4\no2gXQXjsZmgJ4AdhqBjw9UnyiFUZx9SHIXKMwj6Manffn+66iIwFXZISEZFI1MIQEZFI1MIQEZFI\nFBgiIhKJAkNERCJRYIiISCQKDBERiUSBISIikfx/5d3f6H015cUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d6da6f30f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_curve, = plt.plot(costs_train, label = 'Train error')\n",
    "test_curve,  = plt.plot(costs_dev, label = 'Dev error')\n",
    "plt.legend(handles=[train_curve,test_curve])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean log loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by increasing the number of layers, and obviously using the artifacts available on CNN, the model was 2% higher in accuracy and the dev error decreased considerably.  Funny thing is that there are 1000 times less parameters (7.701) than shallow NN (7.864.800). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As fun as it is to create models using tensorflow, we will switch to keras. Although tensorflow is a bit faster, we can build models with less lines and iterate quickly through various approaches. We will draw inspiration from major modelling techniques like: AlexNet, VGG, Googlenet and even make moonshots to ResNeXt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet-like\n",
    "[AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) is the pioneer deep learning architecture. Developed by Alex Krizhevsky, Ilya Sutskever and the godfather of Deep Learning, Geofrey Hinton, this algorithm won the ILSVR 2012 competition by a large marging, proving that deep learning was the top technique for image classification and localization.\n",
    "\n",
    "Alexnet hidden layer architecture is made of eight layers, five convolutional and two fully connected with the following characteristic:\n",
    "* 96 kernels (filters) of size 11x11x3 with a stride of 4 pixels;\n",
    "    * Response normalization\n",
    "    * Pooling (size 3, stride 2)\n",
    "* 256 kernels of size 5x5x96\n",
    "    * Response normalization\n",
    "    * Pooling (size 3, stride 2)\n",
    "* 384 kernels of size 3x3x256\n",
    "* 384 kernels of size 3x3x384\n",
    "* 256 kernels of size 3x3x384\n",
    "* Two consecutive fully connected layers with 4096 neurons each\n",
    "    * With dropout rate of 0.5\n",
    "\n",
    "Our problem differs from the original problem it solves in two aspects. First, the input images were 224x224x3 pixels, while our problem images are 128x128x3 pixels. Second, the original problem was a classification with 1000 possible classes, therefore, the output layer has 1000 neurons. In our problem, there are only 2 possible outputs, therefore, our output layer has only 1 neuron.\n",
    "Therefore, we will build a similar architecture considering the difference between both problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hermes\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Hermes\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# LAYER 1\n",
    "# The actual model used padding = 3. Will use sequential model with padding=\"same\"\n",
    "model.add(Conv2D(96,11,strides=(4,4),padding='same',input_shape=(128,128,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(3,2))\n",
    "# LAYER 2\n",
    "model.add(Conv2D(256,5,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(3,2))\n",
    "# LAYER 3\n",
    "model.add(Conv2D(384,3,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# LAYER 4\n",
    "model.add(Conv2D(384,3,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# LAYER 5\n",
    "model.add(Conv2D(256,3,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# LAYER 6\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "# LAYER 7\n",
    "model.add(Dense(4096))\n",
    "# OUTPUT\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "128/484 [======>.......................] - ETA: 143s - loss: 8.0590 - acc: 0.0938"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-49dd490f6bf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(images_train, labels_train, epochs=2, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(images_dev, labels_dev)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "3-DeepSnakes-CNN",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "none",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
