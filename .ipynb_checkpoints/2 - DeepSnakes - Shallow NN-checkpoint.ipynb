{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Snakes\n",
    "by Hermes, Channel and Jeanne. Rio de Janeiro, Brazil, 2018.\n",
    "\n",
    "## Welcome to project Deep Snakes\n",
    "\n",
    "In this series, we will try different machine learning approaches to identify two classes of snakes; python snakes (family pythonidae) and rattlesnakes (genera crotalus and sistrurus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shalow NN\n",
    "In the previous notebook we showed the minimal performance achieved using a simple Logistic Regression. Continuing the endeavor, we here experiment with shallow neural networks. We will make experiments below using fully connected architectures using only one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "The data is stored in an HDF5 file, so, we built a simple routine \"snake_data\" to retrieve it as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import tensorflow as tf\n",
    "from supporting_functions import snake_data, reg_reshape_snakes\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train_orig, labels_train_orig, images_dev_orig, labels_dev_orig = snake_data()\n",
    "print(\"Shapes\")\n",
    "print(\"Images train:\", images_train_orig.shape)\n",
    "print(\"Labels train:\", labels_train_orig.shape)\n",
    "print(\"Images dev:\", images_dev_orig.shape)\n",
    "print(\"Labels dev:\", labels_dev_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to treat the dataset in order to normalize the pixel values and reshape the 4D array to a 2D array. We do so using reg_reshape_snakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, labels_train = reg_reshape_snakes(images_train_orig,labels_train_orig)\n",
    "images_dev, labels_dev = reg_reshape_snakes(images_dev_orig,labels_dev_orig)\n",
    "imsize = images_train.shape[0] # amount of pixels\n",
    "dsize = images_train.shape[1] # amount of images\n",
    "dsize_dev = images_dev.shape[1] # amount of dev images\n",
    "print(\"Shapes\")\n",
    "print(\"Images train:\", images_train.shape)\n",
    "print(\"Labels train:\", labels_train.shape)\n",
    "print(\"Images dev:\", images_dev.shape)\n",
    "print(\"Labels dev:\", labels_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "### Single hidden layer\n",
    "First, we build a baseline single hidden layer neural network. Following current common practices, we start with ReLU neurons in the hidden layer. To start simple and make quick dataset and error analises, we use 10 neurons in the hidden layer. Since this is a dual class classification problem, we use a single sigmoid neuron in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_builder(num_neurons):\n",
    "    # Builds the models inputs, parameters and feed forward graph.\n",
    "    # Argument: num_neurons - number of neurons\n",
    "    # Returns: tf objects regarding X, Y, costs and a dictionary containing the weights and biases\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(2468)\n",
    "    # Adding placeholders for I/O\n",
    "    X = tf.placeholder(tf.float32,shape=[imsize,None],name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32,shape=[1,None], name=\"Y\")\n",
    "    # Creating the hidden layers\n",
    "    W1 = tf.get_variable(\"W1\",[num_neurons,imsize],tf.float32,tf.contrib.layers.xavier_initializer(seed=7324))\n",
    "    b1 = tf.get_variable(\"b1\",[num_neurons,1],tf.float32,tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\",[1,num_neurons],tf.float32,tf.contrib.layers.xavier_initializer(seed=5236))\n",
    "    b2 = tf.get_variable(\"b2\",[1,1],tf.float32,tf.random_normal_initializer())\n",
    "    # Hidden layer processing\n",
    "    A1 = tf.add(tf.matmul(W1,X),b1)\n",
    "    Z1 = tf.nn.relu(A1)\n",
    "    # Output layer processing\n",
    "    A2 = tf.add(tf.matmul(W2,Z1),b2)\n",
    "    # Cost function\n",
    "    cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y,logits=A2))\n",
    "    # Join weights and biases in a single dictionary\n",
    "    params = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "    return X, Y, cost, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "First, we build three helping functions\n",
    "* `split_data` to split the data into mini-batches.\n",
    "* `predict` to make predictions using the calculated weights and biases\n",
    "* `accuaracy` to computethe percentage match between predicted and target values\n",
    "\n",
    "For the predict function it is easyer and simpler to make the whole feed-forward calculation using numpy, rather than tensorflow, once we already have the W's and b's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(indices,batch_size):\n",
    "    divisible_batches = len(indices)//batch_size\n",
    "    divisible_sequence = divisible_batches*batch_size\n",
    "    splits = np.split(indices[:divisible_sequence],divisible_batches)\n",
    "    if divisible_sequence != len(indices):\n",
    "        splits.append(indices[divisible_sequence:])\n",
    "    return splits\n",
    "\n",
    "def predict(x,w1,b1,w2,b2): #to supporting_functions\n",
    "    a1 = np.dot(w1,x)+b1\n",
    "    z1 = np.maximum(0,a1)\n",
    "    a2 = np.dot(w2,z1)+b2\n",
    "    probs = 1/(1+np.exp(-a2))\n",
    "    y_pred = (probs > 0.5).astype(int)\n",
    "    return y_pred\n",
    "\n",
    "def accuracy(label,y_pred): #to supporting_functions\n",
    "    acc = np.mean(np.equal(label,y_pred))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use stochastic gradient descent for this training. Actually, we will implement batch gradient descent, but use it with bathsize 1 so we can tune the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_trainer(images_train = images_train, labels_train = labels_train, \n",
    "                  num_neurons=100, batch_size=4, epochs=100, alpha=1.e-5, lambd=5., print_every=5):\n",
    "    #The defaoult learning rate (alpha) is small because we are using sum log loss instead of mean log loss\n",
    "    # We are using sum log loss to correctly scale costs when using batch GD.\n",
    "    np.random.seed(4681)\n",
    "    # Call model builder\n",
    "    X, Y, cost, params = model_builder(num_neurons)\n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    # Cost plus regularization\n",
    "    # Will optimize cost plus regularization but will only plot the cost part\n",
    "    cpr = cost + lambd*(tf.nn.l2_loss(W1)+tf.nn.l2_loss(W2))\n",
    "    # Defining the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(alpha).minimize(cpr)\n",
    "    # Initializing variable\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Running session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        costs_train = []\n",
    "        costs_dev = []\n",
    "        cost_dev_best = float(\"inf\")\n",
    "        # Indices to be shuffled\n",
    "        indices = np.arange(dsize)\n",
    "        # splitting the indices in batch_size chunks plus a last different chunk if necessary\n",
    "        splits = split_data(indices,batch_size)\n",
    "        # list containing the number of elements in each split\n",
    "        nelements = [float(len(x)) for x in splits]\n",
    "        for epoch in range(epochs+1):\n",
    "            # shuffling the dataset indices\n",
    "            np.random.shuffle(indices)\n",
    "            # splitting the dataset\n",
    "            splits = split_data(indices,batch_size)\n",
    "            epoch_cost = []\n",
    "            # running the training across batches\n",
    "            for split in splits:\n",
    "                _,batch_cost = sess.run([optimizer,cost],feed_dict={X:images_train[:,split],Y:labels_train[:,split]})\n",
    "                epoch_cost.append(batch_cost/len(split))\n",
    "            # computing metrics\n",
    "            cost_train = np.dot(epoch_cost,nelements)/dsize\n",
    "            costs_train.append(cost_train)\n",
    "            cost_dev = sess.run(cost, feed_dict={X:images_dev,Y:labels_dev})\n",
    "            cost_dev = cost_dev/dsize_dev\n",
    "            costs_dev.append(cost_dev)\n",
    "            # Save parameters for best dev error (early stopping)\n",
    "            if cost_dev < cost_dev_best:\n",
    "                W1v,b1v,W2v,b2v = sess.run([W1,b1,W2,b2]) # \"v\" stands for value\n",
    "                params_train = {\"W1\":W1v,\"b1\":b1v,\"W2\":W2v,\"b2\":b2v,\"epoch\":epoch,\"cost_dev\":cost_dev}\n",
    "                cost_dev_best = cost_dev\n",
    "            # print on screen\n",
    "            if epoch%print_every == 0: \n",
    "                print(\"Epoch {}: train cost = {}, dev cost = {}\".format(epoch,cost_train,cost_dev))\n",
    "                #print(sess.run(W1))\n",
    "        return costs_train, costs_dev, params_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model with our very first choice of hyperparameters and see how it goes. Ok, i might have tweaked the hyperparameters before just a little bit. OK, i've just wasted the whole afternoon manually searching for the best combination instead of putting up a systematic approach for doing so. I'm helpless! :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "costs_train, costs_dev, params_train = model_trainer()\n",
    "W1 = params_train[\"W1\"]\n",
    "b1 = params_train[\"b1\"]\n",
    "W2 = params_train[\"W2\"]\n",
    "b2 = params_train[\"b2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizing the results\n",
    "We will quickly analize the main model's performance metrics. First, let's take a look at the train/dev error curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dev error {:5.4f} on epoch {}.\".format(params_train[\"cost_dev\"],params_train[\"epoch\"]))\n",
    "train_curve, = plt.plot(costs_train, label = 'Train error')\n",
    "test_curve,  = plt.plot(costs_dev, label = 'Dev error')\n",
    "plt.legend(handles=[train_curve,test_curve])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean log loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, predict the class labels using the previously computed model paramaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict(images_train,W1,b1,W2,b2)\n",
    "acc = accuracy(labels_train,y_train_pred)\n",
    "print(\"Train set accuracy:\", acc)\n",
    "y_dev_pred = predict(images_dev,W1,b1,W2,b2)\n",
    "acc = accuracy(labels_dev,y_dev_pred)\n",
    "print(\"Dev set accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see a much less erratic error curved as compared to Logistic Regression. By inserting one hidden layer in the model, as well as adding L2 regularization, the shallow NN dev set accuracy is better than logistic regression by 5%. This gives us motivation to try out deeper achitectures. However, before doing that, we should conduct an error analisys to see exactly where the model is having a hard time and use this to fine tune our modelling strategy and plan our next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analisys\n",
    "To provide faster iterations, error analisys must always be performed on the simplest method. Since logistic regression was so simple that it left few options for improving, we will do this with shallow NNs. We will visually go over the dev set and try to figure out what are the most prevalent sources of mistakes. To avoid making biased judgements over the dataset, we will not analize the train set performance. Error analisys is a systematic approach to tell us which directions to prioritize. For instance, I would most certainly guess that the main source of erros is the low resolution on the images. You will see below that it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_images = images_dev_orig[(y_dev_pred != labels_dev).ravel()]\n",
    "wrong_labels = labels_dev_orig[(y_dev_pred != labels_dev).ravel()]\n",
    "right_images = images_dev_orig[(y_dev_pred == labels_dev).ravel()]\n",
    "right_labels = labels_dev_orig[(y_dev_pred == labels_dev).ravel()]\n",
    "print(\"NUmber of hits:\",len(right_images))\n",
    "print(\"Number of misses:\",len(wrong_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def species(i): #to supporting_functions\n",
    "    # Name the species by its label\n",
    "    if i == 1:\n",
    "        name = \"python\"\n",
    "    elif i==0:\n",
    "        name = \"rattlesnake\"\n",
    "    else:\n",
    "        name = \"dunno\"\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 16\n",
    "print(\"Wrongly predicted image\")\n",
    "print(\"Should be a\", species(wrong_labels[i]))\n",
    "plt.imshow(wrong_images[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 25\n",
    "print(\"Correctly predicted images\")\n",
    "print(\"Yep, it's a\", species(right_labels[i]))\n",
    "plt.imshow(right_images[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after going over all misses here is what I found to be possble sources of error.\n",
    "* 58% - Nothing - That's right. Most of the time there seems to be absolutely nothing wrong with the image.\n",
    "* 25% - <b>Low res</b>. - Either too pixelated given the image complexity or the size of the snake .\n",
    "* 17% - Bad head - Pictures showing the snake without or with only a small head.\n",
    "* 11% - IDR - I don't recognize. Meaning, it's difficult to assert which snake the image is showing.\n",
    "\n",
    "So, we can see that the prevalent cause for erros is NONE. That's probably because of the low expressive power of this method. So, the prefered path would be to use deeper models. However, we can do one or two tricks on shallow NN's like:on the dataset side we can use artificial dataset augmentation. On the model side, hyperparameter tuning. We will try them below and try a more complex model on another notebook. Examples: `wrong_images` #2, #21 and #33.\n",
    "\n",
    "The number two cause of problems - but not nearly close the number one cause- is low resolution. This may happen either because the snake is too small in the image, the background camouflages the snake or the image simply fails to capture the body pattern complexity. This can obviously be solved by increasing the number of pixels, which we will not do right now, for the sake of continuity. We will hope that dataset augmentation may take care of some cases. Examples: #3, #10 and #30.\n",
    "\n",
    "Bad head. This problem may arise because the snake head is missing or isn't exactly clear on the image. As a human, what you should do is try and figure out which snake it is by body features. To induce such a learning in NN, we could randomly crop the images to maybe increase the chance of a picture not showing a head and forcing the NN to look elswhere. Examples: #0, #6 and #31.\n",
    "\n",
    "I don'n recognize is maybe one of the most useful errors. It sets the thresshold of human perception on the problem. These are not exactly that hard to analize, but requires some reflection taking more than 1 second of thought. We will most likely use this class to softly evaluate the model against human performance. Examples: #3, #8 and #16.\n",
    "\n",
    "In this dataset some pictures presented a single problem, while others were overall bad quality, with multiple possible sources of problems. Overall, we will tackle these problems in two ways in this notebook, hyperparameter tuning and dataset augmentation. Another important thing will be increasing the complexity of the model later on another notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
