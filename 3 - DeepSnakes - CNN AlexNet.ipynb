{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWCiggNd_h_f"
   },
   "source": [
    "# Deep Snakes\n",
    "by Hermes, Channel and Jeanne. Rio de Janeiro, Brazil, 2018.\n",
    "\n",
    "## Welcome to project Deep Snakes\n",
    "\n",
    "In this series, we will try different machine learning approaches to identify two classes of snakes; python snakes (family pythonidae) and rattlesnakes (genera crotalus and sistrurus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D66nXmaf_m_Q"
   },
   "source": [
    "## Convolutional Neural Networks\n",
    "In the previous notebook, we developed and tuned a shallow neural network, finishing with an error analysis, showing that most errors had no apparent causes. In this notebook, we use deep learning architectures to improve our model's capacity. In this notebook we will address:\n",
    "* Using a two convolutional layer model (TensorFlow)\n",
    "* Using the pioneering Deep Learning architecture AlexNet (Keras)\n",
    "  * Performing online dataset augmentation (Keras)\n",
    "  * Regularizing the model with dropout\n",
    "  * Normalizing activations with batch norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2YrSUAu_qty"
   },
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m2ue5xaa_r2X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import tensorflow as tf\n",
    "from supporting_functions_colab import snake_data\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1527797496026,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "Mnwm5S0DPXbz",
    "outputId": "bbfd1005-b3b9-4247-ede2-c178aa4dbe05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "Images train: (484, 128, 128, 3)\n",
      "Labels train: (484,)\n",
      "Images dev: (121, 128, 128, 3)\n",
      "Labels dev: (121,)\n"
     ]
    }
   ],
   "source": [
    "images_train_orig, labels_train_orig, images_dev_orig, labels_dev_orig = snake_data()\n",
    "print(\"Shapes\")\n",
    "print(\"Images train:\", images_train_orig.shape)\n",
    "print(\"Labels train:\", labels_train_orig.shape)\n",
    "print(\"Images dev:\", images_dev_orig.shape)\n",
    "print(\"Labels dev:\", labels_dev_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bZDVM6B_yUV"
   },
   "source": [
    "This time we don't need to linearize the dataset, we will only normalize it and set the vector dimensions accordingly. Since CNNs deal with 2D images, the input is a 4D tensor with dimensions `[samples, width, height, channels]`, and the labels have dimensions `[samples,1]`, since this is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1527797496916,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "72jzB6FbLf6a",
    "outputId": "06b9ecb0-6f6e-4f4d-d9ad-18e63cdcd216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "Images train: (484, 128, 128, 3)\n",
      "Labels train: (484, 1)\n",
      "Images dev: (121, 128, 128, 3)\n",
      "Labels dev: (121, 1)\n"
     ]
    }
   ],
   "source": [
    "images_train = images_train_orig/255\n",
    "labels_train = (labels_train_orig).reshape(-1,1)\n",
    "images_dev = images_dev_orig/255\n",
    "labels_dev = (labels_dev_orig).reshape(-1,1)\n",
    "imsize = images_train.shape[1:3] # image_shape\n",
    "dsize = images_train.shape[0] # amount of images\n",
    "dsize_dev = images_dev.shape[0] # amount of dev images\n",
    "print(\"Shapes\")\n",
    "print(\"Images train:\", images_train.shape)\n",
    "print(\"Labels train:\", labels_train.shape)\n",
    "print(\"Images dev:\", images_dev.shape)\n",
    "print(\"Labels dev:\", labels_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPmWMO9aPxVV"
   },
   "source": [
    "## Building the model\n",
    "We begin with a simple approach: using two convolutional layers. The number of neurons in the dense layer is 420, chosen by hyperparameter tuning on the last notebook. Nextm we experiment with AlexNet which has more hidden layers, as well as other accessories to increase model expressiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RPDWqb0Q8jx"
   },
   "source": [
    "### Triple Layer CNN\n",
    "Convolutional Neural Networks (CNN) are made by (as the name suggests), convolutional layers.  The first operation is convolving the image with filters in order to generate a 3D affine transformation matrix. To add the nonlinearity, intrinsic to neural networks, an activation function is applied over the affine transformation matrix to generate an activation matrix. Optinionally,  these layers ends with a pooling operation to reduce the matrix size along the first two axes. Pooling means aggregating the values of neighboting features or representations using a simple function like max or mean. \n",
    "\n",
    "Using a convolutional layers in sequence has the advantage of exponencially increasing the NN's expressive power, which brings two major advantages: less parameters and better accuracy, compared to a shalow (but wide) fully connected models.\n",
    "\n",
    "Typically, as we go deeper into the model architecture, the height and width of the activation matrix decreases while the lenght increases. The last layer is usually a 1D flattened layer. So, we will start with two 2D convolutional layers, foloowed by a flattening operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Te0etsI0Tw73"
   },
   "outputs": [],
   "source": [
    "def model_builder(cparams,hparams):\n",
    "    # Builds the models placeholders for inputs and outputs, parameter \n",
    "    # variables and feed forward computation graph.\n",
    "    # Argument: cparams - constant params, meaning, input/output dimensions.\n",
    "    #           hparams - hyperparameters, meaning, hyperparameters. All of them.\n",
    "    # Returns: tf objects regarding X, Y, logits and costs.\n",
    "    #\n",
    "    \n",
    "    # getting win, hin, nout (inputs width and height, output # of classes)\n",
    "    win = cparams[\"win\"]\n",
    "    hin = cparams[\"hin\"]\n",
    "    nout = cparams[\"nout\"]\n",
    "    \n",
    "    # getting f1,c1,f2,c2. Stride on filters will be 1.\n",
    "    f1 = hparams[\"f1\"] # filter size of conv 1\n",
    "    c1 = hparams[\"c1\"] # num channels(filters) conv 1\n",
    "    f2 = hparams[\"f2\"] # there are 2 types of people in the world,\n",
    "    c2 = hparams[\"c2\"] # those who can extrapolate from incomplete information\n",
    "    \n",
    "    # getting p1, sp2 (pooling layer sizes, will consider size=stride)\n",
    "    p1 = hparams[\"p1\"]\n",
    "    p2 = hparams[\"p2\"]\n",
    "    \n",
    "    # planting the seed\n",
    "    tf.set_random_seed(1988)\n",
    "    \n",
    "    # creating placeholders for input/output\n",
    "    X = tf.placeholder(dtype=tf.float32,shape=[None,win,hin,3],name=\"X\")\n",
    "    Y = tf.placeholder(dtype=tf.float32,shape=[None,nout],name=\"Y\")\n",
    "    \n",
    "    # first layer\n",
    "    W1 = tf.get_variable('conv_1/W',shape=(f1,f1,3,c1), initializer=tf.glorot_normal_initializer(9))\n",
    "    b1 = tf.get_variable('conv_1/b',shape=(c1),initializer=tf.zeros_initializer())\n",
    "    conv_1 = tf.nn.bias_add(tf.nn.conv2d(X,W1,(1,1,1,1),'SAME'),b1)\n",
    "    act_1 = tf.nn.relu(conv_1)\n",
    "    pool_1 = tf.nn.max_pool(act_1, (1,p1,p1,1), (1,p1,p1,1), 'VALID')\n",
    "    \n",
    "    # second layer\n",
    "    W2 = tf.get_variable('conv_2/W',shape=(f2,f2,c1,c2), initializer=tf.glorot_normal_initializer(8))\n",
    "    b2 = tf.get_variable('conv_2/b',shape=(c2),initializer=tf.zeros_initializer())\n",
    "    conv_2 = tf.nn.bias_add(tf.nn.conv2d(pool_1,W2,(1,1,1,1),'SAME'),b2)\n",
    "    act_2 = tf.nn.relu(conv_2)\n",
    "    pool_2 = tf.nn.max_pool(act_2, (1,p2,p2,1), (1,p2,p2,1), 'VALID')\n",
    "    \n",
    "    # flatten the 3D tensors\n",
    "    flat = tf.layers.flatten(pool_2)\n",
    "    \n",
    "    # logits\n",
    "    wpool = int(win/p1/p2)\n",
    "    hpool = wpool\n",
    "    cpool = c2\n",
    "    W3 = tf.get_variable('dense_1/W',shape=(1,wpool*hpool*cpool), initializer=tf.glorot_normal_initializer(7))\n",
    "    b3 = tf.get_variable('dense_1/b',shape=(1,1), initializer=tf.zeros_initializer())\n",
    "    logits = tf.transpose(tf.matmul(W3,tf.transpose(flat))+b3)\n",
    "    \n",
    "    # cost\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    reg = (tf.nn.l2_loss(W1) + \n",
    "           tf.nn.l2_loss(W2) + \n",
    "           tf.nn.l2_loss(W3))/tf.cast(tf.shape(X)[0],tf.float32)\n",
    "    \n",
    "    return X, Y,logits, cost, reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSFtEtsu1KFW"
   },
   "source": [
    "Training the model. For minibatch split, we will try a different approach. Previously, when the number of samples was not divisible by the (mini)batch size, we split the data into n chunks, where n-1 were equally sized and the last one contained the remaining data, which could be of different size. Now, we will use numpy `array_split`:\n",
    ">For an array of length `l` that should be split into `n` sections, it returns `l%n` sub-arrays of size l//n + 1 and the rest of size l//n.\n",
    "\n",
    "Therefore, `batch_size` will no longer be constant, but bounded below by its value and upper bounded by `batch_size+1`. This may bring about some computational efficiency issue, but will minimize some effect cused by the last mini_batch being smaller than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "pJyYWzl11KFX"
   },
   "outputs": [],
   "source": [
    "def model_trainer(hparams, \n",
    "                  images_train = images_train, # Problematic if used incorrectly.\n",
    "                  labels_train = labels_train, # watch out!\n",
    "                  images_dev = images_dev, \n",
    "                  labels_dev = labels_dev, \n",
    "                  print_every=5):\n",
    "    # Trains the model and saves the best iteration.\n",
    "    # Argument: hparams - hyperparameters, meaning, hyperparameters. All of them.\n",
    "    #           images_train - train set state tensor of inputs\n",
    "    #           labels_train - train set state vector of outputs\n",
    "    #           images_dev - dev set state tensor of inputs\n",
    "    #           labels_dev - dev set state vector of outputs\n",
    "    # Returns: train and dev set costs\n",
    "\n",
    "    tf.set_random_seed(1111) # the answer to life the universe and everything\n",
    "    np.random.seed(2222) # Is this a valid turing test question tho? \n",
    "    \n",
    "    # number of train and dev samples\n",
    "    n_train = images_train.shape[0]\n",
    "    n_dev = images_dev.shape[0]\n",
    "    \n",
    "    # Retrieving hyperparameters\n",
    "    epochs = hparams[\"epochs\"]\n",
    "    learning_rate = hparams[\"learning_rate\"]\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    num_batches = n_train//batch_size\n",
    "    lambd = hparams[\"lambd\"]\n",
    "    \n",
    "    # determining cparams\n",
    "    cparams = {\"hin\":images_train.shape[1], \n",
    "               \"win\":images_train.shape[2], \n",
    "               \"nout\":labels_train.shape[1]}\n",
    "    \n",
    "    # building the model\n",
    "    X,Y,logits,cost, reg = model_builder(cparams,hparams)\n",
    "    \n",
    "    # bulding the floppy disc icon\n",
    "    #saver = tf.train.Saver(max_to_keep = 1)\n",
    "    \n",
    "    # defining the optimizer\n",
    "    cpr = cost+lambd*reg\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cpr)\n",
    "    \n",
    "    # defining the prediction\n",
    "    pred = tf.sigmoid(logits)\n",
    "    \n",
    "    # I'm session and I know it!\n",
    "    with tf.Session() as sess: \n",
    "        # Performance variables container\n",
    "        costs_train = []\n",
    "        costs_dev = []\n",
    "        preds_train = []\n",
    "        preds_dev = []\n",
    "        accs_train = []\n",
    "        accs_dev = []\n",
    "        cost_dev_best = float(\"inf\")\n",
    "        \n",
    "        # enumerating the samples\n",
    "        indices = np.arange(n_train) \n",
    "        # note to self: don't use n or m, use explicit names. Almost screwed everything up.\n",
    "        \n",
    "        # Starting the global variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        # iterating through epochs\n",
    "        for epoch in range(epochs+1): # epochs+1 to facilitate print_every\n",
    "            # everyday I'm shufflin\n",
    "            indices = np.random.permutation(indices)\n",
    "            splits = np.array_split(indices,num_batches)\n",
    "            cost_train = 0.\n",
    "            \n",
    "            # running mini batches\n",
    "            pred_train = np.zeros([n_train,1])\n",
    "            for split in splits:\n",
    "                _,temp_cost = sess.run([optimizer,cost],feed_dict={X:images_train[split],Y:labels_train[split]})\n",
    "                cost_train += temp_cost/num_batches\n",
    "                pred_train[split] = sess.run(pred, feed_dict={X:images_train[split],Y:labels_train[split]})\n",
    "            # storing train cost\n",
    "            costs_train.append(cost_train)\n",
    "            preds_train.append(pred_train)\n",
    "            \n",
    "            # computing dev cost\n",
    "            cost_dev=0.\n",
    "            pred_dev = np.zeros([n_dev,1])\n",
    "            # will divide into samples to minimize mamory usage\n",
    "            samples = np.array_split(np.arange(n_dev),n_dev/16)\n",
    "            for sample in samples:\n",
    "                cost_dev += sess.run(cost, feed_dict={X:images_dev[sample],Y:labels_dev[sample]})/(n_dev/16)\n",
    "                pred_dev[sample] = sess.run(pred,feed_dict={X:images_dev[sample],Y:labels_dev[sample]})\n",
    "            costs_dev.append(cost_dev)\n",
    "            preds_dev.append(pred_dev)\n",
    "            \n",
    "            # using numpy to calculate the accuracies (less verbosity)\n",
    "            acc_train = np.mean(np.equal(np.greater(pred_train,0.5).astype(int),labels_train))\n",
    "            acc_dev = np.mean(np.equal(np.greater(pred_dev,0.5).astype(int),labels_dev))\n",
    "            accs_train.append(acc_train)\n",
    "            accs_dev.append(acc_dev)\n",
    "            # saving the best parameters for later reuse\n",
    "            if cost_dev < cost_dev_best:\n",
    "                #saver.save(sess,\"./bestfitcnn.dat\")\n",
    "                cost_dev_best = cost_dev\n",
    "            \n",
    "            # print output\n",
    "            if not epoch%print_every:\n",
    "                print(\"Epoch {}\".format(epoch))\n",
    "                print(\" - loss: {:6.4f} - acc: {:6.4f} - val_loss: {:6.4f} - val_acc: {:6.4f}\".format(cost_train,acc_train,cost_dev,acc_dev))\n",
    "                \n",
    "        return costs_train, costs_dev, accs_train, accs_dev, preds_train, preds_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOWeisSi1KFd"
   },
   "source": [
    "Run Lola run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 16884
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1183646,
     "status": "ok",
     "timestamp": 1527798682009,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "Qo29mOEx1KFe",
    "outputId": "cb3d743c-17fb-472e-c8dc-65999abdc473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      " - loss: 0.6949 - acc: 0.4669 - val_loss: 0.6445 - val_acc: 0.4711\n",
      "Epoch 1\n",
      " - loss: 0.6929 - acc: 0.5186 - val_loss: 0.6434 - val_acc: 0.5041\n",
      "Epoch 2\n",
      " - loss: 0.6910 - acc: 0.5496 - val_loss: 0.6409 - val_acc: 0.4876\n",
      "Epoch 3\n",
      " - loss: 0.6893 - acc: 0.5579 - val_loss: 0.6389 - val_acc: 0.5702\n",
      "Epoch 4\n",
      " - loss: 0.6868 - acc: 0.6136 - val_loss: 0.6388 - val_acc: 0.5207\n",
      "Epoch 5\n",
      " - loss: 0.6852 - acc: 0.6198 - val_loss: 0.6377 - val_acc: 0.5372\n",
      "Epoch 6\n",
      " - loss: 0.6834 - acc: 0.6095 - val_loss: 0.6352 - val_acc: 0.6033\n",
      "Epoch 7\n",
      " - loss: 0.6818 - acc: 0.6302 - val_loss: 0.6348 - val_acc: 0.5372\n",
      "Epoch 8\n",
      " - loss: 0.6802 - acc: 0.6364 - val_loss: 0.6328 - val_acc: 0.5868\n",
      "Epoch 9\n",
      " - loss: 0.6787 - acc: 0.6446 - val_loss: 0.6335 - val_acc: 0.5620\n",
      "Epoch 10\n",
      " - loss: 0.6761 - acc: 0.6322 - val_loss: 0.6301 - val_acc: 0.6033\n",
      "Epoch 11\n",
      " - loss: 0.6748 - acc: 0.6508 - val_loss: 0.6289 - val_acc: 0.5950\n",
      "Epoch 12\n",
      " - loss: 0.6730 - acc: 0.6756 - val_loss: 0.6277 - val_acc: 0.5868\n",
      "Epoch 13\n",
      " - loss: 0.6722 - acc: 0.6488 - val_loss: 0.6250 - val_acc: 0.6198\n",
      "Epoch 14\n",
      " - loss: 0.6701 - acc: 0.6736 - val_loss: 0.6259 - val_acc: 0.5785\n",
      "Epoch 15\n",
      " - loss: 0.6678 - acc: 0.6798 - val_loss: 0.6241 - val_acc: 0.6033\n",
      "Epoch 16\n",
      " - loss: 0.6656 - acc: 0.6550 - val_loss: 0.6224 - val_acc: 0.6364\n",
      "Epoch 17\n",
      " - loss: 0.6638 - acc: 0.6508 - val_loss: 0.6213 - val_acc: 0.6198\n",
      "Epoch 18\n",
      " - loss: 0.6624 - acc: 0.6715 - val_loss: 0.6197 - val_acc: 0.6446\n",
      "Epoch 19\n",
      " - loss: 0.6598 - acc: 0.6818 - val_loss: 0.6184 - val_acc: 0.6446\n",
      "Epoch 20\n",
      " - loss: 0.6584 - acc: 0.6632 - val_loss: 0.6162 - val_acc: 0.6281\n",
      "Epoch 21\n",
      " - loss: 0.6574 - acc: 0.6570 - val_loss: 0.6185 - val_acc: 0.5537\n",
      "Epoch 22\n",
      " - loss: 0.6554 - acc: 0.6653 - val_loss: 0.6170 - val_acc: 0.5868\n",
      "Epoch 23\n",
      " - loss: 0.6537 - acc: 0.6632 - val_loss: 0.6139 - val_acc: 0.6446\n",
      "Epoch 24\n",
      " - loss: 0.6515 - acc: 0.6777 - val_loss: 0.6109 - val_acc: 0.6364\n",
      "Epoch 25\n",
      " - loss: 0.6509 - acc: 0.6715 - val_loss: 0.6100 - val_acc: 0.6364\n",
      "Epoch 26\n",
      " - loss: 0.6487 - acc: 0.6901 - val_loss: 0.6121 - val_acc: 0.6116\n",
      "Epoch 27\n",
      " - loss: 0.6480 - acc: 0.6777 - val_loss: 0.6108 - val_acc: 0.6198\n",
      "Epoch 28\n",
      " - loss: 0.6456 - acc: 0.6818 - val_loss: 0.6074 - val_acc: 0.6446\n",
      "Epoch 29\n",
      " - loss: 0.6440 - acc: 0.6880 - val_loss: 0.6060 - val_acc: 0.6446\n",
      "Epoch 30\n",
      " - loss: 0.6432 - acc: 0.6715 - val_loss: 0.6039 - val_acc: 0.6446\n",
      "Epoch 31\n",
      " - loss: 0.6413 - acc: 0.6942 - val_loss: 0.6039 - val_acc: 0.6529\n",
      "Epoch 32\n",
      " - loss: 0.6400 - acc: 0.6880 - val_loss: 0.6026 - val_acc: 0.6612\n",
      "Epoch 33\n",
      " - loss: 0.6378 - acc: 0.6921 - val_loss: 0.6029 - val_acc: 0.6529\n",
      "Epoch 34\n",
      " - loss: 0.6374 - acc: 0.6818 - val_loss: 0.6005 - val_acc: 0.6612\n",
      "Epoch 35\n",
      " - loss: 0.6359 - acc: 0.6839 - val_loss: 0.5987 - val_acc: 0.6446\n",
      "Epoch 36\n",
      " - loss: 0.6347 - acc: 0.6860 - val_loss: 0.5976 - val_acc: 0.6446\n",
      "Epoch 37\n",
      " - loss: 0.6345 - acc: 0.6715 - val_loss: 0.5967 - val_acc: 0.6364\n",
      "Epoch 38\n",
      " - loss: 0.6330 - acc: 0.6839 - val_loss: 0.5967 - val_acc: 0.6612\n",
      "Epoch 39\n",
      " - loss: 0.6298 - acc: 0.6777 - val_loss: 0.5992 - val_acc: 0.6446\n",
      "Epoch 40\n",
      " - loss: 0.6290 - acc: 0.6983 - val_loss: 0.5960 - val_acc: 0.6529\n",
      "Epoch 41\n",
      " - loss: 0.6283 - acc: 0.6715 - val_loss: 0.5980 - val_acc: 0.6529\n",
      "Epoch 42\n",
      " - loss: 0.6275 - acc: 0.6798 - val_loss: 0.5943 - val_acc: 0.6446\n",
      "Epoch 43\n",
      " - loss: 0.6262 - acc: 0.6963 - val_loss: 0.5933 - val_acc: 0.6694\n",
      "Epoch 44\n",
      " - loss: 0.6245 - acc: 0.7128 - val_loss: 0.5921 - val_acc: 0.6694\n",
      "Epoch 45\n",
      " - loss: 0.6237 - acc: 0.6921 - val_loss: 0.5922 - val_acc: 0.6446\n",
      "Epoch 46\n",
      " - loss: 0.6224 - acc: 0.6963 - val_loss: 0.5903 - val_acc: 0.6612\n",
      "Epoch 47\n",
      " - loss: 0.6225 - acc: 0.6880 - val_loss: 0.5915 - val_acc: 0.6612\n",
      "Epoch 48\n",
      " - loss: 0.6195 - acc: 0.6963 - val_loss: 0.5880 - val_acc: 0.6529\n",
      "Epoch 49\n",
      " - loss: 0.6184 - acc: 0.6839 - val_loss: 0.5902 - val_acc: 0.6694\n",
      "Epoch 50\n",
      " - loss: 0.6185 - acc: 0.6983 - val_loss: 0.5913 - val_acc: 0.6612\n",
      "Epoch 51\n",
      " - loss: 0.6166 - acc: 0.6983 - val_loss: 0.5882 - val_acc: 0.6529\n",
      "Epoch 52\n",
      " - loss: 0.6153 - acc: 0.7107 - val_loss: 0.5856 - val_acc: 0.6694\n",
      "Epoch 53\n",
      " - loss: 0.6140 - acc: 0.7004 - val_loss: 0.5865 - val_acc: 0.6529\n",
      "Epoch 54\n",
      " - loss: 0.6134 - acc: 0.7025 - val_loss: 0.5851 - val_acc: 0.6942\n",
      "Epoch 55\n",
      " - loss: 0.6121 - acc: 0.7087 - val_loss: 0.5851 - val_acc: 0.6529\n",
      "Epoch 56\n",
      " - loss: 0.6113 - acc: 0.7004 - val_loss: 0.5829 - val_acc: 0.6694\n",
      "Epoch 57\n",
      " - loss: 0.6098 - acc: 0.7128 - val_loss: 0.5838 - val_acc: 0.6694\n",
      "Epoch 58\n",
      " - loss: 0.6106 - acc: 0.7211 - val_loss: 0.5830 - val_acc: 0.6777\n",
      "Epoch 59\n",
      " - loss: 0.6084 - acc: 0.7149 - val_loss: 0.5836 - val_acc: 0.6694\n",
      "Epoch 60\n",
      " - loss: 0.6084 - acc: 0.6901 - val_loss: 0.5891 - val_acc: 0.6446\n",
      "Epoch 61\n",
      " - loss: 0.6068 - acc: 0.7025 - val_loss: 0.5813 - val_acc: 0.6860\n",
      "Epoch 62\n",
      " - loss: 0.6052 - acc: 0.7004 - val_loss: 0.5886 - val_acc: 0.6529\n",
      "Epoch 63\n",
      " - loss: 0.6052 - acc: 0.7004 - val_loss: 0.5816 - val_acc: 0.6694\n",
      "Epoch 64\n",
      " - loss: 0.6039 - acc: 0.7107 - val_loss: 0.5808 - val_acc: 0.6777\n",
      "Epoch 65\n",
      " - loss: 0.6019 - acc: 0.7004 - val_loss: 0.5816 - val_acc: 0.6694\n",
      "Epoch 66\n",
      " - loss: 0.6015 - acc: 0.7107 - val_loss: 0.5778 - val_acc: 0.6942\n",
      "Epoch 67\n",
      " - loss: 0.6001 - acc: 0.7211 - val_loss: 0.5803 - val_acc: 0.6694\n",
      "Epoch 68\n",
      " - loss: 0.6005 - acc: 0.7025 - val_loss: 0.5792 - val_acc: 0.6777\n",
      "Epoch 69\n",
      " - loss: 0.5980 - acc: 0.7190 - val_loss: 0.5752 - val_acc: 0.6529\n",
      "Epoch 70\n",
      " - loss: 0.5983 - acc: 0.7128 - val_loss: 0.5774 - val_acc: 0.6860\n",
      "Epoch 71\n",
      " - loss: 0.5967 - acc: 0.7149 - val_loss: 0.5765 - val_acc: 0.6942\n",
      "Epoch 72\n",
      " - loss: 0.5954 - acc: 0.7190 - val_loss: 0.5792 - val_acc: 0.6777\n",
      "Epoch 73\n",
      " - loss: 0.5956 - acc: 0.7149 - val_loss: 0.5787 - val_acc: 0.6777\n",
      "Epoch 74\n",
      " - loss: 0.5943 - acc: 0.7231 - val_loss: 0.5764 - val_acc: 0.6777\n",
      "Epoch 75\n",
      " - loss: 0.5941 - acc: 0.7128 - val_loss: 0.5724 - val_acc: 0.6612\n",
      "Epoch 76\n",
      " - loss: 0.5930 - acc: 0.7211 - val_loss: 0.5769 - val_acc: 0.6777\n",
      "Epoch 77\n",
      " - loss: 0.5925 - acc: 0.7128 - val_loss: 0.5731 - val_acc: 0.7107\n",
      "Epoch 78\n",
      " - loss: 0.5910 - acc: 0.7190 - val_loss: 0.5748 - val_acc: 0.6942\n",
      "Epoch 79\n",
      " - loss: 0.5895 - acc: 0.7128 - val_loss: 0.5719 - val_acc: 0.7025\n",
      "Epoch 80\n",
      " - loss: 0.5890 - acc: 0.7087 - val_loss: 0.5739 - val_acc: 0.6942\n",
      "Epoch 81\n",
      " - loss: 0.5876 - acc: 0.7149 - val_loss: 0.5705 - val_acc: 0.7025\n",
      "Epoch 82\n",
      " - loss: 0.5877 - acc: 0.7190 - val_loss: 0.5717 - val_acc: 0.6942\n",
      "Epoch 83\n",
      " - loss: 0.5878 - acc: 0.7190 - val_loss: 0.5707 - val_acc: 0.7107\n",
      "Epoch 84\n",
      " - loss: 0.5848 - acc: 0.7211 - val_loss: 0.5684 - val_acc: 0.6777\n",
      "Epoch 85\n",
      " - loss: 0.5848 - acc: 0.7252 - val_loss: 0.5704 - val_acc: 0.7025\n",
      "Epoch 86\n",
      " - loss: 0.5834 - acc: 0.7025 - val_loss: 0.5677 - val_acc: 0.6860\n",
      "Epoch 87\n",
      " - loss: 0.5834 - acc: 0.7273 - val_loss: 0.5681 - val_acc: 0.7025\n",
      "Epoch 88\n",
      " - loss: 0.5825 - acc: 0.7335 - val_loss: 0.5675 - val_acc: 0.6942\n",
      "Epoch 89\n",
      " - loss: 0.5813 - acc: 0.7231 - val_loss: 0.5662 - val_acc: 0.6612\n",
      "Epoch 90\n",
      " - loss: 0.5796 - acc: 0.7293 - val_loss: 0.5680 - val_acc: 0.6942\n",
      "Epoch 91\n",
      " - loss: 0.5791 - acc: 0.7355 - val_loss: 0.5675 - val_acc: 0.7025\n",
      "Epoch 92\n",
      " - loss: 0.5784 - acc: 0.7293 - val_loss: 0.5681 - val_acc: 0.7190\n",
      "Epoch 93\n",
      " - loss: 0.5776 - acc: 0.7231 - val_loss: 0.5652 - val_acc: 0.6694\n",
      "Epoch 94\n",
      " - loss: 0.5771 - acc: 0.7335 - val_loss: 0.5705 - val_acc: 0.6942\n",
      "Epoch 95\n",
      " - loss: 0.5763 - acc: 0.7314 - val_loss: 0.5667 - val_acc: 0.7107\n",
      "Epoch 96\n",
      " - loss: 0.5754 - acc: 0.7314 - val_loss: 0.5633 - val_acc: 0.6364\n",
      "Epoch 97\n",
      " - loss: 0.5740 - acc: 0.7355 - val_loss: 0.5649 - val_acc: 0.7107\n",
      "Epoch 98\n",
      " - loss: 0.5744 - acc: 0.7335 - val_loss: 0.5627 - val_acc: 0.6446\n",
      "Epoch 99\n",
      " - loss: 0.5728 - acc: 0.7252 - val_loss: 0.5638 - val_acc: 0.6942\n",
      "Epoch 100\n",
      " - loss: 0.5721 - acc: 0.7397 - val_loss: 0.5649 - val_acc: 0.7107\n",
      "Epoch 101\n",
      " - loss: 0.5719 - acc: 0.7190 - val_loss: 0.5662 - val_acc: 0.6942\n",
      "Epoch 102\n",
      " - loss: 0.5698 - acc: 0.7417 - val_loss: 0.5620 - val_acc: 0.6694\n",
      "Epoch 103\n",
      " - loss: 0.5696 - acc: 0.7355 - val_loss: 0.5637 - val_acc: 0.6942\n",
      "Epoch 104\n",
      " - loss: 0.5680 - acc: 0.7355 - val_loss: 0.5652 - val_acc: 0.6942\n",
      "Epoch 105\n",
      " - loss: 0.5680 - acc: 0.7293 - val_loss: 0.5616 - val_acc: 0.6942\n",
      "Epoch 106\n",
      " - loss: 0.5680 - acc: 0.7479 - val_loss: 0.5622 - val_acc: 0.6860\n",
      "Epoch 107\n",
      " - loss: 0.5671 - acc: 0.7273 - val_loss: 0.5621 - val_acc: 0.6860\n",
      "Epoch 108\n",
      " - loss: 0.5658 - acc: 0.7438 - val_loss: 0.5642 - val_acc: 0.7025\n",
      "Epoch 109\n",
      " - loss: 0.5698 - acc: 0.7314 - val_loss: 0.5653 - val_acc: 0.6942\n",
      "Epoch 110\n",
      " - loss: 0.5663 - acc: 0.7397 - val_loss: 0.5619 - val_acc: 0.7025\n",
      "Epoch 111\n",
      " - loss: 0.5642 - acc: 0.7355 - val_loss: 0.5612 - val_acc: 0.6942\n",
      "Epoch 112\n",
      " - loss: 0.5626 - acc: 0.7335 - val_loss: 0.5620 - val_acc: 0.7025\n",
      "Epoch 113\n",
      " - loss: 0.5616 - acc: 0.7438 - val_loss: 0.5601 - val_acc: 0.6860\n",
      "Epoch 114\n",
      " - loss: 0.5612 - acc: 0.7397 - val_loss: 0.5579 - val_acc: 0.6446\n",
      "Epoch 115\n",
      " - loss: 0.5603 - acc: 0.7376 - val_loss: 0.5581 - val_acc: 0.6694\n",
      "Epoch 116\n",
      " - loss: 0.5597 - acc: 0.7397 - val_loss: 0.5577 - val_acc: 0.6694\n",
      "Epoch 117\n",
      " - loss: 0.5572 - acc: 0.7459 - val_loss: 0.5635 - val_acc: 0.7025\n",
      "Epoch 118\n",
      " - loss: 0.5619 - acc: 0.7355 - val_loss: 0.5629 - val_acc: 0.7025\n",
      "Epoch 119\n",
      " - loss: 0.5572 - acc: 0.7459 - val_loss: 0.5638 - val_acc: 0.6777\n",
      "Epoch 120\n",
      " - loss: 0.5586 - acc: 0.7438 - val_loss: 0.5579 - val_acc: 0.6942\n",
      "Epoch 121\n",
      " - loss: 0.5560 - acc: 0.7479 - val_loss: 0.5576 - val_acc: 0.6942\n",
      "Epoch 122\n",
      " - loss: 0.5557 - acc: 0.7438 - val_loss: 0.5554 - val_acc: 0.6529\n",
      "Epoch 123\n",
      " - loss: 0.5547 - acc: 0.7314 - val_loss: 0.5593 - val_acc: 0.6942\n",
      "Epoch 124\n",
      " - loss: 0.5562 - acc: 0.7438 - val_loss: 0.5553 - val_acc: 0.6446\n",
      "Epoch 125\n",
      " - loss: 0.5533 - acc: 0.7397 - val_loss: 0.5581 - val_acc: 0.6860\n",
      "Epoch 126\n",
      " - loss: 0.5521 - acc: 0.7562 - val_loss: 0.5570 - val_acc: 0.6942\n",
      "Epoch 127\n",
      " - loss: 0.5518 - acc: 0.7541 - val_loss: 0.5541 - val_acc: 0.6529\n",
      "Epoch 128\n",
      " - loss: 0.5507 - acc: 0.7438 - val_loss: 0.5541 - val_acc: 0.6612\n",
      "Epoch 129\n",
      " - loss: 0.5507 - acc: 0.7397 - val_loss: 0.5582 - val_acc: 0.6942\n",
      "Epoch 130\n",
      " - loss: 0.5501 - acc: 0.7541 - val_loss: 0.5548 - val_acc: 0.6942\n",
      "Epoch 131\n",
      " - loss: 0.5494 - acc: 0.7541 - val_loss: 0.5541 - val_acc: 0.6942\n",
      "Epoch 132\n",
      " - loss: 0.5489 - acc: 0.7603 - val_loss: 0.5553 - val_acc: 0.6860\n",
      "Epoch 133\n",
      " - loss: 0.5475 - acc: 0.7438 - val_loss: 0.5565 - val_acc: 0.6942\n",
      "Epoch 134\n",
      " - loss: 0.5455 - acc: 0.7665 - val_loss: 0.5627 - val_acc: 0.6529\n",
      "Epoch 135\n",
      " - loss: 0.5482 - acc: 0.7583 - val_loss: 0.5569 - val_acc: 0.7025\n",
      "Epoch 136\n",
      " - loss: 0.5457 - acc: 0.7541 - val_loss: 0.5525 - val_acc: 0.6694\n",
      "Epoch 137\n",
      " - loss: 0.5440 - acc: 0.7562 - val_loss: 0.5614 - val_acc: 0.6694\n",
      "Epoch 138\n",
      " - loss: 0.5460 - acc: 0.7479 - val_loss: 0.5534 - val_acc: 0.6860\n",
      "Epoch 139\n",
      " - loss: 0.5434 - acc: 0.7583 - val_loss: 0.5519 - val_acc: 0.6694\n",
      "Epoch 140\n",
      " - loss: 0.5422 - acc: 0.7645 - val_loss: 0.5515 - val_acc: 0.6694\n",
      "Epoch 141\n",
      " - loss: 0.5409 - acc: 0.7562 - val_loss: 0.5564 - val_acc: 0.6942\n",
      "Epoch 142\n",
      " - loss: 0.5410 - acc: 0.7479 - val_loss: 0.5506 - val_acc: 0.6446\n",
      "Epoch 143\n",
      " - loss: 0.5422 - acc: 0.7583 - val_loss: 0.5538 - val_acc: 0.6860\n",
      "Epoch 144\n",
      " - loss: 0.5411 - acc: 0.7645 - val_loss: 0.5551 - val_acc: 0.6942\n",
      "Epoch 145\n",
      " - loss: 0.5425 - acc: 0.7521 - val_loss: 0.5547 - val_acc: 0.7025\n",
      "Epoch 146\n",
      " - loss: 0.5389 - acc: 0.7645 - val_loss: 0.5622 - val_acc: 0.6529\n",
      "Epoch 147\n",
      " - loss: 0.5390 - acc: 0.7645 - val_loss: 0.5524 - val_acc: 0.6777\n",
      "Epoch 148\n",
      " - loss: 0.5372 - acc: 0.7521 - val_loss: 0.5533 - val_acc: 0.6942\n",
      "Epoch 149\n",
      " - loss: 0.5375 - acc: 0.7459 - val_loss: 0.5534 - val_acc: 0.6942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      " - loss: 0.5375 - acc: 0.7562 - val_loss: 0.5500 - val_acc: 0.6942\n",
      "Epoch 151\n",
      " - loss: 0.5348 - acc: 0.7665 - val_loss: 0.5513 - val_acc: 0.6777\n",
      "Epoch 152\n",
      " - loss: 0.5344 - acc: 0.7583 - val_loss: 0.5495 - val_acc: 0.6860\n",
      "Epoch 153\n",
      " - loss: 0.5318 - acc: 0.7583 - val_loss: 0.5564 - val_acc: 0.6777\n",
      "Epoch 154\n",
      " - loss: 0.5340 - acc: 0.7541 - val_loss: 0.5529 - val_acc: 0.6860\n",
      "Epoch 155\n",
      " - loss: 0.5326 - acc: 0.7562 - val_loss: 0.5500 - val_acc: 0.6860\n",
      "Epoch 156\n",
      " - loss: 0.5331 - acc: 0.7748 - val_loss: 0.5488 - val_acc: 0.6860\n",
      "Epoch 157\n",
      " - loss: 0.5308 - acc: 0.7686 - val_loss: 0.5529 - val_acc: 0.6860\n",
      "Epoch 158\n",
      " - loss: 0.5309 - acc: 0.7562 - val_loss: 0.5509 - val_acc: 0.6942\n",
      "Epoch 159\n",
      " - loss: 0.5297 - acc: 0.7665 - val_loss: 0.5503 - val_acc: 0.6942\n",
      "Epoch 160\n",
      " - loss: 0.5319 - acc: 0.7603 - val_loss: 0.5492 - val_acc: 0.6860\n",
      "Epoch 161\n",
      " - loss: 0.5295 - acc: 0.7707 - val_loss: 0.5509 - val_acc: 0.6860\n",
      "Epoch 162\n",
      " - loss: 0.5283 - acc: 0.7748 - val_loss: 0.5471 - val_acc: 0.6777\n",
      "Epoch 163\n",
      " - loss: 0.5276 - acc: 0.7665 - val_loss: 0.5488 - val_acc: 0.6860\n",
      "Epoch 164\n",
      " - loss: 0.5278 - acc: 0.7707 - val_loss: 0.5489 - val_acc: 0.6942\n",
      "Epoch 165\n",
      " - loss: 0.5266 - acc: 0.7603 - val_loss: 0.5520 - val_acc: 0.6860\n",
      "Epoch 166\n",
      " - loss: 0.5266 - acc: 0.7748 - val_loss: 0.5501 - val_acc: 0.6860\n",
      "Epoch 167\n",
      " - loss: 0.5248 - acc: 0.7748 - val_loss: 0.5515 - val_acc: 0.6860\n",
      "Epoch 168\n",
      " - loss: 0.5240 - acc: 0.7893 - val_loss: 0.5460 - val_acc: 0.6612\n",
      "Epoch 169\n",
      " - loss: 0.5234 - acc: 0.7893 - val_loss: 0.5479 - val_acc: 0.6942\n",
      "Epoch 170\n",
      " - loss: 0.5221 - acc: 0.7707 - val_loss: 0.5485 - val_acc: 0.7025\n",
      "Epoch 171\n",
      " - loss: 0.5220 - acc: 0.7665 - val_loss: 0.5478 - val_acc: 0.6942\n",
      "Epoch 172\n",
      " - loss: 0.5244 - acc: 0.7686 - val_loss: 0.5531 - val_acc: 0.6694\n",
      "Epoch 173\n",
      " - loss: 0.5220 - acc: 0.7831 - val_loss: 0.5481 - val_acc: 0.7025\n",
      "Epoch 174\n",
      " - loss: 0.5217 - acc: 0.7748 - val_loss: 0.5498 - val_acc: 0.6860\n",
      "Epoch 175\n",
      " - loss: 0.5213 - acc: 0.7769 - val_loss: 0.5499 - val_acc: 0.6860\n",
      "Epoch 176\n",
      " - loss: 0.5221 - acc: 0.7748 - val_loss: 0.5460 - val_acc: 0.6860\n",
      "Epoch 177\n",
      " - loss: 0.5190 - acc: 0.7727 - val_loss: 0.5523 - val_acc: 0.6694\n",
      "Epoch 178\n",
      " - loss: 0.5190 - acc: 0.7810 - val_loss: 0.5507 - val_acc: 0.6777\n",
      "Epoch 179\n",
      " - loss: 0.5177 - acc: 0.7769 - val_loss: 0.5445 - val_acc: 0.6694\n",
      "Epoch 180\n",
      " - loss: 0.5180 - acc: 0.7975 - val_loss: 0.5461 - val_acc: 0.6860\n",
      "Epoch 181\n",
      " - loss: 0.5181 - acc: 0.7934 - val_loss: 0.5525 - val_acc: 0.6694\n",
      "Epoch 182\n",
      " - loss: 0.5148 - acc: 0.7810 - val_loss: 0.5451 - val_acc: 0.6860\n",
      "Epoch 183\n",
      " - loss: 0.5159 - acc: 0.7748 - val_loss: 0.5448 - val_acc: 0.6860\n",
      "Epoch 184\n",
      " - loss: 0.5133 - acc: 0.7934 - val_loss: 0.5519 - val_acc: 0.6694\n",
      "Epoch 185\n",
      " - loss: 0.5158 - acc: 0.7769 - val_loss: 0.5507 - val_acc: 0.6694\n",
      "Epoch 186\n",
      " - loss: 0.5125 - acc: 0.7872 - val_loss: 0.5517 - val_acc: 0.6694\n",
      "Epoch 187\n",
      " - loss: 0.5121 - acc: 0.7913 - val_loss: 0.5449 - val_acc: 0.6860\n",
      "Epoch 188\n",
      " - loss: 0.5112 - acc: 0.7769 - val_loss: 0.5556 - val_acc: 0.6612\n",
      "Epoch 189\n",
      " - loss: 0.5123 - acc: 0.7893 - val_loss: 0.5507 - val_acc: 0.6694\n",
      "Epoch 190\n",
      " - loss: 0.5102 - acc: 0.7851 - val_loss: 0.5446 - val_acc: 0.6860\n",
      "Epoch 191\n",
      " - loss: 0.5118 - acc: 0.7831 - val_loss: 0.5449 - val_acc: 0.6860\n",
      "Epoch 192\n",
      " - loss: 0.5111 - acc: 0.7872 - val_loss: 0.5482 - val_acc: 0.6942\n",
      "Epoch 193\n",
      " - loss: 0.5094 - acc: 0.7893 - val_loss: 0.5470 - val_acc: 0.6942\n",
      "Epoch 194\n",
      " - loss: 0.5101 - acc: 0.7872 - val_loss: 0.5460 - val_acc: 0.6942\n",
      "Epoch 195\n",
      " - loss: 0.5081 - acc: 0.7789 - val_loss: 0.5432 - val_acc: 0.6942\n",
      "Epoch 196\n",
      " - loss: 0.5064 - acc: 0.7913 - val_loss: 0.5522 - val_acc: 0.6612\n",
      "Epoch 197\n",
      " - loss: 0.5072 - acc: 0.7975 - val_loss: 0.5457 - val_acc: 0.6942\n",
      "Epoch 198\n",
      " - loss: 0.5059 - acc: 0.7893 - val_loss: 0.5453 - val_acc: 0.6942\n",
      "Epoch 199\n",
      " - loss: 0.5051 - acc: 0.7955 - val_loss: 0.5486 - val_acc: 0.6777\n",
      "Epoch 200\n",
      " - loss: 0.5054 - acc: 0.7872 - val_loss: 0.5422 - val_acc: 0.6612\n",
      "Epoch 201\n",
      " - loss: 0.5041 - acc: 0.7810 - val_loss: 0.5478 - val_acc: 0.6777\n",
      "Epoch 202\n",
      " - loss: 0.5027 - acc: 0.7913 - val_loss: 0.5421 - val_acc: 0.6612\n",
      "Epoch 203\n",
      " - loss: 0.5034 - acc: 0.7831 - val_loss: 0.5427 - val_acc: 0.7025\n",
      "Epoch 204\n",
      " - loss: 0.5025 - acc: 0.7955 - val_loss: 0.5460 - val_acc: 0.6942\n",
      "Epoch 205\n",
      " - loss: 0.5024 - acc: 0.7872 - val_loss: 0.5432 - val_acc: 0.6777\n",
      "Epoch 206\n",
      " - loss: 0.5022 - acc: 0.7851 - val_loss: 0.5457 - val_acc: 0.6942\n",
      "Epoch 207\n",
      " - loss: 0.5004 - acc: 0.8058 - val_loss: 0.5450 - val_acc: 0.6942\n",
      "Epoch 208\n",
      " - loss: 0.5004 - acc: 0.7872 - val_loss: 0.5453 - val_acc: 0.6942\n",
      "Epoch 209\n",
      " - loss: 0.5022 - acc: 0.7934 - val_loss: 0.5417 - val_acc: 0.6694\n",
      "Epoch 210\n",
      " - loss: 0.5003 - acc: 0.7934 - val_loss: 0.5483 - val_acc: 0.6777\n",
      "Epoch 211\n",
      " - loss: 0.4984 - acc: 0.7893 - val_loss: 0.5413 - val_acc: 0.6612\n",
      "Epoch 212\n",
      " - loss: 0.4991 - acc: 0.8058 - val_loss: 0.5455 - val_acc: 0.6942\n",
      "Epoch 213\n",
      " - loss: 0.4981 - acc: 0.7975 - val_loss: 0.5411 - val_acc: 0.6612\n",
      "Epoch 214\n",
      " - loss: 0.4960 - acc: 0.8058 - val_loss: 0.5508 - val_acc: 0.6612\n",
      "Epoch 215\n",
      " - loss: 0.4979 - acc: 0.7934 - val_loss: 0.5465 - val_acc: 0.6777\n",
      "Epoch 216\n",
      " - loss: 0.4946 - acc: 0.7996 - val_loss: 0.5410 - val_acc: 0.6694\n",
      "Epoch 217\n",
      " - loss: 0.4929 - acc: 0.8099 - val_loss: 0.5547 - val_acc: 0.6529\n",
      "Epoch 218\n",
      " - loss: 0.4965 - acc: 0.7769 - val_loss: 0.5440 - val_acc: 0.6942\n",
      "Epoch 219\n",
      " - loss: 0.4949 - acc: 0.7996 - val_loss: 0.5411 - val_acc: 0.7025\n",
      "Epoch 220\n",
      " - loss: 0.4944 - acc: 0.7913 - val_loss: 0.5418 - val_acc: 0.6942\n",
      "Epoch 221\n",
      " - loss: 0.4933 - acc: 0.7872 - val_loss: 0.5407 - val_acc: 0.6529\n",
      "Epoch 222\n",
      " - loss: 0.4910 - acc: 0.7934 - val_loss: 0.5596 - val_acc: 0.6446\n",
      "Epoch 223\n",
      " - loss: 0.4958 - acc: 0.7955 - val_loss: 0.5435 - val_acc: 0.6942\n",
      "Epoch 224\n",
      " - loss: 0.4918 - acc: 0.7872 - val_loss: 0.5449 - val_acc: 0.6860\n",
      "Epoch 225\n",
      " - loss: 0.4908 - acc: 0.8017 - val_loss: 0.5487 - val_acc: 0.6777\n",
      "Epoch 226\n",
      " - loss: 0.4908 - acc: 0.8058 - val_loss: 0.5415 - val_acc: 0.6942\n",
      "Epoch 227\n",
      " - loss: 0.4912 - acc: 0.8017 - val_loss: 0.5403 - val_acc: 0.6694\n",
      "Epoch 228\n",
      " - loss: 0.4907 - acc: 0.8079 - val_loss: 0.5430 - val_acc: 0.6942\n",
      "Epoch 229\n",
      " - loss: 0.4889 - acc: 0.8037 - val_loss: 0.5436 - val_acc: 0.6942\n",
      "Epoch 230\n",
      " - loss: 0.4876 - acc: 0.8058 - val_loss: 0.5443 - val_acc: 0.6860\n",
      "Epoch 231\n",
      " - loss: 0.4874 - acc: 0.8017 - val_loss: 0.5502 - val_acc: 0.6777\n",
      "Epoch 232\n",
      " - loss: 0.4877 - acc: 0.7934 - val_loss: 0.5413 - val_acc: 0.6860\n",
      "Epoch 233\n",
      " - loss: 0.4869 - acc: 0.8037 - val_loss: 0.5462 - val_acc: 0.6777\n",
      "Epoch 234\n",
      " - loss: 0.4875 - acc: 0.8079 - val_loss: 0.5406 - val_acc: 0.7025\n",
      "Epoch 235\n",
      " - loss: 0.4873 - acc: 0.8140 - val_loss: 0.5398 - val_acc: 0.6694\n",
      "Epoch 236\n",
      " - loss: 0.4855 - acc: 0.8079 - val_loss: 0.5403 - val_acc: 0.7107\n",
      "Epoch 237\n",
      " - loss: 0.4844 - acc: 0.8120 - val_loss: 0.5398 - val_acc: 0.6694\n",
      "Epoch 238\n",
      " - loss: 0.4824 - acc: 0.8140 - val_loss: 0.5453 - val_acc: 0.6777\n",
      "Epoch 239\n",
      " - loss: 0.4847 - acc: 0.8017 - val_loss: 0.5418 - val_acc: 0.6860\n",
      "Epoch 240\n",
      " - loss: 0.4832 - acc: 0.8079 - val_loss: 0.5432 - val_acc: 0.6860\n",
      "Epoch 241\n",
      " - loss: 0.4824 - acc: 0.8079 - val_loss: 0.5474 - val_acc: 0.6860\n",
      "Epoch 242\n",
      " - loss: 0.4834 - acc: 0.7934 - val_loss: 0.5461 - val_acc: 0.6777\n",
      "Epoch 243\n",
      " - loss: 0.4828 - acc: 0.8017 - val_loss: 0.5485 - val_acc: 0.6777\n",
      "Epoch 244\n",
      " - loss: 0.4817 - acc: 0.8120 - val_loss: 0.5410 - val_acc: 0.6860\n",
      "Epoch 245\n",
      " - loss: 0.4799 - acc: 0.8058 - val_loss: 0.5421 - val_acc: 0.6860\n",
      "Epoch 246\n",
      " - loss: 0.4798 - acc: 0.8099 - val_loss: 0.5410 - val_acc: 0.6860\n",
      "Epoch 247\n",
      " - loss: 0.4802 - acc: 0.8037 - val_loss: 0.5415 - val_acc: 0.6942\n",
      "Epoch 248\n",
      " - loss: 0.4775 - acc: 0.8223 - val_loss: 0.5465 - val_acc: 0.6860\n",
      "Epoch 249\n",
      " - loss: 0.4781 - acc: 0.7996 - val_loss: 0.5400 - val_acc: 0.7025\n",
      "Epoch 250\n",
      " - loss: 0.4791 - acc: 0.8161 - val_loss: 0.5436 - val_acc: 0.6777\n",
      "Epoch 251\n",
      " - loss: 0.4767 - acc: 0.8140 - val_loss: 0.5440 - val_acc: 0.6777\n",
      "Epoch 252\n",
      " - loss: 0.4756 - acc: 0.8017 - val_loss: 0.5396 - val_acc: 0.6612\n",
      "Epoch 253\n",
      " - loss: 0.4769 - acc: 0.8120 - val_loss: 0.5398 - val_acc: 0.6942\n",
      "Epoch 254\n",
      " - loss: 0.4773 - acc: 0.8058 - val_loss: 0.5409 - val_acc: 0.6860\n",
      "Epoch 255\n",
      " - loss: 0.4762 - acc: 0.8120 - val_loss: 0.5496 - val_acc: 0.6694\n",
      "Epoch 256\n",
      " - loss: 0.4753 - acc: 0.8244 - val_loss: 0.5429 - val_acc: 0.6860\n",
      "Epoch 257\n",
      " - loss: 0.4745 - acc: 0.8017 - val_loss: 0.5398 - val_acc: 0.7025\n",
      "Epoch 258\n",
      " - loss: 0.4726 - acc: 0.8223 - val_loss: 0.5472 - val_acc: 0.6777\n",
      "Epoch 259\n",
      " - loss: 0.4733 - acc: 0.8120 - val_loss: 0.5397 - val_acc: 0.6529\n",
      "Epoch 260\n",
      " - loss: 0.4728 - acc: 0.8182 - val_loss: 0.5410 - val_acc: 0.6860\n",
      "Epoch 261\n",
      " - loss: 0.4722 - acc: 0.8182 - val_loss: 0.5402 - val_acc: 0.6860\n",
      "Epoch 262\n",
      " - loss: 0.4722 - acc: 0.8223 - val_loss: 0.5393 - val_acc: 0.6942\n",
      "Epoch 263\n",
      " - loss: 0.4721 - acc: 0.8182 - val_loss: 0.5411 - val_acc: 0.6860\n",
      "Epoch 264\n",
      " - loss: 0.4710 - acc: 0.8223 - val_loss: 0.5443 - val_acc: 0.6860\n",
      "Epoch 265\n",
      " - loss: 0.4722 - acc: 0.8058 - val_loss: 0.5403 - val_acc: 0.6860\n",
      "Epoch 266\n",
      " - loss: 0.4681 - acc: 0.8264 - val_loss: 0.5528 - val_acc: 0.6612\n",
      "Epoch 267\n",
      " - loss: 0.4717 - acc: 0.8140 - val_loss: 0.5434 - val_acc: 0.6860\n",
      "Epoch 268\n",
      " - loss: 0.4682 - acc: 0.8264 - val_loss: 0.5394 - val_acc: 0.6694\n",
      "Epoch 269\n",
      " - loss: 0.4696 - acc: 0.8120 - val_loss: 0.5425 - val_acc: 0.6942\n",
      "Epoch 270\n",
      " - loss: 0.4709 - acc: 0.8161 - val_loss: 0.5428 - val_acc: 0.6860\n",
      "Epoch 271\n",
      " - loss: 0.4681 - acc: 0.8161 - val_loss: 0.5395 - val_acc: 0.6942\n",
      "Epoch 272\n",
      " - loss: 0.4681 - acc: 0.8202 - val_loss: 0.5424 - val_acc: 0.6942\n",
      "Epoch 273\n",
      " - loss: 0.4690 - acc: 0.8140 - val_loss: 0.5422 - val_acc: 0.6860\n",
      "Epoch 274\n",
      " - loss: 0.4657 - acc: 0.8244 - val_loss: 0.5401 - val_acc: 0.6612\n",
      "Epoch 275\n",
      " - loss: 0.4690 - acc: 0.8120 - val_loss: 0.5393 - val_acc: 0.6860\n",
      "Epoch 276\n",
      " - loss: 0.4689 - acc: 0.8285 - val_loss: 0.5393 - val_acc: 0.6777\n",
      "Epoch 277\n",
      " - loss: 0.4654 - acc: 0.8223 - val_loss: 0.5393 - val_acc: 0.6694\n",
      "Epoch 278\n",
      " - loss: 0.4633 - acc: 0.8182 - val_loss: 0.5627 - val_acc: 0.6612\n",
      "Epoch 279\n",
      " - loss: 0.4647 - acc: 0.8182 - val_loss: 0.5407 - val_acc: 0.6777\n",
      "Epoch 280\n",
      " - loss: 0.4641 - acc: 0.8202 - val_loss: 0.5400 - val_acc: 0.7025\n",
      "Epoch 281\n",
      " - loss: 0.4605 - acc: 0.8223 - val_loss: 0.5535 - val_acc: 0.6612\n",
      "Epoch 282\n",
      " - loss: 0.4627 - acc: 0.8161 - val_loss: 0.5443 - val_acc: 0.6860\n",
      "Epoch 283\n",
      " - loss: 0.4623 - acc: 0.8202 - val_loss: 0.5474 - val_acc: 0.6694\n",
      "Epoch 284\n",
      " - loss: 0.4585 - acc: 0.8264 - val_loss: 0.5399 - val_acc: 0.6612\n",
      "Epoch 285\n",
      " - loss: 0.4630 - acc: 0.8223 - val_loss: 0.5445 - val_acc: 0.6777\n",
      "Epoch 286\n",
      " - loss: 0.4604 - acc: 0.8347 - val_loss: 0.5392 - val_acc: 0.6942\n",
      "Epoch 287\n",
      " - loss: 0.4609 - acc: 0.8099 - val_loss: 0.5396 - val_acc: 0.7025\n",
      "Epoch 288\n",
      " - loss: 0.4610 - acc: 0.8285 - val_loss: 0.5470 - val_acc: 0.6777\n",
      "Epoch 289\n",
      " - loss: 0.4608 - acc: 0.8306 - val_loss: 0.5399 - val_acc: 0.6694\n",
      "Epoch 290\n",
      " - loss: 0.4595 - acc: 0.8285 - val_loss: 0.5397 - val_acc: 0.7107\n",
      "Epoch 291\n",
      " - loss: 0.4567 - acc: 0.8182 - val_loss: 0.5437 - val_acc: 0.6860\n",
      "Epoch 292\n",
      " - loss: 0.4597 - acc: 0.8161 - val_loss: 0.5411 - val_acc: 0.6860\n",
      "Epoch 293\n",
      " - loss: 0.4573 - acc: 0.8244 - val_loss: 0.5400 - val_acc: 0.6942\n",
      "Epoch 294\n",
      " - loss: 0.4553 - acc: 0.8182 - val_loss: 0.5565 - val_acc: 0.6694\n",
      "Epoch 295\n",
      " - loss: 0.4592 - acc: 0.8306 - val_loss: 0.5396 - val_acc: 0.7107\n",
      "Epoch 296\n",
      " - loss: 0.4562 - acc: 0.8285 - val_loss: 0.5394 - val_acc: 0.6777\n",
      "Epoch 297\n",
      " - loss: 0.4563 - acc: 0.8244 - val_loss: 0.5421 - val_acc: 0.6777\n",
      "Epoch 298\n",
      " - loss: 0.4543 - acc: 0.8182 - val_loss: 0.5463 - val_acc: 0.6777\n",
      "Epoch 299\n",
      " - loss: 0.4565 - acc: 0.8223 - val_loss: 0.5493 - val_acc: 0.6694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n",
      " - loss: 0.4539 - acc: 0.8202 - val_loss: 0.5431 - val_acc: 0.6694\n",
      "Epoch 301\n",
      " - loss: 0.4537 - acc: 0.8202 - val_loss: 0.5394 - val_acc: 0.7025\n",
      "Epoch 302\n",
      " - loss: 0.4529 - acc: 0.8285 - val_loss: 0.5437 - val_acc: 0.6694\n",
      "Epoch 303\n",
      " - loss: 0.4537 - acc: 0.8223 - val_loss: 0.5395 - val_acc: 0.7025\n",
      "Epoch 304\n",
      " - loss: 0.4531 - acc: 0.8264 - val_loss: 0.5435 - val_acc: 0.6694\n",
      "Epoch 305\n",
      " - loss: 0.4555 - acc: 0.8161 - val_loss: 0.5411 - val_acc: 0.6777\n",
      "Epoch 306\n",
      " - loss: 0.4532 - acc: 0.8264 - val_loss: 0.5421 - val_acc: 0.6777\n",
      "Epoch 307\n",
      " - loss: 0.4512 - acc: 0.8244 - val_loss: 0.5411 - val_acc: 0.6777\n",
      "Epoch 308\n",
      " - loss: 0.4509 - acc: 0.8285 - val_loss: 0.5443 - val_acc: 0.6694\n",
      "Epoch 309\n",
      " - loss: 0.4516 - acc: 0.8306 - val_loss: 0.5408 - val_acc: 0.6860\n",
      "Epoch 310\n",
      " - loss: 0.4514 - acc: 0.8285 - val_loss: 0.5501 - val_acc: 0.6612\n",
      "Epoch 311\n",
      " - loss: 0.4486 - acc: 0.8244 - val_loss: 0.5529 - val_acc: 0.6612\n",
      "Epoch 312\n",
      " - loss: 0.4493 - acc: 0.8347 - val_loss: 0.5400 - val_acc: 0.7025\n",
      "Epoch 313\n",
      " - loss: 0.4486 - acc: 0.8306 - val_loss: 0.5407 - val_acc: 0.6942\n",
      "Epoch 314\n",
      " - loss: 0.4473 - acc: 0.8326 - val_loss: 0.5401 - val_acc: 0.7107\n",
      "Epoch 315\n",
      " - loss: 0.4488 - acc: 0.8202 - val_loss: 0.5452 - val_acc: 0.6777\n",
      "Epoch 316\n",
      " - loss: 0.4471 - acc: 0.8409 - val_loss: 0.5548 - val_acc: 0.6694\n",
      "Epoch 317\n",
      " - loss: 0.4477 - acc: 0.8223 - val_loss: 0.5409 - val_acc: 0.6942\n",
      "Epoch 318\n",
      " - loss: 0.4473 - acc: 0.8244 - val_loss: 0.5408 - val_acc: 0.6942\n",
      "Epoch 319\n",
      " - loss: 0.4460 - acc: 0.8471 - val_loss: 0.5408 - val_acc: 0.7025\n",
      "Epoch 320\n",
      " - loss: 0.4462 - acc: 0.8409 - val_loss: 0.5402 - val_acc: 0.7190\n",
      "Epoch 321\n",
      " - loss: 0.4453 - acc: 0.8306 - val_loss: 0.5444 - val_acc: 0.6694\n",
      "Epoch 322\n",
      " - loss: 0.4511 - acc: 0.8223 - val_loss: 0.5423 - val_acc: 0.6777\n",
      "Epoch 323\n",
      " - loss: 0.4443 - acc: 0.8326 - val_loss: 0.5419 - val_acc: 0.6694\n",
      "Epoch 324\n",
      " - loss: 0.4417 - acc: 0.8450 - val_loss: 0.5408 - val_acc: 0.6777\n",
      "Epoch 325\n",
      " - loss: 0.4440 - acc: 0.8347 - val_loss: 0.5421 - val_acc: 0.6694\n",
      "Epoch 326\n",
      " - loss: 0.4429 - acc: 0.8388 - val_loss: 0.5429 - val_acc: 0.6694\n",
      "Epoch 327\n",
      " - loss: 0.4415 - acc: 0.8285 - val_loss: 0.5405 - val_acc: 0.6860\n",
      "Epoch 328\n",
      " - loss: 0.4424 - acc: 0.8347 - val_loss: 0.5407 - val_acc: 0.7025\n",
      "Epoch 329\n",
      " - loss: 0.4435 - acc: 0.8347 - val_loss: 0.5504 - val_acc: 0.6694\n",
      "Epoch 330\n",
      " - loss: 0.4420 - acc: 0.8388 - val_loss: 0.5467 - val_acc: 0.6777\n",
      "Epoch 331\n",
      " - loss: 0.4414 - acc: 0.8306 - val_loss: 0.5410 - val_acc: 0.7107\n",
      "Epoch 332\n",
      " - loss: 0.4410 - acc: 0.8326 - val_loss: 0.5448 - val_acc: 0.6694\n",
      "Epoch 333\n",
      " - loss: 0.4393 - acc: 0.8347 - val_loss: 0.5428 - val_acc: 0.6694\n",
      "Epoch 334\n",
      " - loss: 0.4398 - acc: 0.8326 - val_loss: 0.5415 - val_acc: 0.6942\n",
      "Epoch 335\n",
      " - loss: 0.4384 - acc: 0.8409 - val_loss: 0.5491 - val_acc: 0.6694\n",
      "Epoch 336\n",
      " - loss: 0.4396 - acc: 0.8409 - val_loss: 0.5428 - val_acc: 0.6694\n",
      "Epoch 337\n",
      " - loss: 0.4382 - acc: 0.8388 - val_loss: 0.5422 - val_acc: 0.6777\n",
      "Epoch 338\n",
      " - loss: 0.4373 - acc: 0.8264 - val_loss: 0.5406 - val_acc: 0.7025\n",
      "Epoch 339\n",
      " - loss: 0.4395 - acc: 0.8264 - val_loss: 0.5456 - val_acc: 0.6694\n",
      "Epoch 340\n",
      " - loss: 0.4374 - acc: 0.8306 - val_loss: 0.5421 - val_acc: 0.6860\n",
      "Epoch 341\n",
      " - loss: 0.4398 - acc: 0.8326 - val_loss: 0.5410 - val_acc: 0.7107\n",
      "Epoch 342\n",
      " - loss: 0.4366 - acc: 0.8347 - val_loss: 0.5410 - val_acc: 0.6942\n",
      "Epoch 343\n",
      " - loss: 0.4365 - acc: 0.8306 - val_loss: 0.5498 - val_acc: 0.6694\n",
      "Epoch 344\n",
      " - loss: 0.4362 - acc: 0.8202 - val_loss: 0.5551 - val_acc: 0.6694\n",
      "Epoch 345\n",
      " - loss: 0.4386 - acc: 0.8388 - val_loss: 0.5428 - val_acc: 0.6777\n",
      "Epoch 346\n",
      " - loss: 0.4361 - acc: 0.8244 - val_loss: 0.5470 - val_acc: 0.6777\n",
      "Epoch 347\n",
      " - loss: 0.4343 - acc: 0.8388 - val_loss: 0.5507 - val_acc: 0.6777\n",
      "Epoch 348\n",
      " - loss: 0.4346 - acc: 0.8347 - val_loss: 0.5438 - val_acc: 0.6777\n",
      "Epoch 349\n",
      " - loss: 0.4343 - acc: 0.8430 - val_loss: 0.5417 - val_acc: 0.7025\n",
      "Epoch 350\n",
      " - loss: 0.4354 - acc: 0.8244 - val_loss: 0.5484 - val_acc: 0.6777\n",
      "Epoch 351\n",
      " - loss: 0.4327 - acc: 0.8368 - val_loss: 0.5417 - val_acc: 0.6942\n",
      "Epoch 352\n",
      " - loss: 0.4345 - acc: 0.8244 - val_loss: 0.5432 - val_acc: 0.6777\n",
      "Epoch 353\n",
      " - loss: 0.4315 - acc: 0.8347 - val_loss: 0.5435 - val_acc: 0.6694\n",
      "Epoch 354\n",
      " - loss: 0.4313 - acc: 0.8347 - val_loss: 0.5512 - val_acc: 0.6777\n",
      "Epoch 355\n",
      " - loss: 0.4286 - acc: 0.8450 - val_loss: 0.5626 - val_acc: 0.6777\n",
      "Epoch 356\n",
      " - loss: 0.4307 - acc: 0.8347 - val_loss: 0.5416 - val_acc: 0.6942\n",
      "Epoch 357\n",
      " - loss: 0.4314 - acc: 0.8326 - val_loss: 0.5415 - val_acc: 0.7107\n",
      "Epoch 358\n",
      " - loss: 0.4289 - acc: 0.8306 - val_loss: 0.5508 - val_acc: 0.6694\n",
      "Epoch 359\n",
      " - loss: 0.4336 - acc: 0.8326 - val_loss: 0.5528 - val_acc: 0.6777\n",
      "Epoch 360\n",
      " - loss: 0.4256 - acc: 0.8450 - val_loss: 0.5608 - val_acc: 0.6777\n",
      "Epoch 361\n",
      " - loss: 0.4291 - acc: 0.8409 - val_loss: 0.5426 - val_acc: 0.6777\n",
      "Epoch 362\n",
      " - loss: 0.4286 - acc: 0.8326 - val_loss: 0.5433 - val_acc: 0.6860\n",
      "Epoch 363\n",
      " - loss: 0.4284 - acc: 0.8306 - val_loss: 0.5417 - val_acc: 0.7025\n",
      "Epoch 364\n",
      " - loss: 0.4297 - acc: 0.8388 - val_loss: 0.5429 - val_acc: 0.6942\n",
      "Epoch 365\n",
      " - loss: 0.4278 - acc: 0.8347 - val_loss: 0.5454 - val_acc: 0.6777\n",
      "Epoch 366\n",
      " - loss: 0.4270 - acc: 0.8347 - val_loss: 0.5456 - val_acc: 0.6777\n",
      "Epoch 367\n",
      " - loss: 0.4249 - acc: 0.8471 - val_loss: 0.5459 - val_acc: 0.6777\n",
      "Epoch 368\n",
      " - loss: 0.4300 - acc: 0.8202 - val_loss: 0.5484 - val_acc: 0.6777\n",
      "Epoch 369\n",
      " - loss: 0.4239 - acc: 0.8264 - val_loss: 0.5469 - val_acc: 0.6777\n",
      "Epoch 370\n",
      " - loss: 0.4247 - acc: 0.8285 - val_loss: 0.5463 - val_acc: 0.6777\n",
      "Epoch 371\n",
      " - loss: 0.4252 - acc: 0.8306 - val_loss: 0.5433 - val_acc: 0.6777\n",
      "Epoch 372\n",
      " - loss: 0.4231 - acc: 0.8347 - val_loss: 0.5429 - val_acc: 0.7025\n",
      "Epoch 373\n",
      " - loss: 0.4232 - acc: 0.8388 - val_loss: 0.5428 - val_acc: 0.6942\n",
      "Epoch 374\n",
      " - loss: 0.4233 - acc: 0.8450 - val_loss: 0.5433 - val_acc: 0.6860\n",
      "Epoch 375\n",
      " - loss: 0.4229 - acc: 0.8409 - val_loss: 0.5423 - val_acc: 0.7025\n",
      "Epoch 376\n",
      " - loss: 0.4252 - acc: 0.8285 - val_loss: 0.5435 - val_acc: 0.6777\n",
      "Epoch 377\n",
      " - loss: 0.4234 - acc: 0.8285 - val_loss: 0.5466 - val_acc: 0.6777\n",
      "Epoch 378\n",
      " - loss: 0.4225 - acc: 0.8409 - val_loss: 0.5523 - val_acc: 0.6694\n",
      "Epoch 379\n",
      " - loss: 0.4237 - acc: 0.8512 - val_loss: 0.5482 - val_acc: 0.6694\n",
      "Epoch 380\n",
      " - loss: 0.4205 - acc: 0.8388 - val_loss: 0.5441 - val_acc: 0.6860\n",
      "Epoch 381\n",
      " - loss: 0.4208 - acc: 0.8430 - val_loss: 0.5429 - val_acc: 0.6942\n",
      "Epoch 382\n",
      " - loss: 0.4217 - acc: 0.8368 - val_loss: 0.5517 - val_acc: 0.6694\n",
      "Epoch 383\n",
      " - loss: 0.4215 - acc: 0.8347 - val_loss: 0.5467 - val_acc: 0.6777\n",
      "Epoch 384\n",
      " - loss: 0.4207 - acc: 0.8347 - val_loss: 0.5444 - val_acc: 0.6694\n",
      "Epoch 385\n",
      " - loss: 0.4192 - acc: 0.8347 - val_loss: 0.5435 - val_acc: 0.7025\n",
      "Epoch 386\n",
      " - loss: 0.4193 - acc: 0.8409 - val_loss: 0.5505 - val_acc: 0.6777\n",
      "Epoch 387\n",
      " - loss: 0.4173 - acc: 0.8347 - val_loss: 0.5433 - val_acc: 0.7025\n",
      "Epoch 388\n",
      " - loss: 0.4177 - acc: 0.8471 - val_loss: 0.5431 - val_acc: 0.7025\n",
      "Epoch 389\n",
      " - loss: 0.4180 - acc: 0.8471 - val_loss: 0.5430 - val_acc: 0.6942\n",
      "Epoch 390\n",
      " - loss: 0.4174 - acc: 0.8368 - val_loss: 0.5432 - val_acc: 0.6942\n",
      "Epoch 391\n",
      " - loss: 0.4167 - acc: 0.8471 - val_loss: 0.5434 - val_acc: 0.6942\n",
      "Epoch 392\n",
      " - loss: 0.4174 - acc: 0.8533 - val_loss: 0.5469 - val_acc: 0.6777\n",
      "Epoch 393\n",
      " - loss: 0.4133 - acc: 0.8574 - val_loss: 0.5433 - val_acc: 0.6942\n",
      "Epoch 394\n",
      " - loss: 0.4156 - acc: 0.8450 - val_loss: 0.5433 - val_acc: 0.7025\n",
      "Epoch 395\n",
      " - loss: 0.4158 - acc: 0.8388 - val_loss: 0.5434 - val_acc: 0.7025\n",
      "Epoch 396\n",
      " - loss: 0.4157 - acc: 0.8430 - val_loss: 0.5518 - val_acc: 0.6777\n",
      "Epoch 397\n",
      " - loss: 0.4136 - acc: 0.8450 - val_loss: 0.5476 - val_acc: 0.6777\n",
      "Epoch 398\n",
      " - loss: 0.4165 - acc: 0.8368 - val_loss: 0.5453 - val_acc: 0.6694\n",
      "Epoch 399\n",
      " - loss: 0.4141 - acc: 0.8388 - val_loss: 0.5466 - val_acc: 0.6777\n",
      "Epoch 400\n",
      " - loss: 0.4133 - acc: 0.8471 - val_loss: 0.5456 - val_acc: 0.6694\n",
      "Epoch 401\n",
      " - loss: 0.4128 - acc: 0.8471 - val_loss: 0.5567 - val_acc: 0.6777\n",
      "Epoch 402\n",
      " - loss: 0.4126 - acc: 0.8554 - val_loss: 0.5440 - val_acc: 0.6942\n",
      "Epoch 403\n",
      " - loss: 0.4122 - acc: 0.8430 - val_loss: 0.5439 - val_acc: 0.6942\n",
      "Epoch 404\n",
      " - loss: 0.4109 - acc: 0.8471 - val_loss: 0.5439 - val_acc: 0.6942\n",
      "Epoch 405\n",
      " - loss: 0.4110 - acc: 0.8471 - val_loss: 0.5440 - val_acc: 0.6942\n",
      "Epoch 406\n",
      " - loss: 0.4094 - acc: 0.8471 - val_loss: 0.5554 - val_acc: 0.6860\n",
      "Epoch 407\n",
      " - loss: 0.4105 - acc: 0.8492 - val_loss: 0.5482 - val_acc: 0.6777\n",
      "Epoch 408\n",
      " - loss: 0.4089 - acc: 0.8347 - val_loss: 0.5462 - val_acc: 0.6694\n",
      "Epoch 409\n",
      " - loss: 0.4127 - acc: 0.8409 - val_loss: 0.5509 - val_acc: 0.6777\n",
      "Epoch 410\n",
      " - loss: 0.4103 - acc: 0.8471 - val_loss: 0.5442 - val_acc: 0.6942\n",
      "Epoch 411\n",
      " - loss: 0.4092 - acc: 0.8471 - val_loss: 0.5485 - val_acc: 0.6694\n",
      "Epoch 412\n",
      " - loss: 0.4090 - acc: 0.8492 - val_loss: 0.5444 - val_acc: 0.6942\n",
      "Epoch 413\n",
      " - loss: 0.4091 - acc: 0.8450 - val_loss: 0.5447 - val_acc: 0.6942\n",
      "Epoch 414\n",
      " - loss: 0.4084 - acc: 0.8512 - val_loss: 0.5446 - val_acc: 0.6942\n",
      "Epoch 415\n",
      " - loss: 0.4080 - acc: 0.8492 - val_loss: 0.5443 - val_acc: 0.6942\n",
      "Epoch 416\n",
      " - loss: 0.4081 - acc: 0.8492 - val_loss: 0.5466 - val_acc: 0.6694\n",
      "Epoch 417\n",
      " - loss: 0.4061 - acc: 0.8471 - val_loss: 0.5450 - val_acc: 0.6942\n",
      "Epoch 418\n",
      " - loss: 0.4061 - acc: 0.8409 - val_loss: 0.5451 - val_acc: 0.7025\n",
      "Epoch 419\n",
      " - loss: 0.4072 - acc: 0.8471 - val_loss: 0.5507 - val_acc: 0.6777\n",
      "Epoch 420\n",
      " - loss: 0.4080 - acc: 0.8616 - val_loss: 0.5449 - val_acc: 0.6777\n",
      "Epoch 421\n",
      " - loss: 0.4065 - acc: 0.8512 - val_loss: 0.5461 - val_acc: 0.6860\n",
      "Epoch 422\n",
      " - loss: 0.4047 - acc: 0.8512 - val_loss: 0.5452 - val_acc: 0.6942\n",
      "Epoch 423\n",
      " - loss: 0.4054 - acc: 0.8450 - val_loss: 0.5457 - val_acc: 0.6942\n",
      "Epoch 424\n",
      " - loss: 0.4029 - acc: 0.8595 - val_loss: 0.5512 - val_acc: 0.6777\n",
      "Epoch 425\n",
      " - loss: 0.4039 - acc: 0.8409 - val_loss: 0.5491 - val_acc: 0.6612\n",
      "Epoch 426\n",
      " - loss: 0.4031 - acc: 0.8430 - val_loss: 0.5477 - val_acc: 0.6777\n",
      "Epoch 427\n",
      " - loss: 0.4039 - acc: 0.8368 - val_loss: 0.5456 - val_acc: 0.6942\n",
      "Epoch 428\n",
      " - loss: 0.4032 - acc: 0.8512 - val_loss: 0.5459 - val_acc: 0.7025\n",
      "Epoch 429\n",
      " - loss: 0.4024 - acc: 0.8409 - val_loss: 0.5533 - val_acc: 0.6777\n",
      "Epoch 430\n",
      " - loss: 0.4026 - acc: 0.8450 - val_loss: 0.5456 - val_acc: 0.6942\n",
      "Epoch 431\n",
      " - loss: 0.4011 - acc: 0.8471 - val_loss: 0.5495 - val_acc: 0.6777\n",
      "Epoch 432\n",
      " - loss: 0.4021 - acc: 0.8450 - val_loss: 0.5490 - val_acc: 0.6612\n",
      "Epoch 433\n",
      " - loss: 0.4016 - acc: 0.8554 - val_loss: 0.5456 - val_acc: 0.6942\n",
      "Epoch 434\n",
      " - loss: 0.4008 - acc: 0.8533 - val_loss: 0.5459 - val_acc: 0.6942\n",
      "Epoch 435\n",
      " - loss: 0.4000 - acc: 0.8450 - val_loss: 0.5477 - val_acc: 0.6860\n",
      "Epoch 436\n",
      " - loss: 0.4004 - acc: 0.8512 - val_loss: 0.5460 - val_acc: 0.6942\n",
      "Epoch 437\n",
      " - loss: 0.3981 - acc: 0.8554 - val_loss: 0.5499 - val_acc: 0.6860\n",
      "Epoch 438\n",
      " - loss: 0.4010 - acc: 0.8471 - val_loss: 0.5460 - val_acc: 0.6942\n",
      "Epoch 439\n",
      " - loss: 0.4004 - acc: 0.8471 - val_loss: 0.5475 - val_acc: 0.6777\n",
      "Epoch 440\n",
      " - loss: 0.4002 - acc: 0.8450 - val_loss: 0.5536 - val_acc: 0.6694\n",
      "Epoch 441\n",
      " - loss: 0.3969 - acc: 0.8430 - val_loss: 0.5476 - val_acc: 0.6860\n",
      "Epoch 442\n",
      " - loss: 0.3966 - acc: 0.8492 - val_loss: 0.5501 - val_acc: 0.6612\n",
      "Epoch 443\n",
      " - loss: 0.3972 - acc: 0.8492 - val_loss: 0.5468 - val_acc: 0.6694\n",
      "Epoch 444\n",
      " - loss: 0.3975 - acc: 0.8512 - val_loss: 0.5483 - val_acc: 0.6777\n",
      "Epoch 445\n",
      " - loss: 0.3985 - acc: 0.8595 - val_loss: 0.5531 - val_acc: 0.6860\n",
      "Epoch 446\n",
      " - loss: 0.3971 - acc: 0.8430 - val_loss: 0.5471 - val_acc: 0.6942\n",
      "Epoch 447\n",
      " - loss: 0.3956 - acc: 0.8512 - val_loss: 0.5480 - val_acc: 0.6860\n",
      "Epoch 448\n",
      " - loss: 0.3980 - acc: 0.8471 - val_loss: 0.5475 - val_acc: 0.6942\n",
      "Epoch 449\n",
      " - loss: 0.3959 - acc: 0.8533 - val_loss: 0.5541 - val_acc: 0.6777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450\n",
      " - loss: 0.3969 - acc: 0.8471 - val_loss: 0.5470 - val_acc: 0.6942\n",
      "Epoch 451\n",
      " - loss: 0.3960 - acc: 0.8512 - val_loss: 0.5483 - val_acc: 0.6777\n",
      "Epoch 452\n",
      " - loss: 0.3948 - acc: 0.8492 - val_loss: 0.5479 - val_acc: 0.6942\n",
      "Epoch 453\n",
      " - loss: 0.3915 - acc: 0.8574 - val_loss: 0.5472 - val_acc: 0.6694\n",
      "Epoch 454\n",
      " - loss: 0.3940 - acc: 0.8492 - val_loss: 0.5559 - val_acc: 0.6777\n",
      "Epoch 455\n",
      " - loss: 0.3942 - acc: 0.8471 - val_loss: 0.5471 - val_acc: 0.7025\n",
      "Epoch 456\n",
      " - loss: 0.3949 - acc: 0.8492 - val_loss: 0.5471 - val_acc: 0.6942\n",
      "Epoch 457\n",
      " - loss: 0.3924 - acc: 0.8492 - val_loss: 0.5482 - val_acc: 0.6942\n",
      "Epoch 458\n",
      " - loss: 0.3912 - acc: 0.8554 - val_loss: 0.5474 - val_acc: 0.6694\n",
      "Epoch 459\n",
      " - loss: 0.3932 - acc: 0.8492 - val_loss: 0.5477 - val_acc: 0.6942\n",
      "Epoch 460\n",
      " - loss: 0.3919 - acc: 0.8492 - val_loss: 0.5476 - val_acc: 0.6942\n",
      "Epoch 461\n",
      " - loss: 0.3916 - acc: 0.8616 - val_loss: 0.5513 - val_acc: 0.6694\n",
      "Epoch 462\n",
      " - loss: 0.3891 - acc: 0.8678 - val_loss: 0.5476 - val_acc: 0.6942\n",
      "Epoch 463\n",
      " - loss: 0.3891 - acc: 0.8554 - val_loss: 0.5485 - val_acc: 0.6942\n",
      "Epoch 464\n",
      " - loss: 0.3895 - acc: 0.8533 - val_loss: 0.5649 - val_acc: 0.6860\n",
      "Epoch 465\n",
      " - loss: 0.3921 - acc: 0.8471 - val_loss: 0.5530 - val_acc: 0.6694\n",
      "Epoch 466\n",
      " - loss: 0.3879 - acc: 0.8574 - val_loss: 0.5512 - val_acc: 0.6694\n",
      "Epoch 467\n",
      " - loss: 0.3870 - acc: 0.8636 - val_loss: 0.5700 - val_acc: 0.6777\n",
      "Epoch 468\n",
      " - loss: 0.3888 - acc: 0.8574 - val_loss: 0.5588 - val_acc: 0.6860\n",
      "Epoch 469\n",
      " - loss: 0.3881 - acc: 0.8533 - val_loss: 0.5524 - val_acc: 0.6612\n",
      "Epoch 470\n",
      " - loss: 0.3884 - acc: 0.8492 - val_loss: 0.5492 - val_acc: 0.6942\n",
      "Epoch 471\n",
      " - loss: 0.3909 - acc: 0.8512 - val_loss: 0.5538 - val_acc: 0.6694\n",
      "Epoch 472\n",
      " - loss: 0.3859 - acc: 0.8595 - val_loss: 0.5512 - val_acc: 0.6694\n",
      "Epoch 473\n",
      " - loss: 0.3866 - acc: 0.8574 - val_loss: 0.5497 - val_acc: 0.6777\n",
      "Epoch 474\n",
      " - loss: 0.3859 - acc: 0.8574 - val_loss: 0.5484 - val_acc: 0.6942\n",
      "Epoch 475\n",
      " - loss: 0.3861 - acc: 0.8512 - val_loss: 0.5487 - val_acc: 0.6942\n",
      "Epoch 476\n",
      " - loss: 0.3884 - acc: 0.8471 - val_loss: 0.5491 - val_acc: 0.6942\n",
      "Epoch 477\n",
      " - loss: 0.3846 - acc: 0.8657 - val_loss: 0.5488 - val_acc: 0.6860\n",
      "Epoch 478\n",
      " - loss: 0.3838 - acc: 0.8554 - val_loss: 0.5499 - val_acc: 0.6777\n",
      "Epoch 479\n",
      " - loss: 0.3834 - acc: 0.8595 - val_loss: 0.5489 - val_acc: 0.6942\n",
      "Epoch 480\n",
      " - loss: 0.3831 - acc: 0.8512 - val_loss: 0.5490 - val_acc: 0.6942\n",
      "Epoch 481\n",
      " - loss: 0.3865 - acc: 0.8595 - val_loss: 0.5597 - val_acc: 0.6860\n",
      "Epoch 482\n",
      " - loss: 0.3844 - acc: 0.8636 - val_loss: 0.5500 - val_acc: 0.6942\n",
      "Epoch 483\n",
      " - loss: 0.3824 - acc: 0.8636 - val_loss: 0.5497 - val_acc: 0.6942\n",
      "Epoch 484\n",
      " - loss: 0.3828 - acc: 0.8616 - val_loss: 0.5496 - val_acc: 0.6777\n",
      "Epoch 485\n",
      " - loss: 0.3831 - acc: 0.8616 - val_loss: 0.5650 - val_acc: 0.6777\n",
      "Epoch 486\n",
      " - loss: 0.3839 - acc: 0.8636 - val_loss: 0.5491 - val_acc: 0.6942\n",
      "Epoch 487\n",
      " - loss: 0.3811 - acc: 0.8554 - val_loss: 0.5556 - val_acc: 0.6860\n",
      "Epoch 488\n",
      " - loss: 0.3809 - acc: 0.8678 - val_loss: 0.5537 - val_acc: 0.6612\n",
      "Epoch 489\n",
      " - loss: 0.3804 - acc: 0.8554 - val_loss: 0.5494 - val_acc: 0.6942\n",
      "Epoch 490\n",
      " - loss: 0.3810 - acc: 0.8533 - val_loss: 0.5494 - val_acc: 0.6860\n",
      "Epoch 491\n",
      " - loss: 0.3823 - acc: 0.8636 - val_loss: 0.5501 - val_acc: 0.6942\n",
      "Epoch 492\n",
      " - loss: 0.3788 - acc: 0.8657 - val_loss: 0.5548 - val_acc: 0.6860\n",
      "Epoch 493\n",
      " - loss: 0.3789 - acc: 0.8719 - val_loss: 0.5521 - val_acc: 0.6777\n",
      "Epoch 494\n",
      " - loss: 0.3787 - acc: 0.8554 - val_loss: 0.5496 - val_acc: 0.6860\n",
      "Epoch 495\n",
      " - loss: 0.3789 - acc: 0.8492 - val_loss: 0.5512 - val_acc: 0.6777\n",
      "Epoch 496\n",
      " - loss: 0.3777 - acc: 0.8678 - val_loss: 0.5497 - val_acc: 0.6860\n",
      "Epoch 497\n",
      " - loss: 0.3769 - acc: 0.8636 - val_loss: 0.5502 - val_acc: 0.6860\n",
      "Epoch 498\n",
      " - loss: 0.3793 - acc: 0.8595 - val_loss: 0.5501 - val_acc: 0.6860\n",
      "Epoch 499\n",
      " - loss: 0.3761 - acc: 0.8636 - val_loss: 0.5513 - val_acc: 0.6694\n",
      "Epoch 500\n",
      " - loss: 0.3770 - acc: 0.8719 - val_loss: 0.5502 - val_acc: 0.6942\n",
      "CPU times: user 10min 34s, sys: 8min 53s, total: 19min 28s\n",
      "Wall time: 19min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "# f1 = filter 1 size, c1 = number of channels (filters), p1 = size of pooling\n",
    "hparams = {\"f1\":5, \"c1\":48, \"p1\":2,\n",
    "           \"f2\":3, \"c2\":128, \"p2\":2,\n",
    "           \"epochs\":500,\n",
    "           \"learning_rate\":2e-6, \n",
    "           \"batch_size\":4, \n",
    "           \"lambd\":0.2}\n",
    "costs_train, costs_dev, accs_train, accs_dev, preds_train, preds_dev = model_trainer(hparams, print_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIrxtty3xYwz"
   },
   "source": [
    "Analyzing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1527798682649,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "PR7nP3kluA1g",
    "outputId": "31f311ec-0281-4d4c-9f92-cde6b7fc1d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best epoch is 286\n",
      " - loss: 0.4604 - acc: 0.8347 - val_loss: 0.5392 - val_acc: 0.6942\n"
     ]
    }
   ],
   "source": [
    "best_epoch = np.argmin(costs_dev)\n",
    "cost_train = costs_train[best_epoch]\n",
    "cost_dev = costs_dev[best_epoch]\n",
    "acc_train = accs_train[best_epoch]\n",
    "acc_dev = accs_dev[best_epoch]\n",
    "print(\"The best epoch is {}\".format(best_epoch))\n",
    "print(\" - loss: {:6.4f} - acc: {:6.4f} - val_loss: {:6.4f} - val_acc: {:6.4f}\".format(cost_train,acc_train,cost_dev,acc_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 787,
     "status": "ok",
     "timestamp": 1527798683555,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "bJv3oIq11KFp",
    "outputId": "99433c76-5b2d-462c-9af1-35c0f4299d4b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f207a401940>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_curve, = plt.plot(costs_train, label = 'Train error')\n",
    "test_curve,  = plt.plot(costs_dev, label = 'Dev error')\n",
    "plt.legend(handles=[train_curve,test_curve])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean log loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewbX5dvY1KFt"
   },
   "source": [
    "This result shows that, by using two hidden convolutional layers, the model rendered a close accuracy to the shallow NN a dev error about 9% lower.  Although these are only modest differences, especially for accuracy, this intermediate result reveals a crucial fact: there are over 100 times less parameters in this CNN (190,017) than in the shallow NN (20,644,681). This is a clear demonstration of the expressivity power that convolutional neural networks have over shallow fully connected NN, and shows why we should stick to this type of architecture for real improvements in the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ar1aH4hM1KFv"
   },
   "source": [
    "### AlexNet\n",
    "[AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) is the pioneer deep learning architecture. Developed by Alex Krizhevsky, Ilya Sutskever and the godfather of Deep Learning, Geofrey Hinton, this algorithm won the ILSVR 2012 competition by a large marging, proving that deep learning was the top technique for image classification and localization.\n",
    "\n",
    "Alexnet hidden layer architecture is made of eight layers, five convolutional and two fully connected with the following characteristic:\n",
    "* 96 kernels (filters) of size 11x11x3 with a stride of 4 pixels;\n",
    "    * Response normalization\n",
    "    * Pooling (size 3, stride 2)\n",
    "* 256 kernels of size 5x5x96\n",
    "    * Response normalization\n",
    "    * Pooling (size 3, stride 2)\n",
    "* 384 kernels of size 3x3x256\n",
    "* 384 kernels of size 3x3x384\n",
    "* 256 kernels of size 3x3x384\n",
    "* Two consecutive fully connected layers with 4096 neurons each\n",
    "    * With dropout rate of 0.5\n",
    "\n",
    "Our problem differs from the original problem in two aspects: \n",
    "1. The input images were 224x224x3 pixels, while our problem images are 128x128x3 pixels. \n",
    "2. The original problem was a classification with 1000 possible classes, ours have only two.\n",
    "\n",
    "We will build a similar architecture with another two considerations. \n",
    "1. The original paper used local response normalization. We will use batch normalization.\n",
    "2. The original architecture had two networks, interconnected on layers 3, 6 and 7. We will use only one.\n",
    "\n",
    "Due to the second consideration, all number of filters and neurons in the dense layers will be half the size of the original architecture. This shouldn't be a problem since our number of classes is considerably smaller.\n",
    "\n",
    "The original paper performs dataset augmentation. In this work, we will also use this technique applying random width and height shift, random zooming and random horizontal flip.\n",
    "\n",
    "We shall now use Keras sequential model with tensorflow backend for less verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1527798684203,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "uZKSTnmy1KFx",
    "outputId": "d7a42160-03cc-45c9-fb1a-ac11a783bf69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "np.random.seed(1564)\n",
    "tf.set_random_seed(1564)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmSFul5x5utG"
   },
   "source": [
    "Defining the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "3Q2wJKY95q2X"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# LAYER 1\n",
    "model.add(Conv2D(48,11,strides=(4,4),padding='same',input_shape=(128,128,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(3,2))\n",
    "\n",
    "# LAYER 2\n",
    "model.add(Conv2D(128,5,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(3,2))\n",
    "\n",
    "# LAYER 3\n",
    "model.add(Conv2D(192,3,padding='same',activation='relu'))\n",
    "\n",
    "# LAYER 4\n",
    "model.add(Conv2D(192,3,padding='same',activation='relu'))\n",
    "\n",
    "# LAYER 5\n",
    "model.add(Conv2D(192,3,padding='same',activation='relu'))\n",
    "\n",
    "# LAYER 6\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# LAYER 7\n",
    "model.add(Dense(2048,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# OUTPUT\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tL5uOznN6BWO"
   },
   "source": [
    "Creating a function to re-initialize all trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "cTXQ4gJwW7Jl"
   },
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpGoQnU86SN_"
   },
   "source": [
    "Running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 33667
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3695120,
     "status": "ok",
     "timestamp": 1527812253311,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "zyikcPAFH1YE",
    "outputId": "3e09cc42-e837-41a6-f49c-b485a81e0167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      " - 5s - loss: 0.7204 - acc: 0.5000 - val_loss: 0.7026 - val_acc: 0.4463\n",
      "Epoch 2/1000\n",
      " - 4s - loss: 0.7217 - acc: 0.4669 - val_loss: 0.6932 - val_acc: 0.5207\n",
      "Epoch 3/1000\n",
      " - 4s - loss: 0.7215 - acc: 0.4855 - val_loss: 0.6910 - val_acc: 0.5289\n",
      "Epoch 4/1000\n",
      " - 4s - loss: 0.6968 - acc: 0.5455 - val_loss: 0.6851 - val_acc: 0.5785\n",
      "Epoch 5/1000\n",
      " - 4s - loss: 0.7065 - acc: 0.5310 - val_loss: 0.6826 - val_acc: 0.5455\n",
      "Epoch 6/1000\n",
      " - 4s - loss: 0.7021 - acc: 0.5207 - val_loss: 0.6800 - val_acc: 0.5620\n",
      "Epoch 7/1000\n",
      " - 4s - loss: 0.7028 - acc: 0.5083 - val_loss: 0.6754 - val_acc: 0.6116\n",
      "Epoch 8/1000\n",
      " - 4s - loss: 0.6922 - acc: 0.5640 - val_loss: 0.6718 - val_acc: 0.6364\n",
      "Epoch 9/1000\n",
      " - 4s - loss: 0.7033 - acc: 0.5227 - val_loss: 0.6676 - val_acc: 0.6281\n",
      "Epoch 10/1000\n",
      " - 4s - loss: 0.6908 - acc: 0.5455 - val_loss: 0.6648 - val_acc: 0.6364\n",
      "Epoch 11/1000\n",
      " - 4s - loss: 0.6823 - acc: 0.5496 - val_loss: 0.6627 - val_acc: 0.6198\n",
      "Epoch 12/1000\n",
      " - 4s - loss: 0.6884 - acc: 0.5599 - val_loss: 0.6618 - val_acc: 0.6198\n",
      "Epoch 13/1000\n",
      " - 4s - loss: 0.6794 - acc: 0.5579 - val_loss: 0.6584 - val_acc: 0.6281\n",
      "Epoch 14/1000\n",
      " - 4s - loss: 0.6833 - acc: 0.5579 - val_loss: 0.6564 - val_acc: 0.6281\n",
      "Epoch 15/1000\n",
      " - 4s - loss: 0.6836 - acc: 0.5599 - val_loss: 0.6548 - val_acc: 0.6364\n",
      "Epoch 16/1000\n",
      " - 4s - loss: 0.6892 - acc: 0.5475 - val_loss: 0.6529 - val_acc: 0.6364\n",
      "Epoch 17/1000\n",
      " - 4s - loss: 0.6758 - acc: 0.5723 - val_loss: 0.6511 - val_acc: 0.6198\n",
      "Epoch 18/1000\n",
      " - 4s - loss: 0.6719 - acc: 0.5702 - val_loss: 0.6489 - val_acc: 0.6364\n",
      "Epoch 19/1000\n",
      " - 4s - loss: 0.6767 - acc: 0.5806 - val_loss: 0.6467 - val_acc: 0.6364\n",
      "Epoch 20/1000\n",
      " - 4s - loss: 0.6668 - acc: 0.5847 - val_loss: 0.6435 - val_acc: 0.6446\n",
      "Epoch 21/1000\n",
      " - 4s - loss: 0.6622 - acc: 0.5888 - val_loss: 0.6432 - val_acc: 0.6364\n",
      "Epoch 22/1000\n",
      " - 4s - loss: 0.6742 - acc: 0.5806 - val_loss: 0.6410 - val_acc: 0.6364\n",
      "Epoch 23/1000\n",
      " - 4s - loss: 0.6679 - acc: 0.5723 - val_loss: 0.6377 - val_acc: 0.6529\n",
      "Epoch 24/1000\n",
      " - 4s - loss: 0.6664 - acc: 0.5661 - val_loss: 0.6359 - val_acc: 0.6612\n",
      "Epoch 25/1000\n",
      " - 4s - loss: 0.6699 - acc: 0.5806 - val_loss: 0.6333 - val_acc: 0.6281\n",
      "Epoch 26/1000\n",
      " - 4s - loss: 0.6616 - acc: 0.5847 - val_loss: 0.6318 - val_acc: 0.6529\n",
      "Epoch 27/1000\n",
      " - 4s - loss: 0.6741 - acc: 0.5661 - val_loss: 0.6310 - val_acc: 0.6529\n",
      "Epoch 28/1000\n",
      " - 4s - loss: 0.6720 - acc: 0.5723 - val_loss: 0.6288 - val_acc: 0.6446\n",
      "Epoch 29/1000\n",
      " - 4s - loss: 0.6615 - acc: 0.5909 - val_loss: 0.6291 - val_acc: 0.6364\n",
      "Epoch 30/1000\n",
      " - 4s - loss: 0.6571 - acc: 0.5930 - val_loss: 0.6264 - val_acc: 0.6364\n",
      "Epoch 31/1000\n",
      " - 4s - loss: 0.6713 - acc: 0.5455 - val_loss: 0.6246 - val_acc: 0.6529\n",
      "Epoch 32/1000\n",
      " - 4s - loss: 0.6590 - acc: 0.5682 - val_loss: 0.6217 - val_acc: 0.6446\n",
      "Epoch 33/1000\n",
      " - 4s - loss: 0.6538 - acc: 0.5868 - val_loss: 0.6181 - val_acc: 0.6446\n",
      "Epoch 34/1000\n",
      " - 4s - loss: 0.6468 - acc: 0.6219 - val_loss: 0.6193 - val_acc: 0.6612\n",
      "Epoch 35/1000\n",
      " - 4s - loss: 0.6404 - acc: 0.6488 - val_loss: 0.6142 - val_acc: 0.6446\n",
      "Epoch 36/1000\n",
      " - 4s - loss: 0.6525 - acc: 0.6240 - val_loss: 0.6127 - val_acc: 0.6446\n",
      "Epoch 37/1000\n",
      " - 4s - loss: 0.6436 - acc: 0.6219 - val_loss: 0.6109 - val_acc: 0.6529\n",
      "Epoch 38/1000\n",
      " - 4s - loss: 0.6513 - acc: 0.6219 - val_loss: 0.6105 - val_acc: 0.6446\n",
      "Epoch 39/1000\n",
      " - 4s - loss: 0.6578 - acc: 0.6157 - val_loss: 0.6082 - val_acc: 0.6694\n",
      "Epoch 40/1000\n",
      " - 4s - loss: 0.6294 - acc: 0.6384 - val_loss: 0.6070 - val_acc: 0.6612\n",
      "Epoch 41/1000\n",
      " - 4s - loss: 0.6410 - acc: 0.6157 - val_loss: 0.6103 - val_acc: 0.6529\n",
      "Epoch 42/1000\n",
      " - 4s - loss: 0.6395 - acc: 0.6343 - val_loss: 0.6037 - val_acc: 0.6777\n",
      "Epoch 43/1000\n",
      " - 4s - loss: 0.6218 - acc: 0.6570 - val_loss: 0.6025 - val_acc: 0.6777\n",
      "Epoch 44/1000\n",
      " - 4s - loss: 0.6537 - acc: 0.5950 - val_loss: 0.5984 - val_acc: 0.6694\n",
      "Epoch 45/1000\n",
      " - 4s - loss: 0.6279 - acc: 0.6198 - val_loss: 0.5974 - val_acc: 0.6777\n",
      "Epoch 46/1000\n",
      " - 4s - loss: 0.6361 - acc: 0.6157 - val_loss: 0.5972 - val_acc: 0.6777\n",
      "Epoch 47/1000\n",
      " - 4s - loss: 0.6294 - acc: 0.6508 - val_loss: 0.5993 - val_acc: 0.6612\n",
      "Epoch 48/1000\n",
      " - 4s - loss: 0.6203 - acc: 0.6446 - val_loss: 0.5990 - val_acc: 0.6446\n",
      "Epoch 49/1000\n",
      " - 4s - loss: 0.6432 - acc: 0.6240 - val_loss: 0.5941 - val_acc: 0.6529\n",
      "Epoch 50/1000\n",
      " - 4s - loss: 0.6085 - acc: 0.6591 - val_loss: 0.5901 - val_acc: 0.6529\n",
      "Epoch 51/1000\n",
      " - 4s - loss: 0.6372 - acc: 0.6116 - val_loss: 0.5869 - val_acc: 0.6529\n",
      "Epoch 52/1000\n",
      " - 4s - loss: 0.6279 - acc: 0.6302 - val_loss: 0.5896 - val_acc: 0.6612\n",
      "Epoch 53/1000\n",
      " - 4s - loss: 0.6325 - acc: 0.6302 - val_loss: 0.5891 - val_acc: 0.6529\n",
      "Epoch 54/1000\n",
      " - 4s - loss: 0.6038 - acc: 0.6612 - val_loss: 0.5942 - val_acc: 0.6777\n",
      "Epoch 55/1000\n",
      " - 4s - loss: 0.6043 - acc: 0.6612 - val_loss: 0.5878 - val_acc: 0.6612\n",
      "Epoch 56/1000\n",
      " - 4s - loss: 0.6079 - acc: 0.6612 - val_loss: 0.5834 - val_acc: 0.6446\n",
      "Epoch 57/1000\n",
      " - 4s - loss: 0.6263 - acc: 0.6364 - val_loss: 0.5901 - val_acc: 0.6777\n",
      "Epoch 58/1000\n",
      " - 4s - loss: 0.6155 - acc: 0.6550 - val_loss: 0.5829 - val_acc: 0.6529\n",
      "Epoch 59/1000\n",
      " - 4s - loss: 0.6260 - acc: 0.6508 - val_loss: 0.5814 - val_acc: 0.6529\n",
      "Epoch 60/1000\n",
      " - 4s - loss: 0.6247 - acc: 0.6384 - val_loss: 0.5853 - val_acc: 0.6612\n",
      "Epoch 61/1000\n",
      " - 4s - loss: 0.6147 - acc: 0.6591 - val_loss: 0.5867 - val_acc: 0.6612\n",
      "Epoch 62/1000\n",
      " - 4s - loss: 0.6282 - acc: 0.6488 - val_loss: 0.5920 - val_acc: 0.6612\n",
      "Epoch 63/1000\n",
      " - 4s - loss: 0.6034 - acc: 0.6736 - val_loss: 0.6107 - val_acc: 0.6612\n",
      "Epoch 64/1000\n",
      " - 4s - loss: 0.6091 - acc: 0.6612 - val_loss: 0.5921 - val_acc: 0.6694\n",
      "Epoch 65/1000\n",
      " - 4s - loss: 0.6029 - acc: 0.6715 - val_loss: 0.5860 - val_acc: 0.6529\n",
      "Epoch 66/1000\n",
      " - 4s - loss: 0.5944 - acc: 0.6694 - val_loss: 0.5794 - val_acc: 0.6694\n",
      "Epoch 67/1000\n",
      " - 4s - loss: 0.6171 - acc: 0.6529 - val_loss: 0.6015 - val_acc: 0.6777\n",
      "Epoch 68/1000\n",
      " - 4s - loss: 0.6018 - acc: 0.6529 - val_loss: 0.5967 - val_acc: 0.6777\n",
      "Epoch 69/1000\n",
      " - 4s - loss: 0.6205 - acc: 0.6364 - val_loss: 0.5795 - val_acc: 0.6777\n",
      "Epoch 70/1000\n",
      " - 4s - loss: 0.5824 - acc: 0.6736 - val_loss: 0.5936 - val_acc: 0.6281\n",
      "Epoch 71/1000\n",
      " - 4s - loss: 0.6084 - acc: 0.6364 - val_loss: 0.5807 - val_acc: 0.6694\n",
      "Epoch 72/1000\n",
      " - 4s - loss: 0.5906 - acc: 0.6612 - val_loss: 0.5776 - val_acc: 0.6612\n",
      "Epoch 73/1000\n",
      " - 4s - loss: 0.6002 - acc: 0.6736 - val_loss: 0.5735 - val_acc: 0.6529\n",
      "Epoch 74/1000\n",
      " - 4s - loss: 0.6033 - acc: 0.6612 - val_loss: 0.5969 - val_acc: 0.6694\n",
      "Epoch 75/1000\n",
      " - 4s - loss: 0.5975 - acc: 0.6384 - val_loss: 0.5796 - val_acc: 0.6694\n",
      "Epoch 76/1000\n",
      " - 4s - loss: 0.6088 - acc: 0.6591 - val_loss: 0.5823 - val_acc: 0.6694\n",
      "Epoch 77/1000\n",
      " - 4s - loss: 0.5765 - acc: 0.6818 - val_loss: 0.6042 - val_acc: 0.6777\n",
      "Epoch 78/1000\n",
      " - 4s - loss: 0.6002 - acc: 0.6694 - val_loss: 0.5842 - val_acc: 0.6198\n",
      "Epoch 79/1000\n",
      " - 4s - loss: 0.6231 - acc: 0.6343 - val_loss: 0.5846 - val_acc: 0.6694\n",
      "Epoch 80/1000\n",
      " - 4s - loss: 0.5961 - acc: 0.6632 - val_loss: 0.5978 - val_acc: 0.6860\n",
      "Epoch 81/1000\n",
      " - 4s - loss: 0.5822 - acc: 0.6880 - val_loss: 0.5938 - val_acc: 0.6860\n",
      "Epoch 82/1000\n",
      " - 4s - loss: 0.5854 - acc: 0.6715 - val_loss: 0.5825 - val_acc: 0.6860\n",
      "Epoch 83/1000\n",
      " - 4s - loss: 0.5924 - acc: 0.6901 - val_loss: 0.5756 - val_acc: 0.6612\n",
      "Epoch 84/1000\n",
      " - 4s - loss: 0.5984 - acc: 0.6736 - val_loss: 0.5797 - val_acc: 0.6942\n",
      "Epoch 85/1000\n",
      " - 4s - loss: 0.5957 - acc: 0.6880 - val_loss: 0.5997 - val_acc: 0.6860\n",
      "Epoch 86/1000\n",
      " - 4s - loss: 0.5950 - acc: 0.6570 - val_loss: 0.5823 - val_acc: 0.6694\n",
      "Epoch 87/1000\n",
      " - 4s - loss: 0.5943 - acc: 0.6632 - val_loss: 0.5787 - val_acc: 0.6694\n",
      "Epoch 88/1000\n",
      " - 4s - loss: 0.6019 - acc: 0.6736 - val_loss: 0.6040 - val_acc: 0.6777\n",
      "Epoch 89/1000\n",
      " - 4s - loss: 0.6023 - acc: 0.6839 - val_loss: 0.5808 - val_acc: 0.6777\n",
      "Epoch 90/1000\n",
      " - 4s - loss: 0.5848 - acc: 0.6777 - val_loss: 0.5787 - val_acc: 0.6446\n",
      "Epoch 91/1000\n",
      " - 4s - loss: 0.5730 - acc: 0.7066 - val_loss: 0.5809 - val_acc: 0.6281\n",
      "Epoch 92/1000\n",
      " - 4s - loss: 0.5771 - acc: 0.6880 - val_loss: 0.5947 - val_acc: 0.6612\n",
      "Epoch 93/1000\n",
      " - 4s - loss: 0.5995 - acc: 0.6384 - val_loss: 0.6062 - val_acc: 0.6694\n",
      "Epoch 94/1000\n",
      " - 4s - loss: 0.5886 - acc: 0.6756 - val_loss: 0.5883 - val_acc: 0.7025\n",
      "Epoch 95/1000\n",
      " - 4s - loss: 0.5842 - acc: 0.6860 - val_loss: 0.5746 - val_acc: 0.6364\n",
      "Epoch 96/1000\n",
      " - 4s - loss: 0.5836 - acc: 0.6818 - val_loss: 0.5875 - val_acc: 0.6694\n",
      "Epoch 97/1000\n",
      " - 4s - loss: 0.5781 - acc: 0.6942 - val_loss: 0.5905 - val_acc: 0.6694\n",
      "Epoch 98/1000\n",
      " - 4s - loss: 0.5823 - acc: 0.6818 - val_loss: 0.6262 - val_acc: 0.6860\n",
      "Epoch 99/1000\n",
      " - 4s - loss: 0.5886 - acc: 0.6777 - val_loss: 0.5742 - val_acc: 0.6446\n",
      "Epoch 100/1000\n",
      " - 4s - loss: 0.5749 - acc: 0.7066 - val_loss: 0.5796 - val_acc: 0.6777\n",
      "Epoch 101/1000\n",
      " - 4s - loss: 0.6126 - acc: 0.6302 - val_loss: 0.5838 - val_acc: 0.6612\n",
      "Epoch 102/1000\n",
      " - 4s - loss: 0.5884 - acc: 0.6694 - val_loss: 0.6032 - val_acc: 0.6860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/1000\n",
      " - 4s - loss: 0.5739 - acc: 0.6921 - val_loss: 0.5819 - val_acc: 0.6860\n",
      "Epoch 104/1000\n",
      " - 4s - loss: 0.5745 - acc: 0.6839 - val_loss: 0.5956 - val_acc: 0.6777\n",
      "Epoch 105/1000\n",
      " - 4s - loss: 0.5922 - acc: 0.6777 - val_loss: 0.5762 - val_acc: 0.6364\n",
      "Epoch 106/1000\n",
      " - 4s - loss: 0.5874 - acc: 0.6694 - val_loss: 0.5860 - val_acc: 0.6777\n",
      "Epoch 107/1000\n",
      " - 4s - loss: 0.5775 - acc: 0.6777 - val_loss: 0.5680 - val_acc: 0.7025\n",
      "Epoch 108/1000\n",
      " - 4s - loss: 0.5819 - acc: 0.6529 - val_loss: 0.5690 - val_acc: 0.7025\n",
      "Epoch 109/1000\n",
      " - 4s - loss: 0.5922 - acc: 0.6777 - val_loss: 0.5848 - val_acc: 0.6694\n",
      "Epoch 110/1000\n",
      " - 4s - loss: 0.5779 - acc: 0.6963 - val_loss: 0.6145 - val_acc: 0.6694\n",
      "Epoch 111/1000\n",
      " - 4s - loss: 0.5734 - acc: 0.6963 - val_loss: 0.5865 - val_acc: 0.6777\n",
      "Epoch 112/1000\n",
      " - 4s - loss: 0.5658 - acc: 0.7025 - val_loss: 0.5978 - val_acc: 0.6694\n",
      "Epoch 113/1000\n",
      " - 4s - loss: 0.5752 - acc: 0.6818 - val_loss: 0.5743 - val_acc: 0.7025\n",
      "Epoch 114/1000\n",
      " - 4s - loss: 0.5835 - acc: 0.6860 - val_loss: 0.5817 - val_acc: 0.6777\n",
      "Epoch 115/1000\n",
      " - 4s - loss: 0.5611 - acc: 0.7190 - val_loss: 0.5636 - val_acc: 0.6529\n",
      "Epoch 116/1000\n",
      " - 4s - loss: 0.5485 - acc: 0.7107 - val_loss: 0.5712 - val_acc: 0.7107\n",
      "Epoch 117/1000\n",
      " - 4s - loss: 0.5885 - acc: 0.6736 - val_loss: 0.5625 - val_acc: 0.6694\n",
      "Epoch 118/1000\n",
      " - 4s - loss: 0.5675 - acc: 0.6963 - val_loss: 0.5802 - val_acc: 0.6942\n",
      "Epoch 119/1000\n",
      " - 4s - loss: 0.5506 - acc: 0.7190 - val_loss: 0.5679 - val_acc: 0.6942\n",
      "Epoch 120/1000\n",
      " - 4s - loss: 0.5664 - acc: 0.6777 - val_loss: 0.5675 - val_acc: 0.6612\n",
      "Epoch 121/1000\n",
      " - 4s - loss: 0.5661 - acc: 0.6901 - val_loss: 0.5925 - val_acc: 0.6860\n",
      "Epoch 122/1000\n",
      " - 4s - loss: 0.5732 - acc: 0.6591 - val_loss: 0.5962 - val_acc: 0.6860\n",
      "Epoch 123/1000\n",
      " - 4s - loss: 0.5646 - acc: 0.6942 - val_loss: 0.5827 - val_acc: 0.6860\n",
      "Epoch 124/1000\n",
      " - 4s - loss: 0.5533 - acc: 0.7025 - val_loss: 0.5978 - val_acc: 0.6777\n",
      "Epoch 125/1000\n",
      " - 4s - loss: 0.5722 - acc: 0.6963 - val_loss: 0.6328 - val_acc: 0.6942\n",
      "Epoch 126/1000\n",
      " - 4s - loss: 0.5457 - acc: 0.7314 - val_loss: 0.5683 - val_acc: 0.6860\n",
      "Epoch 127/1000\n",
      " - 4s - loss: 0.5487 - acc: 0.7087 - val_loss: 0.5735 - val_acc: 0.6694\n",
      "Epoch 128/1000\n",
      " - 4s - loss: 0.5615 - acc: 0.6942 - val_loss: 0.5806 - val_acc: 0.6942\n",
      "Epoch 129/1000\n",
      " - 4s - loss: 0.5531 - acc: 0.7045 - val_loss: 0.5649 - val_acc: 0.6694\n",
      "Epoch 130/1000\n",
      " - 4s - loss: 0.5415 - acc: 0.7025 - val_loss: 0.5627 - val_acc: 0.6942\n",
      "Epoch 131/1000\n",
      " - 4s - loss: 0.5667 - acc: 0.7087 - val_loss: 0.5519 - val_acc: 0.6860\n",
      "Epoch 132/1000\n",
      " - 4s - loss: 0.5539 - acc: 0.7169 - val_loss: 0.5483 - val_acc: 0.6694\n",
      "Epoch 133/1000\n",
      " - 4s - loss: 0.5644 - acc: 0.7066 - val_loss: 0.5534 - val_acc: 0.6777\n",
      "Epoch 134/1000\n",
      " - 4s - loss: 0.5448 - acc: 0.7107 - val_loss: 0.5778 - val_acc: 0.6860\n",
      "Epoch 135/1000\n",
      " - 4s - loss: 0.5636 - acc: 0.7045 - val_loss: 0.5606 - val_acc: 0.6777\n",
      "Epoch 136/1000\n",
      " - 4s - loss: 0.5386 - acc: 0.7335 - val_loss: 0.6637 - val_acc: 0.6860\n",
      "Epoch 137/1000\n",
      " - 4s - loss: 0.5540 - acc: 0.7025 - val_loss: 0.5707 - val_acc: 0.6942\n",
      "Epoch 138/1000\n",
      " - 4s - loss: 0.5342 - acc: 0.7355 - val_loss: 0.5751 - val_acc: 0.7107\n",
      "Epoch 139/1000\n",
      " - 4s - loss: 0.5439 - acc: 0.6983 - val_loss: 0.5486 - val_acc: 0.7107\n",
      "Epoch 140/1000\n",
      " - 4s - loss: 0.5464 - acc: 0.7128 - val_loss: 0.6366 - val_acc: 0.6942\n",
      "Epoch 141/1000\n",
      " - 4s - loss: 0.5359 - acc: 0.7169 - val_loss: 0.5543 - val_acc: 0.7025\n",
      "Epoch 142/1000\n",
      " - 4s - loss: 0.5639 - acc: 0.7107 - val_loss: 0.5411 - val_acc: 0.6860\n",
      "Epoch 143/1000\n",
      " - 4s - loss: 0.5505 - acc: 0.7128 - val_loss: 0.5686 - val_acc: 0.7025\n",
      "Epoch 144/1000\n",
      " - 4s - loss: 0.5359 - acc: 0.7335 - val_loss: 0.5376 - val_acc: 0.7025\n",
      "Epoch 145/1000\n",
      " - 4s - loss: 0.5395 - acc: 0.7273 - val_loss: 0.6913 - val_acc: 0.6612\n",
      "Epoch 146/1000\n",
      " - 4s - loss: 0.5329 - acc: 0.7190 - val_loss: 0.5440 - val_acc: 0.7190\n",
      "Epoch 147/1000\n",
      " - 4s - loss: 0.5453 - acc: 0.7066 - val_loss: 0.5468 - val_acc: 0.6942\n",
      "Epoch 148/1000\n",
      " - 4s - loss: 0.5509 - acc: 0.7128 - val_loss: 0.5354 - val_acc: 0.7273\n",
      "Epoch 149/1000\n",
      " - 4s - loss: 0.5162 - acc: 0.7273 - val_loss: 0.6045 - val_acc: 0.6942\n",
      "Epoch 150/1000\n",
      " - 4s - loss: 0.5258 - acc: 0.7273 - val_loss: 0.5995 - val_acc: 0.7025\n",
      "Epoch 151/1000\n",
      " - 4s - loss: 0.5306 - acc: 0.7314 - val_loss: 0.6637 - val_acc: 0.6612\n",
      "Epoch 152/1000\n",
      " - 4s - loss: 0.5476 - acc: 0.7314 - val_loss: 0.5272 - val_acc: 0.7107\n",
      "Epoch 153/1000\n",
      " - 4s - loss: 0.5352 - acc: 0.7252 - val_loss: 0.5421 - val_acc: 0.7190\n",
      "Epoch 154/1000\n",
      " - 4s - loss: 0.5378 - acc: 0.7252 - val_loss: 0.5906 - val_acc: 0.6942\n",
      "Epoch 155/1000\n",
      " - 4s - loss: 0.5303 - acc: 0.7045 - val_loss: 0.6175 - val_acc: 0.6942\n",
      "Epoch 156/1000\n",
      " - 4s - loss: 0.5386 - acc: 0.7231 - val_loss: 0.5547 - val_acc: 0.7273\n",
      "Epoch 157/1000\n",
      " - 4s - loss: 0.5170 - acc: 0.7190 - val_loss: 0.5514 - val_acc: 0.7107\n",
      "Epoch 158/1000\n",
      " - 4s - loss: 0.5115 - acc: 0.7293 - val_loss: 0.5178 - val_acc: 0.7107\n",
      "Epoch 159/1000\n",
      " - 4s - loss: 0.5143 - acc: 0.7335 - val_loss: 0.6989 - val_acc: 0.6529\n",
      "Epoch 160/1000\n",
      " - 4s - loss: 0.5128 - acc: 0.7500 - val_loss: 0.6848 - val_acc: 0.6942\n",
      "Epoch 161/1000\n",
      " - 4s - loss: 0.5138 - acc: 0.7397 - val_loss: 0.6064 - val_acc: 0.6942\n",
      "Epoch 162/1000\n",
      " - 4s - loss: 0.5393 - acc: 0.7025 - val_loss: 0.7005 - val_acc: 0.6612\n",
      "Epoch 163/1000\n",
      " - 4s - loss: 0.5227 - acc: 0.7438 - val_loss: 0.5130 - val_acc: 0.7355\n",
      "Epoch 164/1000\n",
      " - 4s - loss: 0.5164 - acc: 0.7355 - val_loss: 0.8495 - val_acc: 0.6281\n",
      "Epoch 165/1000\n",
      " - 4s - loss: 0.5114 - acc: 0.7438 - val_loss: 0.5293 - val_acc: 0.7355\n",
      "Epoch 166/1000\n",
      " - 4s - loss: 0.5121 - acc: 0.7541 - val_loss: 0.7791 - val_acc: 0.6364\n",
      "Epoch 167/1000\n",
      " - 4s - loss: 0.5272 - acc: 0.7273 - val_loss: 0.5423 - val_acc: 0.7355\n",
      "Epoch 168/1000\n",
      " - 4s - loss: 0.5074 - acc: 0.7500 - val_loss: 0.5431 - val_acc: 0.7438\n",
      "Epoch 169/1000\n",
      " - 4s - loss: 0.5139 - acc: 0.7397 - val_loss: 0.6042 - val_acc: 0.7107\n",
      "Epoch 170/1000\n",
      " - 4s - loss: 0.5014 - acc: 0.7562 - val_loss: 0.5301 - val_acc: 0.7355\n",
      "Epoch 171/1000\n",
      " - 4s - loss: 0.5034 - acc: 0.7583 - val_loss: 0.5125 - val_acc: 0.7769\n",
      "Epoch 172/1000\n",
      " - 4s - loss: 0.5065 - acc: 0.7769 - val_loss: 0.8941 - val_acc: 0.6364\n",
      "Epoch 173/1000\n",
      " - 4s - loss: 0.5155 - acc: 0.7314 - val_loss: 0.5262 - val_acc: 0.7603\n",
      "Epoch 174/1000\n",
      " - 4s - loss: 0.5222 - acc: 0.7107 - val_loss: 0.4993 - val_acc: 0.7769\n",
      "Epoch 175/1000\n",
      " - 4s - loss: 0.5253 - acc: 0.7335 - val_loss: 0.5401 - val_acc: 0.7190\n",
      "Epoch 176/1000\n",
      " - 4s - loss: 0.5168 - acc: 0.7397 - val_loss: 0.4779 - val_acc: 0.7769\n",
      "Epoch 177/1000\n",
      " - 4s - loss: 0.5170 - acc: 0.7500 - val_loss: 0.5239 - val_acc: 0.7603\n",
      "Epoch 178/1000\n",
      " - 4s - loss: 0.5175 - acc: 0.7293 - val_loss: 0.4837 - val_acc: 0.7603\n",
      "Epoch 179/1000\n",
      " - 4s - loss: 0.5241 - acc: 0.7438 - val_loss: 0.4912 - val_acc: 0.7603\n",
      "Epoch 180/1000\n",
      " - 4s - loss: 0.5126 - acc: 0.7190 - val_loss: 0.5431 - val_acc: 0.7355\n",
      "Epoch 181/1000\n",
      " - 4s - loss: 0.4995 - acc: 0.7686 - val_loss: 1.0068 - val_acc: 0.5372\n",
      "Epoch 182/1000\n",
      " - 4s - loss: 0.5054 - acc: 0.7397 - val_loss: 0.9259 - val_acc: 0.5372\n",
      "Epoch 183/1000\n",
      " - 4s - loss: 0.5107 - acc: 0.7376 - val_loss: 0.6524 - val_acc: 0.6694\n",
      "Epoch 184/1000\n",
      " - 4s - loss: 0.4649 - acc: 0.7686 - val_loss: 0.5620 - val_acc: 0.7107\n",
      "Epoch 185/1000\n",
      " - 4s - loss: 0.4957 - acc: 0.7479 - val_loss: 0.4871 - val_acc: 0.7769\n",
      "Epoch 186/1000\n",
      " - 4s - loss: 0.5055 - acc: 0.7293 - val_loss: 0.5005 - val_acc: 0.7769\n",
      "Epoch 187/1000\n",
      " - 4s - loss: 0.5225 - acc: 0.7355 - val_loss: 0.4730 - val_acc: 0.7934\n",
      "Epoch 188/1000\n",
      " - 4s - loss: 0.4912 - acc: 0.7314 - val_loss: 0.5409 - val_acc: 0.7355\n",
      "Epoch 189/1000\n",
      " - 4s - loss: 0.4868 - acc: 0.7521 - val_loss: 0.7918 - val_acc: 0.6364\n",
      "Epoch 190/1000\n",
      " - 4s - loss: 0.4646 - acc: 0.7603 - val_loss: 0.6639 - val_acc: 0.7025\n",
      "Epoch 191/1000\n",
      " - 4s - loss: 0.4772 - acc: 0.7603 - val_loss: 0.5435 - val_acc: 0.7355\n",
      "Epoch 192/1000\n",
      " - 4s - loss: 0.4989 - acc: 0.7500 - val_loss: 1.0430 - val_acc: 0.5785\n",
      "Epoch 193/1000\n",
      " - 4s - loss: 0.4825 - acc: 0.7500 - val_loss: 0.5199 - val_acc: 0.7603\n",
      "Epoch 194/1000\n",
      " - 4s - loss: 0.4911 - acc: 0.7872 - val_loss: 0.4738 - val_acc: 0.8099\n",
      "Epoch 195/1000\n",
      " - 4s - loss: 0.4854 - acc: 0.7376 - val_loss: 0.9549 - val_acc: 0.6033\n",
      "Epoch 196/1000\n",
      " - 4s - loss: 0.4911 - acc: 0.7686 - val_loss: 0.6227 - val_acc: 0.6694\n",
      "Epoch 197/1000\n",
      " - 4s - loss: 0.4758 - acc: 0.7686 - val_loss: 1.3579 - val_acc: 0.5041\n",
      "Epoch 198/1000\n",
      " - 4s - loss: 0.4730 - acc: 0.7893 - val_loss: 0.5193 - val_acc: 0.7603\n",
      "Epoch 199/1000\n",
      " - 4s - loss: 0.4803 - acc: 0.7686 - val_loss: 1.1345 - val_acc: 0.5289\n",
      "Epoch 200/1000\n",
      " - 4s - loss: 0.4872 - acc: 0.7397 - val_loss: 0.5425 - val_acc: 0.7521\n",
      "Epoch 201/1000\n",
      " - 4s - loss: 0.4615 - acc: 0.7707 - val_loss: 0.9779 - val_acc: 0.5702\n",
      "Epoch 202/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.4845 - acc: 0.7562 - val_loss: 1.0716 - val_acc: 0.5868\n",
      "Epoch 203/1000\n",
      " - 4s - loss: 0.4914 - acc: 0.7665 - val_loss: 1.1408 - val_acc: 0.5785\n",
      "Epoch 204/1000\n",
      " - 4s - loss: 0.4595 - acc: 0.7707 - val_loss: 0.8929 - val_acc: 0.5950\n",
      "Epoch 205/1000\n",
      " - 4s - loss: 0.4888 - acc: 0.7479 - val_loss: 0.5607 - val_acc: 0.7107\n",
      "Epoch 206/1000\n",
      " - 4s - loss: 0.4970 - acc: 0.7541 - val_loss: 2.0025 - val_acc: 0.5041\n",
      "Epoch 207/1000\n",
      " - 4s - loss: 0.4819 - acc: 0.7810 - val_loss: 0.8030 - val_acc: 0.6694\n",
      "Epoch 208/1000\n",
      " - 4s - loss: 0.4673 - acc: 0.7686 - val_loss: 0.4612 - val_acc: 0.8347\n",
      "Epoch 209/1000\n",
      " - 4s - loss: 0.4719 - acc: 0.7479 - val_loss: 0.4439 - val_acc: 0.8264\n",
      "Epoch 210/1000\n",
      " - 4s - loss: 0.4863 - acc: 0.7521 - val_loss: 0.4635 - val_acc: 0.8182\n",
      "Epoch 211/1000\n",
      " - 4s - loss: 0.4694 - acc: 0.7769 - val_loss: 0.4427 - val_acc: 0.7934\n",
      "Epoch 212/1000\n",
      " - 4s - loss: 0.4893 - acc: 0.7727 - val_loss: 0.4500 - val_acc: 0.8264\n",
      "Epoch 213/1000\n",
      " - 4s - loss: 0.4731 - acc: 0.7624 - val_loss: 0.8348 - val_acc: 0.6612\n",
      "Epoch 214/1000\n",
      " - 4s - loss: 0.4710 - acc: 0.7686 - val_loss: 0.8675 - val_acc: 0.6281\n",
      "Epoch 215/1000\n",
      " - 4s - loss: 0.4615 - acc: 0.7521 - val_loss: 0.5279 - val_acc: 0.7438\n",
      "Epoch 216/1000\n",
      " - 4s - loss: 0.4643 - acc: 0.7769 - val_loss: 0.7450 - val_acc: 0.6612\n",
      "Epoch 217/1000\n",
      " - 4s - loss: 0.4833 - acc: 0.7645 - val_loss: 0.4367 - val_acc: 0.8182\n",
      "Epoch 218/1000\n",
      " - 4s - loss: 0.4820 - acc: 0.7624 - val_loss: 0.7134 - val_acc: 0.6694\n",
      "Epoch 219/1000\n",
      " - 4s - loss: 0.4792 - acc: 0.7603 - val_loss: 0.6303 - val_acc: 0.6860\n",
      "Epoch 220/1000\n",
      " - 4s - loss: 0.4791 - acc: 0.7686 - val_loss: 0.4283 - val_acc: 0.8182\n",
      "Epoch 221/1000\n",
      " - 4s - loss: 0.4855 - acc: 0.7624 - val_loss: 0.4755 - val_acc: 0.7851\n",
      "Epoch 222/1000\n",
      " - 4s - loss: 0.4631 - acc: 0.7934 - val_loss: 0.5113 - val_acc: 0.7769\n",
      "Epoch 223/1000\n",
      " - 4s - loss: 0.4783 - acc: 0.7789 - val_loss: 0.7491 - val_acc: 0.6694\n",
      "Epoch 224/1000\n",
      " - 4s - loss: 0.4383 - acc: 0.7934 - val_loss: 0.4599 - val_acc: 0.7934\n",
      "Epoch 225/1000\n",
      " - 4s - loss: 0.4744 - acc: 0.7562 - val_loss: 0.5542 - val_acc: 0.7355\n",
      "Epoch 226/1000\n",
      " - 4s - loss: 0.4675 - acc: 0.7686 - val_loss: 0.4315 - val_acc: 0.8347\n",
      "Epoch 227/1000\n",
      " - 4s - loss: 0.4275 - acc: 0.7934 - val_loss: 0.4826 - val_acc: 0.7934\n",
      "Epoch 228/1000\n",
      " - 4s - loss: 0.4585 - acc: 0.7851 - val_loss: 0.4245 - val_acc: 0.8017\n",
      "Epoch 229/1000\n",
      " - 4s - loss: 0.4622 - acc: 0.7955 - val_loss: 0.6507 - val_acc: 0.6942\n",
      "Epoch 230/1000\n",
      " - 4s - loss: 0.4559 - acc: 0.7789 - val_loss: 0.4595 - val_acc: 0.7934\n",
      "Epoch 231/1000\n",
      " - 4s - loss: 0.4480 - acc: 0.7831 - val_loss: 0.9342 - val_acc: 0.6364\n",
      "Epoch 232/1000\n",
      " - 4s - loss: 0.4584 - acc: 0.7851 - val_loss: 0.9453 - val_acc: 0.5868\n",
      "Epoch 233/1000\n",
      " - 4s - loss: 0.4304 - acc: 0.8079 - val_loss: 0.6394 - val_acc: 0.7107\n",
      "Epoch 234/1000\n",
      " - 4s - loss: 0.4617 - acc: 0.7975 - val_loss: 0.4869 - val_acc: 0.7769\n",
      "Epoch 235/1000\n",
      " - 4s - loss: 0.4899 - acc: 0.7562 - val_loss: 1.3989 - val_acc: 0.5041\n",
      "Epoch 236/1000\n",
      " - 4s - loss: 0.4441 - acc: 0.7624 - val_loss: 0.4490 - val_acc: 0.7851\n",
      "Epoch 237/1000\n",
      " - 4s - loss: 0.4354 - acc: 0.7872 - val_loss: 0.5703 - val_acc: 0.7438\n",
      "Epoch 238/1000\n",
      " - 4s - loss: 0.4513 - acc: 0.7748 - val_loss: 0.7596 - val_acc: 0.6694\n",
      "Epoch 239/1000\n",
      " - 4s - loss: 0.4528 - acc: 0.7665 - val_loss: 0.4759 - val_acc: 0.7934\n",
      "Epoch 240/1000\n",
      " - 4s - loss: 0.4614 - acc: 0.7748 - val_loss: 0.8211 - val_acc: 0.6446\n",
      "Epoch 241/1000\n",
      " - 4s - loss: 0.4401 - acc: 0.8017 - val_loss: 0.6378 - val_acc: 0.7107\n",
      "Epoch 242/1000\n",
      " - 4s - loss: 0.4473 - acc: 0.7955 - val_loss: 0.6423 - val_acc: 0.7190\n",
      "Epoch 243/1000\n",
      " - 4s - loss: 0.4534 - acc: 0.7975 - val_loss: 0.5364 - val_acc: 0.7603\n",
      "Epoch 244/1000\n",
      " - 4s - loss: 0.4147 - acc: 0.8202 - val_loss: 1.2910 - val_acc: 0.5289\n",
      "Epoch 245/1000\n",
      " - 4s - loss: 0.4239 - acc: 0.7955 - val_loss: 0.6456 - val_acc: 0.7107\n",
      "Epoch 246/1000\n",
      " - 4s - loss: 0.4152 - acc: 0.8120 - val_loss: 0.4150 - val_acc: 0.8099\n",
      "Epoch 247/1000\n",
      " - 4s - loss: 0.4448 - acc: 0.7913 - val_loss: 0.5823 - val_acc: 0.7273\n",
      "Epoch 248/1000\n",
      " - 4s - loss: 0.4400 - acc: 0.7996 - val_loss: 0.5561 - val_acc: 0.7355\n",
      "Epoch 249/1000\n",
      " - 4s - loss: 0.4251 - acc: 0.7996 - val_loss: 0.4662 - val_acc: 0.7851\n",
      "Epoch 250/1000\n",
      " - 4s - loss: 0.4085 - acc: 0.8099 - val_loss: 0.8034 - val_acc: 0.6777\n",
      "Epoch 251/1000\n",
      " - 4s - loss: 0.4419 - acc: 0.7810 - val_loss: 1.1861 - val_acc: 0.5289\n",
      "Epoch 252/1000\n",
      " - 4s - loss: 0.4272 - acc: 0.7913 - val_loss: 0.4440 - val_acc: 0.7934\n",
      "Epoch 253/1000\n",
      " - 4s - loss: 0.4474 - acc: 0.7831 - val_loss: 0.6997 - val_acc: 0.6860\n",
      "Epoch 254/1000\n",
      " - 4s - loss: 0.4276 - acc: 0.7975 - val_loss: 0.7873 - val_acc: 0.6612\n",
      "Epoch 255/1000\n",
      " - 4s - loss: 0.4441 - acc: 0.7810 - val_loss: 0.7297 - val_acc: 0.6942\n",
      "Epoch 256/1000\n",
      " - 4s - loss: 0.4311 - acc: 0.7872 - val_loss: 1.3278 - val_acc: 0.5372\n",
      "Epoch 257/1000\n",
      " - 4s - loss: 0.4047 - acc: 0.8202 - val_loss: 0.4892 - val_acc: 0.7934\n",
      "Epoch 258/1000\n",
      " - 4s - loss: 0.4390 - acc: 0.7934 - val_loss: 1.3709 - val_acc: 0.5124\n",
      "Epoch 259/1000\n",
      " - 4s - loss: 0.4195 - acc: 0.8037 - val_loss: 1.6278 - val_acc: 0.5041\n",
      "Epoch 260/1000\n",
      " - 4s - loss: 0.4028 - acc: 0.8140 - val_loss: 0.8390 - val_acc: 0.6364\n",
      "Epoch 261/1000\n",
      " - 4s - loss: 0.4769 - acc: 0.7872 - val_loss: 1.1334 - val_acc: 0.6281\n",
      "Epoch 262/1000\n",
      " - 4s - loss: 0.4183 - acc: 0.8368 - val_loss: 0.5316 - val_acc: 0.7190\n",
      "Epoch 263/1000\n",
      " - 4s - loss: 0.4244 - acc: 0.8079 - val_loss: 0.4804 - val_acc: 0.8017\n",
      "Epoch 264/1000\n",
      " - 4s - loss: 0.4157 - acc: 0.8244 - val_loss: 0.5699 - val_acc: 0.7355\n",
      "Epoch 265/1000\n",
      " - 4s - loss: 0.4276 - acc: 0.7934 - val_loss: 0.4192 - val_acc: 0.8099\n",
      "Epoch 266/1000\n",
      " - 4s - loss: 0.4415 - acc: 0.7769 - val_loss: 0.4484 - val_acc: 0.8595\n",
      "Epoch 267/1000\n",
      " - 4s - loss: 0.4383 - acc: 0.7934 - val_loss: 0.8914 - val_acc: 0.6116\n",
      "Epoch 268/1000\n",
      " - 4s - loss: 0.4556 - acc: 0.7727 - val_loss: 0.4466 - val_acc: 0.8347\n",
      "Epoch 269/1000\n",
      " - 4s - loss: 0.4250 - acc: 0.7934 - val_loss: 0.5444 - val_acc: 0.7355\n",
      "Epoch 270/1000\n",
      " - 4s - loss: 0.4192 - acc: 0.8037 - val_loss: 0.6378 - val_acc: 0.7355\n",
      "Epoch 271/1000\n",
      " - 4s - loss: 0.4148 - acc: 0.7996 - val_loss: 0.4210 - val_acc: 0.8264\n",
      "Epoch 272/1000\n",
      " - 4s - loss: 0.4127 - acc: 0.8223 - val_loss: 0.4378 - val_acc: 0.8099\n",
      "Epoch 273/1000\n",
      " - 4s - loss: 0.4331 - acc: 0.7955 - val_loss: 0.8535 - val_acc: 0.6529\n",
      "Epoch 274/1000\n",
      " - 4s - loss: 0.4148 - acc: 0.8140 - val_loss: 1.4977 - val_acc: 0.5124\n",
      "Epoch 275/1000\n",
      " - 4s - loss: 0.4222 - acc: 0.7913 - val_loss: 0.4114 - val_acc: 0.8017\n",
      "Epoch 276/1000\n",
      " - 4s - loss: 0.4274 - acc: 0.8120 - val_loss: 0.9562 - val_acc: 0.6446\n",
      "Epoch 277/1000\n",
      " - 4s - loss: 0.3888 - acc: 0.8223 - val_loss: 0.4495 - val_acc: 0.8264\n",
      "Epoch 278/1000\n",
      " - 4s - loss: 0.4342 - acc: 0.8058 - val_loss: 0.7954 - val_acc: 0.6860\n",
      "Epoch 279/1000\n",
      " - 4s - loss: 0.4305 - acc: 0.7996 - val_loss: 0.4834 - val_acc: 0.7851\n",
      "Epoch 280/1000\n",
      " - 4s - loss: 0.4065 - acc: 0.8058 - val_loss: 0.4443 - val_acc: 0.8099\n",
      "Epoch 281/1000\n",
      " - 4s - loss: 0.4302 - acc: 0.7996 - val_loss: 0.5138 - val_acc: 0.7521\n",
      "Epoch 282/1000\n",
      " - 4s - loss: 0.4366 - acc: 0.7955 - val_loss: 0.4360 - val_acc: 0.8017\n",
      "Epoch 283/1000\n",
      " - 4s - loss: 0.4166 - acc: 0.8099 - val_loss: 1.3208 - val_acc: 0.5207\n",
      "Epoch 284/1000\n",
      " - 4s - loss: 0.4226 - acc: 0.8017 - val_loss: 0.9065 - val_acc: 0.6116\n",
      "Epoch 285/1000\n",
      " - 4s - loss: 0.4337 - acc: 0.8182 - val_loss: 0.5238 - val_acc: 0.7603\n",
      "Epoch 286/1000\n",
      " - 4s - loss: 0.4238 - acc: 0.8079 - val_loss: 0.4322 - val_acc: 0.8182\n",
      "Epoch 287/1000\n",
      " - 4s - loss: 0.4549 - acc: 0.7810 - val_loss: 0.4230 - val_acc: 0.8017\n",
      "Epoch 288/1000\n",
      " - 4s - loss: 0.4213 - acc: 0.8223 - val_loss: 0.8932 - val_acc: 0.6446\n",
      "Epoch 289/1000\n",
      " - 4s - loss: 0.4385 - acc: 0.7975 - val_loss: 0.5069 - val_acc: 0.7686\n",
      "Epoch 290/1000\n",
      " - 4s - loss: 0.4477 - acc: 0.7789 - val_loss: 1.1912 - val_acc: 0.6116\n",
      "Epoch 291/1000\n",
      " - 4s - loss: 0.4269 - acc: 0.7810 - val_loss: 0.8557 - val_acc: 0.5868\n",
      "Epoch 292/1000\n",
      " - 4s - loss: 0.4280 - acc: 0.7913 - val_loss: 0.5415 - val_acc: 0.7521\n",
      "Epoch 293/1000\n",
      " - 4s - loss: 0.4160 - acc: 0.8079 - val_loss: 0.9459 - val_acc: 0.6198\n",
      "Epoch 294/1000\n",
      " - 4s - loss: 0.4245 - acc: 0.8202 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 295/1000\n",
      " - 4s - loss: 0.4118 - acc: 0.8058 - val_loss: 1.0272 - val_acc: 0.5537\n",
      "Epoch 296/1000\n",
      " - 4s - loss: 0.4076 - acc: 0.8140 - val_loss: 0.5000 - val_acc: 0.8017\n",
      "Epoch 297/1000\n",
      " - 4s - loss: 0.4424 - acc: 0.8017 - val_loss: 0.4214 - val_acc: 0.8264\n",
      "Epoch 298/1000\n",
      " - 4s - loss: 0.4162 - acc: 0.7975 - val_loss: 0.4903 - val_acc: 0.7603\n",
      "Epoch 299/1000\n",
      " - 4s - loss: 0.4064 - acc: 0.7955 - val_loss: 0.5879 - val_acc: 0.7438\n",
      "Epoch 300/1000\n",
      " - 4s - loss: 0.4052 - acc: 0.8140 - val_loss: 0.4536 - val_acc: 0.8099\n",
      "Epoch 301/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.4327 - acc: 0.7872 - val_loss: 0.4653 - val_acc: 0.7934\n",
      "Epoch 302/1000\n",
      " - 4s - loss: 0.4260 - acc: 0.8017 - val_loss: 0.6911 - val_acc: 0.6612\n",
      "Epoch 303/1000\n",
      " - 4s - loss: 0.3923 - acc: 0.8223 - val_loss: 0.5650 - val_acc: 0.7355\n",
      "Epoch 304/1000\n",
      " - 4s - loss: 0.4251 - acc: 0.7913 - val_loss: 0.7392 - val_acc: 0.6942\n",
      "Epoch 305/1000\n",
      " - 4s - loss: 0.4200 - acc: 0.8120 - val_loss: 0.4392 - val_acc: 0.7934\n",
      "Epoch 306/1000\n",
      " - 4s - loss: 0.3920 - acc: 0.8058 - val_loss: 1.0432 - val_acc: 0.5868\n",
      "Epoch 307/1000\n",
      " - 4s - loss: 0.4002 - acc: 0.8182 - val_loss: 0.5928 - val_acc: 0.7273\n",
      "Epoch 308/1000\n",
      " - 4s - loss: 0.3991 - acc: 0.8264 - val_loss: 0.4601 - val_acc: 0.8182\n",
      "Epoch 309/1000\n",
      " - 4s - loss: 0.4471 - acc: 0.7851 - val_loss: 0.5640 - val_acc: 0.7273\n",
      "Epoch 310/1000\n",
      " - 4s - loss: 0.4056 - acc: 0.8182 - val_loss: 0.4485 - val_acc: 0.8099\n",
      "Epoch 311/1000\n",
      " - 4s - loss: 0.3993 - acc: 0.8182 - val_loss: 0.9777 - val_acc: 0.6116\n",
      "Epoch 312/1000\n",
      " - 4s - loss: 0.4058 - acc: 0.8161 - val_loss: 0.6086 - val_acc: 0.7107\n",
      "Epoch 313/1000\n",
      " - 4s - loss: 0.4013 - acc: 0.8202 - val_loss: 1.1671 - val_acc: 0.6364\n",
      "Epoch 314/1000\n",
      " - 4s - loss: 0.4157 - acc: 0.7996 - val_loss: 0.8517 - val_acc: 0.6529\n",
      "Epoch 315/1000\n",
      " - 4s - loss: 0.3987 - acc: 0.8285 - val_loss: 0.6575 - val_acc: 0.7107\n",
      "Epoch 316/1000\n",
      " - 4s - loss: 0.3746 - acc: 0.8182 - val_loss: 0.4215 - val_acc: 0.8264\n",
      "Epoch 317/1000\n",
      " - 4s - loss: 0.3920 - acc: 0.8244 - val_loss: 0.4331 - val_acc: 0.8182\n",
      "Epoch 318/1000\n",
      " - 4s - loss: 0.4174 - acc: 0.7975 - val_loss: 1.6871 - val_acc: 0.5124\n",
      "Epoch 319/1000\n",
      " - 4s - loss: 0.4021 - acc: 0.8017 - val_loss: 0.6079 - val_acc: 0.7273\n",
      "Epoch 320/1000\n",
      " - 4s - loss: 0.4154 - acc: 0.8099 - val_loss: 0.4906 - val_acc: 0.8017\n",
      "Epoch 321/1000\n",
      " - 4s - loss: 0.3986 - acc: 0.8223 - val_loss: 0.4285 - val_acc: 0.8182\n",
      "Epoch 322/1000\n",
      " - 4s - loss: 0.3796 - acc: 0.8244 - val_loss: 0.4870 - val_acc: 0.8017\n",
      "Epoch 323/1000\n",
      " - 4s - loss: 0.3966 - acc: 0.7975 - val_loss: 0.4364 - val_acc: 0.8182\n",
      "Epoch 324/1000\n",
      " - 4s - loss: 0.3942 - acc: 0.8223 - val_loss: 0.9429 - val_acc: 0.6446\n",
      "Epoch 325/1000\n",
      " - 4s - loss: 0.4000 - acc: 0.8140 - val_loss: 0.9293 - val_acc: 0.6529\n",
      "Epoch 326/1000\n",
      " - 4s - loss: 0.3995 - acc: 0.8202 - val_loss: 0.4510 - val_acc: 0.8182\n",
      "Epoch 327/1000\n",
      " - 4s - loss: 0.3870 - acc: 0.8368 - val_loss: 0.5533 - val_acc: 0.7438\n",
      "Epoch 328/1000\n",
      " - 4s - loss: 0.3985 - acc: 0.8037 - val_loss: 1.3741 - val_acc: 0.5207\n",
      "Epoch 329/1000\n",
      " - 4s - loss: 0.3873 - acc: 0.8182 - val_loss: 0.5255 - val_acc: 0.7851\n",
      "Epoch 330/1000\n",
      " - 4s - loss: 0.3897 - acc: 0.8161 - val_loss: 1.1584 - val_acc: 0.5537\n",
      "Epoch 331/1000\n",
      " - 4s - loss: 0.3960 - acc: 0.8079 - val_loss: 1.3877 - val_acc: 0.5702\n",
      "Epoch 332/1000\n",
      " - 4s - loss: 0.4297 - acc: 0.7913 - val_loss: 0.7616 - val_acc: 0.6777\n",
      "Epoch 333/1000\n",
      " - 4s - loss: 0.3934 - acc: 0.8285 - val_loss: 1.3130 - val_acc: 0.6198\n",
      "Epoch 334/1000\n",
      " - 4s - loss: 0.3945 - acc: 0.8244 - val_loss: 0.8248 - val_acc: 0.6860\n",
      "Epoch 335/1000\n",
      " - 4s - loss: 0.4070 - acc: 0.8140 - val_loss: 0.7106 - val_acc: 0.7273\n",
      "Epoch 336/1000\n",
      " - 4s - loss: 0.3937 - acc: 0.8140 - val_loss: 0.4567 - val_acc: 0.8182\n",
      "Epoch 337/1000\n",
      " - 4s - loss: 0.3846 - acc: 0.8285 - val_loss: 1.0277 - val_acc: 0.6529\n",
      "Epoch 338/1000\n",
      " - 4s - loss: 0.3832 - acc: 0.8306 - val_loss: 1.6617 - val_acc: 0.5207\n",
      "Epoch 339/1000\n",
      " - 4s - loss: 0.3905 - acc: 0.8202 - val_loss: 0.4230 - val_acc: 0.7934\n",
      "Epoch 340/1000\n",
      " - 4s - loss: 0.3786 - acc: 0.8182 - val_loss: 1.0399 - val_acc: 0.6529\n",
      "Epoch 341/1000\n",
      " - 4s - loss: 0.3759 - acc: 0.8368 - val_loss: 0.7922 - val_acc: 0.6860\n",
      "Epoch 342/1000\n",
      " - 4s - loss: 0.3769 - acc: 0.8244 - val_loss: 1.6444 - val_acc: 0.5124\n",
      "Epoch 343/1000\n",
      " - 4s - loss: 0.4032 - acc: 0.8264 - val_loss: 0.5937 - val_acc: 0.7355\n",
      "Epoch 344/1000\n",
      " - 4s - loss: 0.3568 - acc: 0.8285 - val_loss: 1.2880 - val_acc: 0.6116\n",
      "Epoch 345/1000\n",
      " - 4s - loss: 0.3828 - acc: 0.8161 - val_loss: 0.6262 - val_acc: 0.7355\n",
      "Epoch 346/1000\n",
      " - 4s - loss: 0.3990 - acc: 0.7996 - val_loss: 0.9002 - val_acc: 0.6694\n",
      "Epoch 347/1000\n",
      " - 4s - loss: 0.3751 - acc: 0.8326 - val_loss: 0.9940 - val_acc: 0.5868\n",
      "Epoch 348/1000\n",
      " - 4s - loss: 0.3858 - acc: 0.8409 - val_loss: 0.4154 - val_acc: 0.8264\n",
      "Epoch 349/1000\n",
      " - 4s - loss: 0.4193 - acc: 0.7955 - val_loss: 0.4453 - val_acc: 0.8099\n",
      "Epoch 350/1000\n",
      " - 4s - loss: 0.3829 - acc: 0.8368 - val_loss: 0.5538 - val_acc: 0.7603\n",
      "Epoch 351/1000\n",
      " - 4s - loss: 0.3728 - acc: 0.8326 - val_loss: 1.2982 - val_acc: 0.5372\n",
      "Epoch 352/1000\n",
      " - 4s - loss: 0.4170 - acc: 0.8182 - val_loss: 0.9505 - val_acc: 0.6777\n",
      "Epoch 353/1000\n",
      " - 4s - loss: 0.3619 - acc: 0.8430 - val_loss: 0.4763 - val_acc: 0.8017\n",
      "Epoch 354/1000\n",
      " - 4s - loss: 0.3744 - acc: 0.8244 - val_loss: 0.4964 - val_acc: 0.8347\n",
      "Epoch 355/1000\n",
      " - 4s - loss: 0.3927 - acc: 0.8202 - val_loss: 0.8290 - val_acc: 0.6694\n",
      "Epoch 356/1000\n",
      " - 4s - loss: 0.4166 - acc: 0.7996 - val_loss: 0.5841 - val_acc: 0.7438\n",
      "Epoch 357/1000\n",
      " - 4s - loss: 0.3959 - acc: 0.8140 - val_loss: 0.6406 - val_acc: 0.7355\n",
      "Epoch 358/1000\n",
      " - 4s - loss: 0.3701 - acc: 0.8285 - val_loss: 0.4393 - val_acc: 0.8347\n",
      "Epoch 359/1000\n",
      " - 4s - loss: 0.3486 - acc: 0.8471 - val_loss: 0.7116 - val_acc: 0.7107\n",
      "Epoch 360/1000\n",
      " - 4s - loss: 0.3732 - acc: 0.8326 - val_loss: 0.6308 - val_acc: 0.7355\n",
      "Epoch 361/1000\n",
      " - 4s - loss: 0.3743 - acc: 0.8306 - val_loss: 0.6784 - val_acc: 0.7025\n",
      "Epoch 362/1000\n",
      " - 4s - loss: 0.3789 - acc: 0.8161 - val_loss: 1.1887 - val_acc: 0.6364\n",
      "Epoch 363/1000\n",
      " - 4s - loss: 0.3630 - acc: 0.8368 - val_loss: 0.6783 - val_acc: 0.7190\n",
      "Epoch 364/1000\n",
      " - 4s - loss: 0.3650 - acc: 0.8326 - val_loss: 0.4384 - val_acc: 0.8182\n",
      "Epoch 365/1000\n",
      " - 4s - loss: 0.4083 - acc: 0.8037 - val_loss: 0.4965 - val_acc: 0.8099\n",
      "Epoch 366/1000\n",
      " - 4s - loss: 0.3631 - acc: 0.8409 - val_loss: 1.4305 - val_acc: 0.5950\n",
      "Epoch 367/1000\n",
      " - 4s - loss: 0.3799 - acc: 0.8202 - val_loss: 1.4439 - val_acc: 0.5289\n",
      "Epoch 368/1000\n",
      " - 4s - loss: 0.4002 - acc: 0.8202 - val_loss: 0.4197 - val_acc: 0.8430\n",
      "Epoch 369/1000\n",
      " - 4s - loss: 0.4067 - acc: 0.8037 - val_loss: 0.5105 - val_acc: 0.7851\n",
      "Epoch 370/1000\n",
      " - 4s - loss: 0.3647 - acc: 0.8264 - val_loss: 0.4109 - val_acc: 0.8182\n",
      "Epoch 371/1000\n",
      " - 4s - loss: 0.3764 - acc: 0.8306 - val_loss: 0.4775 - val_acc: 0.8099\n",
      "Epoch 372/1000\n",
      " - 4s - loss: 0.3745 - acc: 0.8244 - val_loss: 0.8929 - val_acc: 0.6033\n",
      "Epoch 373/1000\n",
      " - 4s - loss: 0.3722 - acc: 0.8306 - val_loss: 0.7572 - val_acc: 0.6860\n",
      "Epoch 374/1000\n",
      " - 4s - loss: 0.3507 - acc: 0.8409 - val_loss: 0.6543 - val_acc: 0.7355\n",
      "Epoch 375/1000\n",
      " - 4s - loss: 0.4035 - acc: 0.8326 - val_loss: 0.4252 - val_acc: 0.8347\n",
      "Epoch 376/1000\n",
      " - 4s - loss: 0.4010 - acc: 0.8037 - val_loss: 0.6147 - val_acc: 0.7107\n",
      "Epoch 377/1000\n",
      " - 4s - loss: 0.4031 - acc: 0.8347 - val_loss: 0.4207 - val_acc: 0.8430\n",
      "Epoch 378/1000\n",
      " - 4s - loss: 0.3628 - acc: 0.8409 - val_loss: 0.6729 - val_acc: 0.7273\n",
      "Epoch 379/1000\n",
      " - 4s - loss: 0.3872 - acc: 0.8409 - val_loss: 0.5375 - val_acc: 0.7686\n",
      "Epoch 380/1000\n",
      " - 4s - loss: 0.3518 - acc: 0.8636 - val_loss: 1.0181 - val_acc: 0.5785\n",
      "Epoch 381/1000\n",
      " - 4s - loss: 0.3765 - acc: 0.8368 - val_loss: 0.7540 - val_acc: 0.6777\n",
      "Epoch 382/1000\n",
      " - 4s - loss: 0.3776 - acc: 0.8223 - val_loss: 0.4909 - val_acc: 0.7769\n",
      "Epoch 383/1000\n",
      " - 4s - loss: 0.3673 - acc: 0.8244 - val_loss: 0.7990 - val_acc: 0.6446\n",
      "Epoch 384/1000\n",
      " - 4s - loss: 0.3715 - acc: 0.8388 - val_loss: 0.7417 - val_acc: 0.6777\n",
      "Epoch 385/1000\n",
      " - 4s - loss: 0.3469 - acc: 0.8430 - val_loss: 0.3971 - val_acc: 0.8430\n",
      "Epoch 386/1000\n",
      " - 4s - loss: 0.4065 - acc: 0.8161 - val_loss: 0.6535 - val_acc: 0.7521\n",
      "Epoch 387/1000\n",
      " - 4s - loss: 0.3996 - acc: 0.8161 - val_loss: 0.4190 - val_acc: 0.8430\n",
      "Epoch 388/1000\n",
      " - 4s - loss: 0.3804 - acc: 0.8182 - val_loss: 2.7535 - val_acc: 0.5041\n",
      "Epoch 389/1000\n",
      " - 4s - loss: 0.3842 - acc: 0.8161 - val_loss: 0.4430 - val_acc: 0.8099\n",
      "Epoch 390/1000\n",
      " - 4s - loss: 0.3563 - acc: 0.8347 - val_loss: 0.4951 - val_acc: 0.8264\n",
      "Epoch 391/1000\n",
      " - 4s - loss: 0.3757 - acc: 0.8285 - val_loss: 0.4382 - val_acc: 0.8264\n",
      "Epoch 392/1000\n",
      " - 4s - loss: 0.3896 - acc: 0.8244 - val_loss: 1.0128 - val_acc: 0.6529\n",
      "Epoch 393/1000\n",
      " - 4s - loss: 0.3458 - acc: 0.8409 - val_loss: 0.9471 - val_acc: 0.6198\n",
      "Epoch 394/1000\n",
      " - 4s - loss: 0.3799 - acc: 0.8223 - val_loss: 0.8388 - val_acc: 0.6860\n",
      "Epoch 395/1000\n",
      " - 4s - loss: 0.3804 - acc: 0.8264 - val_loss: 0.4928 - val_acc: 0.7769\n",
      "Epoch 396/1000\n",
      " - 4s - loss: 0.3670 - acc: 0.8326 - val_loss: 0.7157 - val_acc: 0.6612\n",
      "Epoch 397/1000\n",
      " - 4s - loss: 0.3713 - acc: 0.8285 - val_loss: 0.4701 - val_acc: 0.8264\n",
      "Epoch 398/1000\n",
      " - 4s - loss: 0.3760 - acc: 0.8140 - val_loss: 2.3954 - val_acc: 0.5041\n",
      "Epoch 399/1000\n",
      " - 4s - loss: 0.3543 - acc: 0.8533 - val_loss: 1.2212 - val_acc: 0.6116\n",
      "Epoch 400/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.3546 - acc: 0.8388 - val_loss: 0.4341 - val_acc: 0.8264\n",
      "Epoch 401/1000\n",
      " - 4s - loss: 0.3595 - acc: 0.8347 - val_loss: 0.4407 - val_acc: 0.8182\n",
      "Epoch 402/1000\n",
      " - 4s - loss: 0.3684 - acc: 0.8347 - val_loss: 0.4420 - val_acc: 0.8182\n",
      "Epoch 403/1000\n",
      " - 4s - loss: 0.3577 - acc: 0.8285 - val_loss: 0.4178 - val_acc: 0.8430\n",
      "Epoch 404/1000\n",
      " - 4s - loss: 0.3570 - acc: 0.8533 - val_loss: 0.7341 - val_acc: 0.7273\n",
      "Epoch 405/1000\n",
      " - 4s - loss: 0.3733 - acc: 0.8285 - val_loss: 0.9121 - val_acc: 0.6116\n",
      "Epoch 406/1000\n",
      " - 4s - loss: 0.3464 - acc: 0.8636 - val_loss: 2.5667 - val_acc: 0.5041\n",
      "Epoch 407/1000\n",
      " - 4s - loss: 0.3679 - acc: 0.8388 - val_loss: 0.8980 - val_acc: 0.6777\n",
      "Epoch 408/1000\n",
      " - 4s - loss: 0.3739 - acc: 0.8037 - val_loss: 0.7829 - val_acc: 0.6777\n",
      "Epoch 409/1000\n",
      " - 4s - loss: 0.4039 - acc: 0.7996 - val_loss: 0.4237 - val_acc: 0.8099\n",
      "Epoch 410/1000\n",
      " - 4s - loss: 0.3239 - acc: 0.8740 - val_loss: 0.5809 - val_acc: 0.7521\n",
      "Epoch 411/1000\n",
      " - 4s - loss: 0.3781 - acc: 0.8471 - val_loss: 0.5720 - val_acc: 0.7603\n",
      "Epoch 412/1000\n",
      " - 4s - loss: 0.3767 - acc: 0.8285 - val_loss: 0.4616 - val_acc: 0.8264\n",
      "Epoch 413/1000\n",
      " - 4s - loss: 0.3319 - acc: 0.8595 - val_loss: 0.6818 - val_acc: 0.7025\n",
      "Epoch 414/1000\n",
      " - 4s - loss: 0.3474 - acc: 0.8409 - val_loss: 0.4955 - val_acc: 0.7934\n",
      "Epoch 415/1000\n",
      " - 4s - loss: 0.3936 - acc: 0.8140 - val_loss: 0.5177 - val_acc: 0.7934\n",
      "Epoch 416/1000\n",
      " - 4s - loss: 0.3760 - acc: 0.8368 - val_loss: 0.5561 - val_acc: 0.7686\n",
      "Epoch 417/1000\n",
      " - 4s - loss: 0.3354 - acc: 0.8512 - val_loss: 0.4436 - val_acc: 0.8017\n",
      "Epoch 418/1000\n",
      " - 4s - loss: 0.3361 - acc: 0.8388 - val_loss: 1.2591 - val_acc: 0.5785\n",
      "Epoch 419/1000\n",
      " - 4s - loss: 0.3604 - acc: 0.8409 - val_loss: 0.3917 - val_acc: 0.8430\n",
      "Epoch 420/1000\n",
      " - 4s - loss: 0.3456 - acc: 0.8512 - val_loss: 0.4211 - val_acc: 0.8430\n",
      "Epoch 421/1000\n",
      " - 4s - loss: 0.3750 - acc: 0.8140 - val_loss: 1.1178 - val_acc: 0.5868\n",
      "Epoch 422/1000\n",
      " - 4s - loss: 0.3767 - acc: 0.8430 - val_loss: 0.7031 - val_acc: 0.7438\n",
      "Epoch 423/1000\n",
      " - 4s - loss: 0.3645 - acc: 0.8244 - val_loss: 0.4551 - val_acc: 0.8017\n",
      "Epoch 424/1000\n",
      " - 4s - loss: 0.3289 - acc: 0.8533 - val_loss: 0.7231 - val_acc: 0.7273\n",
      "Epoch 425/1000\n",
      " - 4s - loss: 0.3492 - acc: 0.8574 - val_loss: 0.4093 - val_acc: 0.8430\n",
      "Epoch 426/1000\n",
      " - 4s - loss: 0.3324 - acc: 0.8678 - val_loss: 0.6024 - val_acc: 0.7521\n",
      "Epoch 427/1000\n",
      " - 4s - loss: 0.3725 - acc: 0.8388 - val_loss: 0.4169 - val_acc: 0.8264\n",
      "Epoch 428/1000\n",
      " - 4s - loss: 0.3579 - acc: 0.8347 - val_loss: 0.4001 - val_acc: 0.8430\n",
      "Epoch 429/1000\n",
      " - 4s - loss: 0.3471 - acc: 0.8533 - val_loss: 0.7825 - val_acc: 0.6777\n",
      "Epoch 430/1000\n",
      " - 4s - loss: 0.3442 - acc: 0.8450 - val_loss: 0.6315 - val_acc: 0.7273\n",
      "Epoch 431/1000\n",
      " - 4s - loss: 0.3552 - acc: 0.8450 - val_loss: 1.9026 - val_acc: 0.5455\n",
      "Epoch 432/1000\n",
      " - 4s - loss: 0.3558 - acc: 0.8471 - val_loss: 0.4343 - val_acc: 0.7934\n",
      "Epoch 433/1000\n",
      " - 4s - loss: 0.3432 - acc: 0.8512 - val_loss: 0.4131 - val_acc: 0.8595\n",
      "Epoch 434/1000\n",
      " - 4s - loss: 0.3377 - acc: 0.8306 - val_loss: 0.9955 - val_acc: 0.6529\n",
      "Epoch 435/1000\n",
      " - 4s - loss: 0.3520 - acc: 0.8574 - val_loss: 0.4096 - val_acc: 0.8264\n",
      "Epoch 436/1000\n",
      " - 4s - loss: 0.3471 - acc: 0.8223 - val_loss: 0.5252 - val_acc: 0.7851\n",
      "Epoch 437/1000\n",
      " - 4s - loss: 0.3351 - acc: 0.8388 - val_loss: 0.4325 - val_acc: 0.8182\n",
      "Epoch 438/1000\n",
      " - 4s - loss: 0.3677 - acc: 0.8471 - val_loss: 0.3838 - val_acc: 0.8595\n",
      "Epoch 439/1000\n",
      " - 4s - loss: 0.3683 - acc: 0.8430 - val_loss: 0.8414 - val_acc: 0.7107\n",
      "Epoch 440/1000\n",
      " - 4s - loss: 0.3862 - acc: 0.8161 - val_loss: 1.3301 - val_acc: 0.5620\n",
      "Epoch 441/1000\n",
      " - 4s - loss: 0.3480 - acc: 0.8368 - val_loss: 0.3821 - val_acc: 0.8595\n",
      "Epoch 442/1000\n",
      " - 4s - loss: 0.3322 - acc: 0.8595 - val_loss: 1.0677 - val_acc: 0.6446\n",
      "Epoch 443/1000\n",
      " - 4s - loss: 0.3392 - acc: 0.8512 - val_loss: 0.5873 - val_acc: 0.7521\n",
      "Epoch 444/1000\n",
      " - 4s - loss: 0.3755 - acc: 0.8099 - val_loss: 0.4953 - val_acc: 0.8017\n",
      "Epoch 445/1000\n",
      " - 4s - loss: 0.3586 - acc: 0.8409 - val_loss: 1.6077 - val_acc: 0.5289\n",
      "Epoch 446/1000\n",
      " - 4s - loss: 0.3236 - acc: 0.8492 - val_loss: 2.6260 - val_acc: 0.5041\n",
      "Epoch 447/1000\n",
      " - 4s - loss: 0.3541 - acc: 0.8326 - val_loss: 0.7902 - val_acc: 0.6694\n",
      "Epoch 448/1000\n",
      " - 4s - loss: 0.3352 - acc: 0.8492 - val_loss: 1.2811 - val_acc: 0.6198\n",
      "Epoch 449/1000\n",
      " - 4s - loss: 0.3441 - acc: 0.8595 - val_loss: 0.4762 - val_acc: 0.7934\n",
      "Epoch 450/1000\n",
      " - 4s - loss: 0.3164 - acc: 0.8678 - val_loss: 0.6626 - val_acc: 0.7355\n",
      "Epoch 451/1000\n",
      " - 4s - loss: 0.3213 - acc: 0.8492 - val_loss: 0.4460 - val_acc: 0.8182\n",
      "Epoch 452/1000\n",
      " - 4s - loss: 0.3550 - acc: 0.8368 - val_loss: 0.5197 - val_acc: 0.7521\n",
      "Epoch 453/1000\n",
      " - 4s - loss: 0.3610 - acc: 0.8471 - val_loss: 1.1884 - val_acc: 0.5785\n",
      "Epoch 454/1000\n",
      " - 4s - loss: 0.3130 - acc: 0.8781 - val_loss: 0.4720 - val_acc: 0.8182\n",
      "Epoch 455/1000\n",
      " - 4s - loss: 0.3405 - acc: 0.8471 - val_loss: 0.4178 - val_acc: 0.8347\n",
      "Epoch 456/1000\n",
      " - 4s - loss: 0.3313 - acc: 0.8595 - val_loss: 1.6985 - val_acc: 0.5372\n",
      "Epoch 457/1000\n",
      " - 4s - loss: 0.3438 - acc: 0.8492 - val_loss: 1.2756 - val_acc: 0.6281\n",
      "Epoch 458/1000\n",
      " - 4s - loss: 0.3270 - acc: 0.8471 - val_loss: 1.5389 - val_acc: 0.5537\n",
      "Epoch 459/1000\n",
      " - 4s - loss: 0.3413 - acc: 0.8368 - val_loss: 0.4348 - val_acc: 0.8182\n",
      "Epoch 460/1000\n",
      " - 4s - loss: 0.3374 - acc: 0.8430 - val_loss: 0.7316 - val_acc: 0.7025\n",
      "Epoch 461/1000\n",
      " - 4s - loss: 0.3662 - acc: 0.8326 - val_loss: 0.4434 - val_acc: 0.8430\n",
      "Epoch 462/1000\n",
      " - 4s - loss: 0.3433 - acc: 0.8492 - val_loss: 0.4424 - val_acc: 0.8264\n",
      "Epoch 463/1000\n",
      " - 4s - loss: 0.3282 - acc: 0.8533 - val_loss: 1.2634 - val_acc: 0.6281\n",
      "Epoch 464/1000\n",
      " - 4s - loss: 0.3455 - acc: 0.8326 - val_loss: 0.4334 - val_acc: 0.8430\n",
      "Epoch 465/1000\n",
      " - 4s - loss: 0.3380 - acc: 0.8554 - val_loss: 0.7130 - val_acc: 0.6942\n",
      "Epoch 466/1000\n",
      " - 4s - loss: 0.3545 - acc: 0.8471 - val_loss: 0.4916 - val_acc: 0.8182\n",
      "Epoch 467/1000\n",
      " - 4s - loss: 0.3341 - acc: 0.8450 - val_loss: 0.4167 - val_acc: 0.8347\n",
      "Epoch 468/1000\n",
      " - 4s - loss: 0.3462 - acc: 0.8306 - val_loss: 0.4449 - val_acc: 0.8264\n",
      "Epoch 469/1000\n",
      " - 4s - loss: 0.3396 - acc: 0.8512 - val_loss: 0.5364 - val_acc: 0.8017\n",
      "Epoch 470/1000\n",
      " - 4s - loss: 0.3402 - acc: 0.8533 - val_loss: 0.9433 - val_acc: 0.6281\n",
      "Epoch 471/1000\n",
      " - 4s - loss: 0.3101 - acc: 0.8657 - val_loss: 0.4149 - val_acc: 0.8512\n",
      "Epoch 472/1000\n",
      " - 4s - loss: 0.3024 - acc: 0.8574 - val_loss: 0.4158 - val_acc: 0.8347\n",
      "Epoch 473/1000\n",
      " - 4s - loss: 0.3113 - acc: 0.8719 - val_loss: 0.4449 - val_acc: 0.8347\n",
      "Epoch 474/1000\n",
      " - 4s - loss: 0.3235 - acc: 0.8492 - val_loss: 0.5474 - val_acc: 0.7686\n",
      "Epoch 475/1000\n",
      " - 4s - loss: 0.3863 - acc: 0.8202 - val_loss: 0.5361 - val_acc: 0.7769\n",
      "Epoch 476/1000\n",
      " - 4s - loss: 0.3444 - acc: 0.8554 - val_loss: 0.4659 - val_acc: 0.8017\n",
      "Epoch 477/1000\n",
      " - 4s - loss: 0.3431 - acc: 0.8388 - val_loss: 0.5992 - val_acc: 0.7521\n",
      "Epoch 478/1000\n",
      " - 4s - loss: 0.3175 - acc: 0.8636 - val_loss: 0.9466 - val_acc: 0.6612\n",
      "Epoch 479/1000\n",
      " - 4s - loss: 0.3361 - acc: 0.8574 - val_loss: 1.6498 - val_acc: 0.5455\n",
      "Epoch 480/1000\n",
      " - 4s - loss: 0.3344 - acc: 0.8657 - val_loss: 0.6705 - val_acc: 0.7521\n",
      "Epoch 481/1000\n",
      " - 4s - loss: 0.3204 - acc: 0.8595 - val_loss: 0.6483 - val_acc: 0.7355\n",
      "Epoch 482/1000\n",
      " - 4s - loss: 0.3545 - acc: 0.8368 - val_loss: 0.8518 - val_acc: 0.6529\n",
      "Epoch 483/1000\n",
      " - 4s - loss: 0.3453 - acc: 0.8595 - val_loss: 2.3023 - val_acc: 0.5207\n",
      "Epoch 484/1000\n",
      " - 4s - loss: 0.3276 - acc: 0.8657 - val_loss: 0.7755 - val_acc: 0.7025\n",
      "Epoch 485/1000\n",
      " - 4s - loss: 0.3002 - acc: 0.8781 - val_loss: 0.5250 - val_acc: 0.8264\n",
      "Epoch 486/1000\n",
      " - 4s - loss: 0.3172 - acc: 0.8595 - val_loss: 1.8734 - val_acc: 0.5207\n",
      "Epoch 487/1000\n",
      " - 4s - loss: 0.3350 - acc: 0.8471 - val_loss: 0.5314 - val_acc: 0.7851\n",
      "Epoch 488/1000\n",
      " - 4s - loss: 0.2872 - acc: 0.8678 - val_loss: 0.4128 - val_acc: 0.8512\n",
      "Epoch 489/1000\n",
      " - 4s - loss: 0.3313 - acc: 0.8471 - val_loss: 0.4525 - val_acc: 0.8430\n",
      "Epoch 490/1000\n",
      " - 4s - loss: 0.3432 - acc: 0.8512 - val_loss: 1.2691 - val_acc: 0.5702\n",
      "Epoch 491/1000\n",
      " - 4s - loss: 0.3430 - acc: 0.8326 - val_loss: 0.6777 - val_acc: 0.7521\n",
      "Epoch 492/1000\n",
      " - 4s - loss: 0.3240 - acc: 0.8450 - val_loss: 0.5598 - val_acc: 0.7603\n",
      "Epoch 493/1000\n",
      " - 4s - loss: 0.3182 - acc: 0.8512 - val_loss: 0.5222 - val_acc: 0.7934\n",
      "Epoch 494/1000\n",
      " - 4s - loss: 0.3449 - acc: 0.8409 - val_loss: 0.7681 - val_acc: 0.6942\n",
      "Epoch 495/1000\n",
      " - 4s - loss: 0.3054 - acc: 0.8678 - val_loss: 0.4751 - val_acc: 0.7851\n",
      "Epoch 496/1000\n",
      " - 4s - loss: 0.3404 - acc: 0.8492 - val_loss: 0.7133 - val_acc: 0.7190\n",
      "Epoch 497/1000\n",
      " - 4s - loss: 0.3285 - acc: 0.8616 - val_loss: 0.4334 - val_acc: 0.8182\n",
      "Epoch 498/1000\n",
      " - 4s - loss: 0.3055 - acc: 0.8740 - val_loss: 0.3794 - val_acc: 0.8512\n",
      "Epoch 499/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.3349 - acc: 0.8368 - val_loss: 0.4841 - val_acc: 0.7769\n",
      "Epoch 500/1000\n",
      " - 4s - loss: 0.2999 - acc: 0.8533 - val_loss: 0.5445 - val_acc: 0.7851\n",
      "Epoch 501/1000\n",
      " - 4s - loss: 0.3578 - acc: 0.8450 - val_loss: 0.9559 - val_acc: 0.6529\n",
      "Epoch 502/1000\n",
      " - 4s - loss: 0.3161 - acc: 0.8616 - val_loss: 0.5772 - val_acc: 0.7769\n",
      "Epoch 503/1000\n",
      " - 4s - loss: 0.3199 - acc: 0.8616 - val_loss: 0.9624 - val_acc: 0.6198\n",
      "Epoch 504/1000\n",
      " - 4s - loss: 0.3022 - acc: 0.8740 - val_loss: 0.6679 - val_acc: 0.7273\n",
      "Epoch 505/1000\n",
      " - 4s - loss: 0.3207 - acc: 0.8554 - val_loss: 0.4757 - val_acc: 0.8347\n",
      "Epoch 506/1000\n",
      " - 4s - loss: 0.3106 - acc: 0.8657 - val_loss: 0.4125 - val_acc: 0.8347\n",
      "Epoch 507/1000\n",
      " - 4s - loss: 0.3457 - acc: 0.8492 - val_loss: 0.4414 - val_acc: 0.8182\n",
      "Epoch 508/1000\n",
      " - 4s - loss: 0.3327 - acc: 0.8430 - val_loss: 0.3962 - val_acc: 0.8430\n",
      "Epoch 509/1000\n",
      " - 4s - loss: 0.3059 - acc: 0.8595 - val_loss: 1.1752 - val_acc: 0.5785\n",
      "Epoch 510/1000\n",
      " - 4s - loss: 0.3140 - acc: 0.8678 - val_loss: 0.4803 - val_acc: 0.8347\n",
      "Epoch 511/1000\n",
      " - 4s - loss: 0.3056 - acc: 0.8740 - val_loss: 0.4524 - val_acc: 0.8347\n",
      "Epoch 512/1000\n",
      " - 4s - loss: 0.3056 - acc: 0.8657 - val_loss: 0.4418 - val_acc: 0.8182\n",
      "Epoch 513/1000\n",
      " - 4s - loss: 0.3363 - acc: 0.8471 - val_loss: 1.1700 - val_acc: 0.5868\n",
      "Epoch 514/1000\n",
      " - 4s - loss: 0.2976 - acc: 0.8698 - val_loss: 0.4123 - val_acc: 0.8264\n",
      "Epoch 515/1000\n",
      " - 4s - loss: 0.2937 - acc: 0.8616 - val_loss: 0.4737 - val_acc: 0.8182\n",
      "Epoch 516/1000\n",
      " - 4s - loss: 0.2790 - acc: 0.8802 - val_loss: 0.5775 - val_acc: 0.7851\n",
      "Epoch 517/1000\n",
      " - 4s - loss: 0.3321 - acc: 0.8471 - val_loss: 0.4641 - val_acc: 0.8264\n",
      "Epoch 518/1000\n",
      " - 4s - loss: 0.3266 - acc: 0.8471 - val_loss: 0.5899 - val_acc: 0.7769\n",
      "Epoch 519/1000\n",
      " - 4s - loss: 0.2982 - acc: 0.8740 - val_loss: 0.4961 - val_acc: 0.8017\n",
      "Epoch 520/1000\n",
      " - 4s - loss: 0.3061 - acc: 0.8781 - val_loss: 0.8723 - val_acc: 0.7025\n",
      "Epoch 521/1000\n",
      " - 4s - loss: 0.3379 - acc: 0.8430 - val_loss: 0.9205 - val_acc: 0.6364\n",
      "Epoch 522/1000\n",
      " - 4s - loss: 0.3523 - acc: 0.8368 - val_loss: 0.4560 - val_acc: 0.8264\n",
      "Epoch 523/1000\n",
      " - 4s - loss: 0.2884 - acc: 0.8781 - val_loss: 0.4638 - val_acc: 0.8347\n",
      "Epoch 524/1000\n",
      " - 4s - loss: 0.3002 - acc: 0.8636 - val_loss: 0.4133 - val_acc: 0.8512\n",
      "Epoch 525/1000\n",
      " - 4s - loss: 0.3121 - acc: 0.8678 - val_loss: 0.4183 - val_acc: 0.8017\n",
      "Epoch 526/1000\n",
      " - 4s - loss: 0.3159 - acc: 0.8760 - val_loss: 0.4214 - val_acc: 0.8264\n",
      "Epoch 527/1000\n",
      " - 4s - loss: 0.2955 - acc: 0.8678 - val_loss: 2.5907 - val_acc: 0.5124\n",
      "Epoch 528/1000\n",
      " - 4s - loss: 0.3620 - acc: 0.8430 - val_loss: 1.2331 - val_acc: 0.6529\n",
      "Epoch 529/1000\n",
      " - 4s - loss: 0.3158 - acc: 0.8636 - val_loss: 0.4352 - val_acc: 0.8099\n",
      "Epoch 530/1000\n",
      " - 4s - loss: 0.3018 - acc: 0.8657 - val_loss: 0.3937 - val_acc: 0.8678\n",
      "Epoch 531/1000\n",
      " - 4s - loss: 0.3484 - acc: 0.8430 - val_loss: 0.5514 - val_acc: 0.7851\n",
      "Epoch 532/1000\n",
      " - 4s - loss: 0.3120 - acc: 0.8533 - val_loss: 0.3776 - val_acc: 0.8595\n",
      "Epoch 533/1000\n",
      " - 4s - loss: 0.2745 - acc: 0.8781 - val_loss: 0.7164 - val_acc: 0.7025\n",
      "Epoch 534/1000\n",
      " - 4s - loss: 0.3375 - acc: 0.8347 - val_loss: 0.8091 - val_acc: 0.6860\n",
      "Epoch 535/1000\n",
      " - 4s - loss: 0.3131 - acc: 0.8574 - val_loss: 0.6019 - val_acc: 0.7521\n",
      "Epoch 536/1000\n",
      " - 4s - loss: 0.3131 - acc: 0.8616 - val_loss: 0.7632 - val_acc: 0.7273\n",
      "Epoch 537/1000\n",
      " - 4s - loss: 0.3216 - acc: 0.8595 - val_loss: 0.8964 - val_acc: 0.6446\n",
      "Epoch 538/1000\n",
      " - 4s - loss: 0.3277 - acc: 0.8492 - val_loss: 1.3147 - val_acc: 0.6198\n",
      "Epoch 539/1000\n",
      " - 4s - loss: 0.3159 - acc: 0.8450 - val_loss: 0.3773 - val_acc: 0.8595\n",
      "Epoch 540/1000\n",
      " - 4s - loss: 0.3447 - acc: 0.8554 - val_loss: 0.3770 - val_acc: 0.8678\n",
      "Epoch 541/1000\n",
      " - 4s - loss: 0.3020 - acc: 0.8822 - val_loss: 0.4895 - val_acc: 0.8347\n",
      "Epoch 542/1000\n",
      " - 4s - loss: 0.2797 - acc: 0.8843 - val_loss: 2.9068 - val_acc: 0.5041\n",
      "Epoch 543/1000\n",
      " - 4s - loss: 0.3054 - acc: 0.8740 - val_loss: 0.7772 - val_acc: 0.7273\n",
      "Epoch 544/1000\n",
      " - 4s - loss: 0.3004 - acc: 0.8678 - val_loss: 0.5493 - val_acc: 0.7603\n",
      "Epoch 545/1000\n",
      " - 4s - loss: 0.3048 - acc: 0.8616 - val_loss: 0.5490 - val_acc: 0.7769\n",
      "Epoch 546/1000\n",
      " - 4s - loss: 0.2941 - acc: 0.8843 - val_loss: 0.4014 - val_acc: 0.8595\n",
      "Epoch 547/1000\n",
      " - 4s - loss: 0.3244 - acc: 0.8492 - val_loss: 1.6009 - val_acc: 0.5620\n",
      "Epoch 548/1000\n",
      " - 4s - loss: 0.3163 - acc: 0.8533 - val_loss: 0.5469 - val_acc: 0.7934\n",
      "Epoch 549/1000\n",
      " - 4s - loss: 0.3009 - acc: 0.8616 - val_loss: 0.7901 - val_acc: 0.7025\n",
      "Epoch 550/1000\n",
      " - 4s - loss: 0.3398 - acc: 0.8450 - val_loss: 0.8741 - val_acc: 0.7107\n",
      "Epoch 551/1000\n",
      " - 4s - loss: 0.2702 - acc: 0.8802 - val_loss: 1.7665 - val_acc: 0.5702\n",
      "Epoch 552/1000\n",
      " - 4s - loss: 0.2682 - acc: 0.8905 - val_loss: 2.4264 - val_acc: 0.5124\n",
      "Epoch 553/1000\n",
      " - 4s - loss: 0.2779 - acc: 0.8905 - val_loss: 0.4253 - val_acc: 0.8347\n",
      "Epoch 554/1000\n",
      " - 4s - loss: 0.3235 - acc: 0.8388 - val_loss: 0.5999 - val_acc: 0.7355\n",
      "Epoch 555/1000\n",
      " - 4s - loss: 0.2899 - acc: 0.8822 - val_loss: 0.9173 - val_acc: 0.6612\n",
      "Epoch 556/1000\n",
      " - 4s - loss: 0.3056 - acc: 0.8657 - val_loss: 1.3613 - val_acc: 0.5785\n",
      "Epoch 557/1000\n",
      " - 4s - loss: 0.3237 - acc: 0.8533 - val_loss: 0.4170 - val_acc: 0.8430\n",
      "Epoch 558/1000\n",
      " - 4s - loss: 0.2903 - acc: 0.8802 - val_loss: 0.6324 - val_acc: 0.7190\n",
      "Epoch 559/1000\n",
      " - 4s - loss: 0.3060 - acc: 0.8657 - val_loss: 0.4528 - val_acc: 0.8347\n",
      "Epoch 560/1000\n",
      " - 4s - loss: 0.2820 - acc: 0.8698 - val_loss: 1.7785 - val_acc: 0.5289\n",
      "Epoch 561/1000\n",
      " - 4s - loss: 0.3077 - acc: 0.8678 - val_loss: 2.1760 - val_acc: 0.5289\n",
      "Epoch 562/1000\n",
      " - 4s - loss: 0.2786 - acc: 0.8822 - val_loss: 0.7774 - val_acc: 0.7190\n",
      "Epoch 563/1000\n",
      " - 4s - loss: 0.2899 - acc: 0.8760 - val_loss: 1.8823 - val_acc: 0.5455\n",
      "Epoch 564/1000\n",
      " - 4s - loss: 0.3208 - acc: 0.8760 - val_loss: 1.0796 - val_acc: 0.6777\n",
      "Epoch 565/1000\n",
      " - 4s - loss: 0.2855 - acc: 0.8926 - val_loss: 0.6759 - val_acc: 0.7190\n",
      "Epoch 566/1000\n",
      " - 4s - loss: 0.3221 - acc: 0.8430 - val_loss: 0.5480 - val_acc: 0.7851\n",
      "Epoch 567/1000\n",
      " - 4s - loss: 0.2921 - acc: 0.8698 - val_loss: 0.6226 - val_acc: 0.7190\n",
      "Epoch 568/1000\n",
      " - 4s - loss: 0.2788 - acc: 0.8760 - val_loss: 0.4282 - val_acc: 0.8512\n",
      "Epoch 569/1000\n",
      " - 4s - loss: 0.3013 - acc: 0.8822 - val_loss: 0.4479 - val_acc: 0.8347\n",
      "Epoch 570/1000\n",
      " - 4s - loss: 0.2763 - acc: 0.8946 - val_loss: 0.3773 - val_acc: 0.8595\n",
      "Epoch 571/1000\n",
      " - 4s - loss: 0.2944 - acc: 0.8719 - val_loss: 1.1628 - val_acc: 0.6033\n",
      "Epoch 572/1000\n",
      " - 4s - loss: 0.3027 - acc: 0.8595 - val_loss: 0.6829 - val_acc: 0.7273\n",
      "Epoch 573/1000\n",
      " - 4s - loss: 0.2941 - acc: 0.8698 - val_loss: 0.4058 - val_acc: 0.8264\n",
      "Epoch 574/1000\n",
      " - 4s - loss: 0.2807 - acc: 0.8884 - val_loss: 0.4397 - val_acc: 0.8182\n",
      "Epoch 575/1000\n",
      " - 4s - loss: 0.3245 - acc: 0.8719 - val_loss: 0.6303 - val_acc: 0.7190\n",
      "Epoch 576/1000\n",
      " - 4s - loss: 0.3048 - acc: 0.8574 - val_loss: 0.4079 - val_acc: 0.8678\n",
      "Epoch 577/1000\n",
      " - 4s - loss: 0.2899 - acc: 0.8595 - val_loss: 0.4120 - val_acc: 0.8430\n",
      "Epoch 578/1000\n",
      " - 4s - loss: 0.2833 - acc: 0.8822 - val_loss: 0.7084 - val_acc: 0.7521\n",
      "Epoch 579/1000\n",
      " - 4s - loss: 0.2738 - acc: 0.8719 - val_loss: 0.5436 - val_acc: 0.8182\n",
      "Epoch 580/1000\n",
      " - 4s - loss: 0.2843 - acc: 0.8781 - val_loss: 0.4765 - val_acc: 0.8264\n",
      "Epoch 581/1000\n",
      " - 4s - loss: 0.3058 - acc: 0.8595 - val_loss: 0.3793 - val_acc: 0.8595\n",
      "Epoch 582/1000\n",
      " - 4s - loss: 0.3070 - acc: 0.8926 - val_loss: 0.7044 - val_acc: 0.7438\n",
      "Epoch 583/1000\n",
      " - 4s - loss: 0.2924 - acc: 0.8616 - val_loss: 0.5816 - val_acc: 0.7769\n",
      "Epoch 584/1000\n",
      " - 4s - loss: 0.3270 - acc: 0.8430 - val_loss: 0.3741 - val_acc: 0.8678\n",
      "Epoch 585/1000\n",
      " - 4s - loss: 0.3128 - acc: 0.8657 - val_loss: 0.4732 - val_acc: 0.8182\n",
      "Epoch 586/1000\n",
      " - 4s - loss: 0.3053 - acc: 0.8657 - val_loss: 0.7409 - val_acc: 0.7190\n",
      "Epoch 587/1000\n",
      " - 4s - loss: 0.2929 - acc: 0.8554 - val_loss: 0.4896 - val_acc: 0.7851\n",
      "Epoch 588/1000\n",
      " - 4s - loss: 0.2781 - acc: 0.8802 - val_loss: 1.6524 - val_acc: 0.5950\n",
      "Epoch 589/1000\n",
      " - 4s - loss: 0.2773 - acc: 0.8760 - val_loss: 0.3979 - val_acc: 0.8678\n",
      "Epoch 590/1000\n",
      " - 4s - loss: 0.2877 - acc: 0.8760 - val_loss: 2.8241 - val_acc: 0.5124\n",
      "Epoch 591/1000\n",
      " - 4s - loss: 0.2630 - acc: 0.8822 - val_loss: 0.6453 - val_acc: 0.7521\n",
      "Epoch 592/1000\n",
      " - 4s - loss: 0.2644 - acc: 0.8988 - val_loss: 0.5674 - val_acc: 0.7603\n",
      "Epoch 593/1000\n",
      " - 4s - loss: 0.3044 - acc: 0.8740 - val_loss: 0.3958 - val_acc: 0.8595\n",
      "Epoch 594/1000\n",
      " - 4s - loss: 0.2732 - acc: 0.8905 - val_loss: 0.5343 - val_acc: 0.8017\n",
      "Epoch 595/1000\n",
      " - 4s - loss: 0.2589 - acc: 0.8988 - val_loss: 1.0583 - val_acc: 0.6281\n",
      "Epoch 596/1000\n",
      " - 4s - loss: 0.2825 - acc: 0.8698 - val_loss: 0.6087 - val_acc: 0.7438\n",
      "Epoch 597/1000\n",
      " - 4s - loss: 0.2732 - acc: 0.8843 - val_loss: 0.3906 - val_acc: 0.8595\n",
      "Epoch 598/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.2824 - acc: 0.8740 - val_loss: 1.4071 - val_acc: 0.6364\n",
      "Epoch 599/1000\n",
      " - 4s - loss: 0.2827 - acc: 0.8616 - val_loss: 1.0565 - val_acc: 0.6860\n",
      "Epoch 600/1000\n",
      " - 4s - loss: 0.3025 - acc: 0.8657 - val_loss: 0.4767 - val_acc: 0.8017\n",
      "Epoch 601/1000\n",
      " - 4s - loss: 0.2957 - acc: 0.8698 - val_loss: 1.0742 - val_acc: 0.6446\n",
      "Epoch 602/1000\n",
      " - 4s - loss: 0.2881 - acc: 0.8760 - val_loss: 0.4645 - val_acc: 0.8099\n",
      "Epoch 603/1000\n",
      " - 4s - loss: 0.2935 - acc: 0.8864 - val_loss: 0.3792 - val_acc: 0.8678\n",
      "Epoch 604/1000\n",
      " - 4s - loss: 0.3078 - acc: 0.8657 - val_loss: 0.4713 - val_acc: 0.8264\n",
      "Epoch 605/1000\n",
      " - 4s - loss: 0.2898 - acc: 0.8905 - val_loss: 0.5975 - val_acc: 0.7769\n",
      "Epoch 606/1000\n",
      " - 4s - loss: 0.2884 - acc: 0.8760 - val_loss: 0.4171 - val_acc: 0.8264\n",
      "Epoch 607/1000\n",
      " - 4s - loss: 0.2790 - acc: 0.8802 - val_loss: 0.6060 - val_acc: 0.7934\n",
      "Epoch 608/1000\n",
      " - 4s - loss: 0.2768 - acc: 0.8946 - val_loss: 0.4198 - val_acc: 0.8182\n",
      "Epoch 609/1000\n",
      " - 4s - loss: 0.2752 - acc: 0.8884 - val_loss: 0.3537 - val_acc: 0.8926\n",
      "Epoch 610/1000\n",
      " - 4s - loss: 0.2921 - acc: 0.8843 - val_loss: 1.5770 - val_acc: 0.5620\n",
      "Epoch 611/1000\n",
      " - 4s - loss: 0.3091 - acc: 0.8595 - val_loss: 1.7271 - val_acc: 0.5702\n",
      "Epoch 612/1000\n",
      " - 4s - loss: 0.2871 - acc: 0.8781 - val_loss: 0.4698 - val_acc: 0.8182\n",
      "Epoch 613/1000\n",
      " - 4s - loss: 0.2759 - acc: 0.8760 - val_loss: 1.8679 - val_acc: 0.5372\n",
      "Epoch 614/1000\n",
      " - 4s - loss: 0.2556 - acc: 0.8884 - val_loss: 0.3900 - val_acc: 0.8430\n",
      "Epoch 615/1000\n",
      " - 4s - loss: 0.2593 - acc: 0.8905 - val_loss: 0.5366 - val_acc: 0.8017\n",
      "Epoch 616/1000\n",
      " - 4s - loss: 0.2781 - acc: 0.8864 - val_loss: 0.8924 - val_acc: 0.6777\n",
      "Epoch 617/1000\n",
      " - 4s - loss: 0.3146 - acc: 0.8636 - val_loss: 0.4167 - val_acc: 0.8678\n",
      "Epoch 618/1000\n",
      " - 4s - loss: 0.2594 - acc: 0.8698 - val_loss: 0.6663 - val_acc: 0.7355\n",
      "Epoch 619/1000\n",
      " - 4s - loss: 0.2831 - acc: 0.8719 - val_loss: 0.4918 - val_acc: 0.8512\n",
      "Epoch 620/1000\n",
      " - 4s - loss: 0.2991 - acc: 0.8719 - val_loss: 0.3937 - val_acc: 0.8595\n",
      "Epoch 621/1000\n",
      " - 4s - loss: 0.3097 - acc: 0.8533 - val_loss: 0.5133 - val_acc: 0.7851\n",
      "Epoch 622/1000\n",
      " - 4s - loss: 0.2718 - acc: 0.8822 - val_loss: 0.4309 - val_acc: 0.8430\n",
      "Epoch 623/1000\n",
      " - 4s - loss: 0.2642 - acc: 0.8926 - val_loss: 0.4170 - val_acc: 0.8678\n",
      "Epoch 624/1000\n",
      " - 4s - loss: 0.2711 - acc: 0.8802 - val_loss: 0.4178 - val_acc: 0.8678\n",
      "Epoch 625/1000\n",
      " - 4s - loss: 0.2537 - acc: 0.8926 - val_loss: 0.3762 - val_acc: 0.8595\n",
      "Epoch 626/1000\n",
      " - 4s - loss: 0.2980 - acc: 0.8781 - val_loss: 1.0860 - val_acc: 0.6612\n",
      "Epoch 627/1000\n",
      " - 4s - loss: 0.2716 - acc: 0.8884 - val_loss: 1.4275 - val_acc: 0.5620\n",
      "Epoch 628/1000\n",
      " - 4s - loss: 0.3105 - acc: 0.8636 - val_loss: 0.5910 - val_acc: 0.7355\n",
      "Epoch 629/1000\n",
      " - 4s - loss: 0.2960 - acc: 0.8719 - val_loss: 0.8884 - val_acc: 0.6860\n",
      "Epoch 630/1000\n",
      " - 4s - loss: 0.2665 - acc: 0.8967 - val_loss: 0.6320 - val_acc: 0.7521\n",
      "Epoch 631/1000\n",
      " - 4s - loss: 0.2707 - acc: 0.8926 - val_loss: 1.7615 - val_acc: 0.5455\n",
      "Epoch 632/1000\n",
      " - 4s - loss: 0.2763 - acc: 0.8802 - val_loss: 0.5439 - val_acc: 0.7686\n",
      "Epoch 633/1000\n",
      " - 4s - loss: 0.2635 - acc: 0.8926 - val_loss: 0.7237 - val_acc: 0.7273\n",
      "Epoch 634/1000\n",
      " - 4s - loss: 0.2869 - acc: 0.8740 - val_loss: 0.9955 - val_acc: 0.6777\n",
      "Epoch 635/1000\n",
      " - 4s - loss: 0.2704 - acc: 0.8864 - val_loss: 0.7854 - val_acc: 0.7107\n",
      "Epoch 636/1000\n",
      " - 4s - loss: 0.3215 - acc: 0.8554 - val_loss: 1.5894 - val_acc: 0.5785\n",
      "Epoch 637/1000\n",
      " - 4s - loss: 0.2751 - acc: 0.8760 - val_loss: 0.4326 - val_acc: 0.8595\n",
      "Epoch 638/1000\n",
      " - 4s - loss: 0.2581 - acc: 0.8946 - val_loss: 1.0260 - val_acc: 0.6694\n",
      "Epoch 639/1000\n",
      " - 4s - loss: 0.2698 - acc: 0.8781 - val_loss: 1.4999 - val_acc: 0.5868\n",
      "Epoch 640/1000\n",
      " - 4s - loss: 0.2556 - acc: 0.8967 - val_loss: 0.3901 - val_acc: 0.8512\n",
      "Epoch 641/1000\n",
      " - 4s - loss: 0.2551 - acc: 0.8946 - val_loss: 0.4636 - val_acc: 0.8264\n",
      "Epoch 642/1000\n",
      " - 4s - loss: 0.2853 - acc: 0.8636 - val_loss: 0.3696 - val_acc: 0.8678\n",
      "Epoch 643/1000\n",
      " - 4s - loss: 0.2890 - acc: 0.8926 - val_loss: 0.4127 - val_acc: 0.8347\n",
      "Epoch 644/1000\n",
      " - 4s - loss: 0.2645 - acc: 0.8864 - val_loss: 0.6586 - val_acc: 0.7190\n",
      "Epoch 645/1000\n",
      " - 4s - loss: 0.2402 - acc: 0.9091 - val_loss: 0.9415 - val_acc: 0.6860\n",
      "Epoch 646/1000\n",
      " - 4s - loss: 0.2934 - acc: 0.8884 - val_loss: 0.5140 - val_acc: 0.8017\n",
      "Epoch 647/1000\n",
      " - 4s - loss: 0.2883 - acc: 0.8905 - val_loss: 0.3652 - val_acc: 0.8678\n",
      "Epoch 648/1000\n",
      " - 4s - loss: 0.2744 - acc: 0.8781 - val_loss: 0.3793 - val_acc: 0.8595\n",
      "Epoch 649/1000\n",
      " - 4s - loss: 0.2669 - acc: 0.8864 - val_loss: 0.5580 - val_acc: 0.7851\n",
      "Epoch 650/1000\n",
      " - 4s - loss: 0.2707 - acc: 0.8760 - val_loss: 0.3932 - val_acc: 0.8430\n",
      "Epoch 651/1000\n",
      " - 4s - loss: 0.2816 - acc: 0.8740 - val_loss: 0.3767 - val_acc: 0.8595\n",
      "Epoch 652/1000\n",
      " - 4s - loss: 0.2812 - acc: 0.8884 - val_loss: 1.2844 - val_acc: 0.6116\n",
      "Epoch 653/1000\n",
      " - 4s - loss: 0.2602 - acc: 0.8905 - val_loss: 0.6364 - val_acc: 0.7273\n",
      "Epoch 654/1000\n",
      " - 4s - loss: 0.2766 - acc: 0.8905 - val_loss: 0.4235 - val_acc: 0.8595\n",
      "Epoch 655/1000\n",
      " - 4s - loss: 0.2749 - acc: 0.8802 - val_loss: 0.4025 - val_acc: 0.8760\n",
      "Epoch 656/1000\n",
      " - 4s - loss: 0.2552 - acc: 0.8988 - val_loss: 0.6766 - val_acc: 0.7355\n",
      "Epoch 657/1000\n",
      " - 4s - loss: 0.2809 - acc: 0.8781 - val_loss: 0.7974 - val_acc: 0.7438\n",
      "Epoch 658/1000\n",
      " - 4s - loss: 0.2487 - acc: 0.8946 - val_loss: 0.4690 - val_acc: 0.8347\n",
      "Epoch 659/1000\n",
      " - 4s - loss: 0.2367 - acc: 0.8864 - val_loss: 0.5375 - val_acc: 0.8017\n",
      "Epoch 660/1000\n",
      " - 4s - loss: 0.2750 - acc: 0.8905 - val_loss: 0.5233 - val_acc: 0.8182\n",
      "Epoch 661/1000\n",
      " - 4s - loss: 0.2582 - acc: 0.9008 - val_loss: 0.8293 - val_acc: 0.6860\n",
      "Epoch 662/1000\n",
      " - 4s - loss: 0.2576 - acc: 0.8884 - val_loss: 0.3916 - val_acc: 0.8678\n",
      "Epoch 663/1000\n",
      " - 4s - loss: 0.2283 - acc: 0.9132 - val_loss: 0.9158 - val_acc: 0.7190\n",
      "Epoch 664/1000\n",
      " - 4s - loss: 0.2691 - acc: 0.8802 - val_loss: 0.3937 - val_acc: 0.8926\n",
      "Epoch 665/1000\n",
      " - 4s - loss: 0.2144 - acc: 0.9236 - val_loss: 1.2395 - val_acc: 0.6281\n",
      "Epoch 666/1000\n",
      " - 4s - loss: 0.2736 - acc: 0.8822 - val_loss: 0.3824 - val_acc: 0.8430\n",
      "Epoch 667/1000\n",
      " - 4s - loss: 0.2806 - acc: 0.8843 - val_loss: 2.0493 - val_acc: 0.5455\n",
      "Epoch 668/1000\n",
      " - 4s - loss: 0.2896 - acc: 0.8657 - val_loss: 1.0239 - val_acc: 0.6860\n",
      "Epoch 669/1000\n",
      " - 4s - loss: 0.2620 - acc: 0.8864 - val_loss: 0.6585 - val_acc: 0.7355\n",
      "Epoch 670/1000\n",
      " - 4s - loss: 0.2807 - acc: 0.8760 - val_loss: 0.4190 - val_acc: 0.8678\n",
      "Epoch 671/1000\n",
      " - 4s - loss: 0.2969 - acc: 0.8719 - val_loss: 0.8941 - val_acc: 0.7025\n",
      "Epoch 672/1000\n",
      " - 4s - loss: 0.2395 - acc: 0.8864 - val_loss: 0.5401 - val_acc: 0.8017\n",
      "Epoch 673/1000\n",
      " - 4s - loss: 0.2958 - acc: 0.8760 - val_loss: 0.7002 - val_acc: 0.7603\n",
      "Epoch 674/1000\n",
      " - 4s - loss: 0.2469 - acc: 0.8884 - val_loss: 0.4376 - val_acc: 0.8595\n",
      "Epoch 675/1000\n",
      " - 4s - loss: 0.2332 - acc: 0.9029 - val_loss: 0.7206 - val_acc: 0.7355\n",
      "Epoch 676/1000\n",
      " - 4s - loss: 0.2467 - acc: 0.8988 - val_loss: 0.5808 - val_acc: 0.7438\n",
      "Epoch 677/1000\n",
      " - 4s - loss: 0.2967 - acc: 0.8657 - val_loss: 0.7000 - val_acc: 0.7521\n",
      "Epoch 678/1000\n",
      " - 4s - loss: 0.2494 - acc: 0.8926 - val_loss: 0.6943 - val_acc: 0.7273\n",
      "Epoch 679/1000\n",
      " - 4s - loss: 0.2579 - acc: 0.9029 - val_loss: 0.5002 - val_acc: 0.8430\n",
      "Epoch 680/1000\n",
      " - 4s - loss: 0.2279 - acc: 0.9153 - val_loss: 0.3661 - val_acc: 0.8512\n",
      "Epoch 681/1000\n",
      " - 4s - loss: 0.2940 - acc: 0.8884 - val_loss: 0.3556 - val_acc: 0.8595\n",
      "Epoch 682/1000\n",
      " - 4s - loss: 0.2614 - acc: 0.8760 - val_loss: 0.9866 - val_acc: 0.7025\n",
      "Epoch 683/1000\n",
      " - 4s - loss: 0.2318 - acc: 0.9050 - val_loss: 0.7589 - val_acc: 0.7025\n",
      "Epoch 684/1000\n",
      " - 4s - loss: 0.2287 - acc: 0.9091 - val_loss: 0.4016 - val_acc: 0.8843\n",
      "Epoch 685/1000\n",
      " - 4s - loss: 0.2517 - acc: 0.8946 - val_loss: 0.4325 - val_acc: 0.8595\n",
      "Epoch 686/1000\n",
      " - 4s - loss: 0.2536 - acc: 0.8988 - val_loss: 0.4268 - val_acc: 0.8595\n",
      "Epoch 687/1000\n",
      " - 4s - loss: 0.2301 - acc: 0.8967 - val_loss: 0.6065 - val_acc: 0.7603\n",
      "Epoch 688/1000\n",
      " - 4s - loss: 0.2627 - acc: 0.8843 - val_loss: 0.5998 - val_acc: 0.7934\n",
      "Epoch 689/1000\n",
      " - 4s - loss: 0.2538 - acc: 0.9008 - val_loss: 0.5664 - val_acc: 0.7521\n",
      "Epoch 690/1000\n",
      " - 4s - loss: 0.2695 - acc: 0.8802 - val_loss: 1.1154 - val_acc: 0.6446\n",
      "Epoch 691/1000\n",
      " - 4s - loss: 0.2503 - acc: 0.8905 - val_loss: 0.3981 - val_acc: 0.8595\n",
      "Epoch 692/1000\n",
      " - 4s - loss: 0.2807 - acc: 0.8802 - val_loss: 0.3846 - val_acc: 0.8430\n",
      "Epoch 693/1000\n",
      " - 4s - loss: 0.2320 - acc: 0.8967 - val_loss: 0.3846 - val_acc: 0.8595\n",
      "Epoch 694/1000\n",
      " - 4s - loss: 0.2524 - acc: 0.8946 - val_loss: 0.3792 - val_acc: 0.8595\n",
      "Epoch 695/1000\n",
      " - 4s - loss: 0.2615 - acc: 0.8760 - val_loss: 0.6394 - val_acc: 0.7438\n",
      "Epoch 696/1000\n",
      " - 4s - loss: 0.2360 - acc: 0.9070 - val_loss: 0.5513 - val_acc: 0.7851\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.2076 - acc: 0.9194 - val_loss: 0.3927 - val_acc: 0.8678\n",
      "Epoch 698/1000\n",
      " - 4s - loss: 0.2584 - acc: 0.8967 - val_loss: 0.8974 - val_acc: 0.7190\n",
      "Epoch 699/1000\n",
      " - 4s - loss: 0.2467 - acc: 0.9070 - val_loss: 0.4892 - val_acc: 0.8678\n",
      "Epoch 700/1000\n",
      " - 4s - loss: 0.2618 - acc: 0.8967 - val_loss: 1.1033 - val_acc: 0.6446\n",
      "Epoch 701/1000\n",
      " - 4s - loss: 0.2544 - acc: 0.8988 - val_loss: 1.2994 - val_acc: 0.6033\n",
      "Epoch 702/1000\n",
      " - 4s - loss: 0.2495 - acc: 0.8864 - val_loss: 0.4655 - val_acc: 0.8182\n",
      "Epoch 703/1000\n",
      " - 4s - loss: 0.2615 - acc: 0.8967 - val_loss: 0.6613 - val_acc: 0.7521\n",
      "Epoch 704/1000\n",
      " - 4s - loss: 0.2683 - acc: 0.8802 - val_loss: 2.4558 - val_acc: 0.5124\n",
      "Epoch 705/1000\n",
      " - 4s - loss: 0.2635 - acc: 0.8822 - val_loss: 0.4312 - val_acc: 0.8595\n",
      "Epoch 706/1000\n",
      " - 4s - loss: 0.2519 - acc: 0.8926 - val_loss: 1.6489 - val_acc: 0.5372\n",
      "Epoch 707/1000\n",
      " - 4s - loss: 0.2295 - acc: 0.9215 - val_loss: 1.0397 - val_acc: 0.6446\n",
      "Epoch 708/1000\n",
      " - 4s - loss: 0.2551 - acc: 0.8864 - val_loss: 0.4416 - val_acc: 0.8430\n",
      "Epoch 709/1000\n",
      " - 4s - loss: 0.2599 - acc: 0.8905 - val_loss: 0.3769 - val_acc: 0.8760\n",
      "Epoch 710/1000\n",
      " - 4s - loss: 0.2431 - acc: 0.8988 - val_loss: 0.3967 - val_acc: 0.8843\n",
      "Epoch 711/1000\n",
      " - 4s - loss: 0.2404 - acc: 0.9050 - val_loss: 3.7539 - val_acc: 0.5041\n",
      "Epoch 712/1000\n",
      " - 4s - loss: 0.2632 - acc: 0.8988 - val_loss: 0.3993 - val_acc: 0.8843\n",
      "Epoch 713/1000\n",
      " - 4s - loss: 0.2564 - acc: 0.8843 - val_loss: 0.8066 - val_acc: 0.7355\n",
      "Epoch 714/1000\n",
      " - 4s - loss: 0.2669 - acc: 0.8843 - val_loss: 0.4413 - val_acc: 0.8430\n",
      "Epoch 715/1000\n",
      " - 4s - loss: 0.2432 - acc: 0.8926 - val_loss: 0.4318 - val_acc: 0.8760\n",
      "Epoch 716/1000\n",
      " - 4s - loss: 0.2704 - acc: 0.8926 - val_loss: 0.4344 - val_acc: 0.8512\n",
      "Epoch 717/1000\n",
      " - 4s - loss: 0.2371 - acc: 0.8905 - val_loss: 0.6385 - val_acc: 0.7521\n",
      "Epoch 718/1000\n",
      " - 4s - loss: 0.2482 - acc: 0.9008 - val_loss: 0.6562 - val_acc: 0.7686\n",
      "Epoch 719/1000\n",
      " - 4s - loss: 0.2390 - acc: 0.8988 - val_loss: 0.4088 - val_acc: 0.8430\n",
      "Epoch 720/1000\n",
      " - 4s - loss: 0.2521 - acc: 0.8946 - val_loss: 0.5460 - val_acc: 0.7521\n",
      "Epoch 721/1000\n",
      " - 4s - loss: 0.2205 - acc: 0.9091 - val_loss: 0.4586 - val_acc: 0.8678\n",
      "Epoch 722/1000\n",
      " - 4s - loss: 0.2773 - acc: 0.8988 - val_loss: 0.6409 - val_acc: 0.7769\n",
      "Epoch 723/1000\n",
      " - 4s - loss: 0.2414 - acc: 0.8905 - val_loss: 0.6163 - val_acc: 0.7769\n",
      "Epoch 724/1000\n",
      " - 4s - loss: 0.2157 - acc: 0.9215 - val_loss: 0.5584 - val_acc: 0.7851\n",
      "Epoch 725/1000\n",
      " - 4s - loss: 0.2360 - acc: 0.9236 - val_loss: 0.4384 - val_acc: 0.8595\n",
      "Epoch 726/1000\n",
      " - 4s - loss: 0.2580 - acc: 0.9008 - val_loss: 0.4882 - val_acc: 0.8512\n",
      "Epoch 727/1000\n",
      " - 4s - loss: 0.2500 - acc: 0.8926 - val_loss: 0.4520 - val_acc: 0.8595\n",
      "Epoch 728/1000\n",
      " - 4s - loss: 0.2797 - acc: 0.8781 - val_loss: 0.6225 - val_acc: 0.7521\n",
      "Epoch 729/1000\n",
      " - 4s - loss: 0.2649 - acc: 0.8926 - val_loss: 2.5728 - val_acc: 0.5041\n",
      "Epoch 730/1000\n",
      " - 4s - loss: 0.2480 - acc: 0.9008 - val_loss: 0.5648 - val_acc: 0.7769\n",
      "Epoch 731/1000\n",
      " - 4s - loss: 0.2323 - acc: 0.8967 - val_loss: 0.3843 - val_acc: 0.8843\n",
      "Epoch 732/1000\n",
      " - 4s - loss: 0.2414 - acc: 0.9091 - val_loss: 0.4994 - val_acc: 0.8017\n",
      "Epoch 733/1000\n",
      " - 4s - loss: 0.2039 - acc: 0.9174 - val_loss: 0.4185 - val_acc: 0.8595\n",
      "Epoch 734/1000\n",
      " - 4s - loss: 0.2171 - acc: 0.9194 - val_loss: 0.4599 - val_acc: 0.8099\n",
      "Epoch 735/1000\n",
      " - 4s - loss: 0.2080 - acc: 0.9050 - val_loss: 0.4306 - val_acc: 0.8512\n",
      "Epoch 736/1000\n",
      " - 4s - loss: 0.2280 - acc: 0.9236 - val_loss: 0.5760 - val_acc: 0.8099\n",
      "Epoch 737/1000\n",
      " - 4s - loss: 0.2334 - acc: 0.8988 - val_loss: 0.4426 - val_acc: 0.8512\n",
      "Epoch 738/1000\n",
      " - 4s - loss: 0.2505 - acc: 0.9091 - val_loss: 0.3731 - val_acc: 0.9008\n",
      "Epoch 739/1000\n",
      " - 4s - loss: 0.2113 - acc: 0.9215 - val_loss: 1.4176 - val_acc: 0.6033\n",
      "Epoch 740/1000\n",
      " - 4s - loss: 0.2102 - acc: 0.9132 - val_loss: 1.4143 - val_acc: 0.6446\n",
      "Epoch 741/1000\n",
      " - 4s - loss: 0.2718 - acc: 0.8905 - val_loss: 0.5011 - val_acc: 0.8430\n",
      "Epoch 742/1000\n",
      " - 4s - loss: 0.2457 - acc: 0.8864 - val_loss: 1.0409 - val_acc: 0.6446\n",
      "Epoch 743/1000\n",
      " - 4s - loss: 0.2377 - acc: 0.9153 - val_loss: 1.7402 - val_acc: 0.5702\n",
      "Epoch 744/1000\n",
      " - 4s - loss: 0.2774 - acc: 0.8781 - val_loss: 0.9894 - val_acc: 0.6694\n",
      "Epoch 745/1000\n",
      " - 4s - loss: 0.2324 - acc: 0.9029 - val_loss: 0.9706 - val_acc: 0.6446\n",
      "Epoch 746/1000\n",
      " - 4s - loss: 0.2397 - acc: 0.8926 - val_loss: 0.4905 - val_acc: 0.8099\n",
      "Epoch 747/1000\n",
      " - 4s - loss: 0.2155 - acc: 0.9112 - val_loss: 0.9373 - val_acc: 0.7025\n",
      "Epoch 748/1000\n",
      " - 4s - loss: 0.2215 - acc: 0.9029 - val_loss: 0.4133 - val_acc: 0.8512\n",
      "Epoch 749/1000\n",
      " - 4s - loss: 0.2399 - acc: 0.9132 - val_loss: 0.5397 - val_acc: 0.8017\n",
      "Epoch 750/1000\n",
      " - 4s - loss: 0.2423 - acc: 0.8967 - val_loss: 0.8068 - val_acc: 0.7107\n",
      "Epoch 751/1000\n",
      " - 4s - loss: 0.2267 - acc: 0.9112 - val_loss: 0.3860 - val_acc: 0.9091\n",
      "Epoch 752/1000\n",
      " - 4s - loss: 0.2085 - acc: 0.9132 - val_loss: 0.5475 - val_acc: 0.8264\n",
      "Epoch 753/1000\n",
      " - 4s - loss: 0.2193 - acc: 0.9112 - val_loss: 0.5016 - val_acc: 0.8017\n",
      "Epoch 754/1000\n",
      " - 4s - loss: 0.2630 - acc: 0.8926 - val_loss: 0.4104 - val_acc: 0.8512\n",
      "Epoch 755/1000\n",
      " - 4s - loss: 0.2102 - acc: 0.9236 - val_loss: 0.3993 - val_acc: 0.8595\n",
      "Epoch 756/1000\n",
      " - 4s - loss: 0.2109 - acc: 0.9215 - val_loss: 0.3948 - val_acc: 0.8595\n",
      "Epoch 757/1000\n",
      " - 4s - loss: 0.2482 - acc: 0.8988 - val_loss: 0.4088 - val_acc: 0.8843\n",
      "Epoch 758/1000\n",
      " - 4s - loss: 0.2159 - acc: 0.9091 - val_loss: 0.5821 - val_acc: 0.7686\n",
      "Epoch 759/1000\n",
      " - 4s - loss: 0.2265 - acc: 0.9091 - val_loss: 0.4622 - val_acc: 0.8843\n",
      "Epoch 760/1000\n",
      " - 4s - loss: 0.2434 - acc: 0.9050 - val_loss: 1.4802 - val_acc: 0.6529\n",
      "Epoch 761/1000\n",
      " - 4s - loss: 0.2333 - acc: 0.9050 - val_loss: 1.1002 - val_acc: 0.6364\n",
      "Epoch 762/1000\n",
      " - 4s - loss: 0.2079 - acc: 0.8988 - val_loss: 1.3387 - val_acc: 0.5785\n",
      "Epoch 763/1000\n",
      " - 4s - loss: 0.2581 - acc: 0.8946 - val_loss: 0.4877 - val_acc: 0.8099\n",
      "Epoch 764/1000\n",
      " - 4s - loss: 0.2093 - acc: 0.9153 - val_loss: 0.4585 - val_acc: 0.8595\n",
      "Epoch 765/1000\n",
      " - 4s - loss: 0.2313 - acc: 0.8988 - val_loss: 0.5027 - val_acc: 0.8264\n",
      "Epoch 766/1000\n",
      " - 4s - loss: 0.2449 - acc: 0.9070 - val_loss: 0.7298 - val_acc: 0.7769\n",
      "Epoch 767/1000\n",
      " - 4s - loss: 0.2674 - acc: 0.9029 - val_loss: 0.5073 - val_acc: 0.8099\n",
      "Epoch 768/1000\n",
      " - 4s - loss: 0.2332 - acc: 0.8988 - val_loss: 0.3933 - val_acc: 0.8926\n",
      "Epoch 769/1000\n",
      " - 4s - loss: 0.2307 - acc: 0.9132 - val_loss: 0.7533 - val_acc: 0.7603\n",
      "Epoch 770/1000\n",
      " - 4s - loss: 0.1905 - acc: 0.9215 - val_loss: 0.7279 - val_acc: 0.7355\n",
      "Epoch 771/1000\n",
      " - 4s - loss: 0.2329 - acc: 0.8967 - val_loss: 0.6084 - val_acc: 0.7686\n",
      "Epoch 772/1000\n",
      " - 4s - loss: 0.2534 - acc: 0.8822 - val_loss: 0.6953 - val_acc: 0.7438\n",
      "Epoch 773/1000\n",
      " - 4s - loss: 0.2135 - acc: 0.9132 - val_loss: 0.7287 - val_acc: 0.7438\n",
      "Epoch 774/1000\n",
      " - 4s - loss: 0.2210 - acc: 0.9132 - val_loss: 0.6468 - val_acc: 0.7521\n",
      "Epoch 775/1000\n",
      " - 4s - loss: 0.2453 - acc: 0.9070 - val_loss: 0.8036 - val_acc: 0.7438\n",
      "Epoch 776/1000\n",
      " - 4s - loss: 0.2140 - acc: 0.9194 - val_loss: 0.4010 - val_acc: 0.9008\n",
      "Epoch 777/1000\n",
      " - 4s - loss: 0.2207 - acc: 0.9008 - val_loss: 0.4087 - val_acc: 0.8678\n",
      "Epoch 778/1000\n",
      " - 4s - loss: 0.2286 - acc: 0.8988 - val_loss: 0.4481 - val_acc: 0.8512\n",
      "Epoch 779/1000\n",
      " - 4s - loss: 0.2474 - acc: 0.9029 - val_loss: 0.4130 - val_acc: 0.8843\n",
      "Epoch 780/1000\n",
      " - 4s - loss: 0.1947 - acc: 0.9112 - val_loss: 0.3644 - val_acc: 0.8678\n",
      "Epoch 781/1000\n",
      " - 4s - loss: 0.2117 - acc: 0.9174 - val_loss: 0.3841 - val_acc: 0.8843\n",
      "Epoch 782/1000\n",
      " - 4s - loss: 0.2218 - acc: 0.9236 - val_loss: 0.4633 - val_acc: 0.8512\n",
      "Epoch 783/1000\n",
      " - 4s - loss: 0.2046 - acc: 0.9256 - val_loss: 0.9696 - val_acc: 0.7025\n",
      "Epoch 784/1000\n",
      " - 4s - loss: 0.1949 - acc: 0.9132 - val_loss: 0.7820 - val_acc: 0.7438\n",
      "Epoch 785/1000\n",
      " - 4s - loss: 0.2196 - acc: 0.9070 - val_loss: 2.1281 - val_acc: 0.5372\n",
      "Epoch 786/1000\n",
      " - 4s - loss: 0.2647 - acc: 0.8864 - val_loss: 1.1271 - val_acc: 0.6612\n",
      "Epoch 787/1000\n",
      " - 4s - loss: 0.2086 - acc: 0.9215 - val_loss: 0.3666 - val_acc: 0.8926\n",
      "Epoch 788/1000\n",
      " - 4s - loss: 0.2256 - acc: 0.8988 - val_loss: 0.4705 - val_acc: 0.8512\n",
      "Epoch 789/1000\n",
      " - 4s - loss: 0.2105 - acc: 0.9277 - val_loss: 0.7813 - val_acc: 0.7273\n",
      "Epoch 790/1000\n",
      " - 4s - loss: 0.2279 - acc: 0.9029 - val_loss: 0.3656 - val_acc: 0.8843\n",
      "Epoch 791/1000\n",
      " - 4s - loss: 0.1720 - acc: 0.9442 - val_loss: 0.4570 - val_acc: 0.8512\n",
      "Epoch 792/1000\n",
      " - 4s - loss: 0.2345 - acc: 0.9029 - val_loss: 0.5626 - val_acc: 0.8017\n",
      "Epoch 793/1000\n",
      " - 4s - loss: 0.2080 - acc: 0.9153 - val_loss: 0.8560 - val_acc: 0.7355\n",
      "Epoch 794/1000\n",
      " - 4s - loss: 0.2215 - acc: 0.9029 - val_loss: 0.8959 - val_acc: 0.7438\n",
      "Epoch 795/1000\n",
      " - 4s - loss: 0.2220 - acc: 0.9112 - val_loss: 0.7713 - val_acc: 0.7603\n",
      "Epoch 796/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.1862 - acc: 0.9236 - val_loss: 0.3825 - val_acc: 0.9008\n",
      "Epoch 797/1000\n",
      " - 4s - loss: 0.2299 - acc: 0.9029 - val_loss: 0.3872 - val_acc: 0.8760\n",
      "Epoch 798/1000\n",
      " - 4s - loss: 0.2221 - acc: 0.9132 - val_loss: 1.2300 - val_acc: 0.6694\n",
      "Epoch 799/1000\n",
      " - 4s - loss: 0.2326 - acc: 0.9029 - val_loss: 1.7136 - val_acc: 0.6033\n",
      "Epoch 800/1000\n",
      " - 4s - loss: 0.2214 - acc: 0.9153 - val_loss: 0.9394 - val_acc: 0.6860\n",
      "Epoch 801/1000\n",
      " - 4s - loss: 0.2284 - acc: 0.9215 - val_loss: 0.6974 - val_acc: 0.7438\n",
      "Epoch 802/1000\n",
      " - 4s - loss: 0.2090 - acc: 0.9029 - val_loss: 0.7449 - val_acc: 0.7190\n",
      "Epoch 803/1000\n",
      " - 4s - loss: 0.2074 - acc: 0.9174 - val_loss: 0.9202 - val_acc: 0.7273\n",
      "Epoch 804/1000\n",
      " - 4s - loss: 0.2063 - acc: 0.9112 - val_loss: 1.2720 - val_acc: 0.6198\n",
      "Epoch 805/1000\n",
      " - 4s - loss: 0.2191 - acc: 0.9174 - val_loss: 0.6133 - val_acc: 0.7769\n",
      "Epoch 806/1000\n",
      " - 4s - loss: 0.2121 - acc: 0.9153 - val_loss: 0.4840 - val_acc: 0.8264\n",
      "Epoch 807/1000\n",
      " - 4s - loss: 0.2082 - acc: 0.9050 - val_loss: 0.5971 - val_acc: 0.7686\n",
      "Epoch 808/1000\n",
      " - 4s - loss: 0.2239 - acc: 0.9112 - val_loss: 0.6541 - val_acc: 0.7273\n",
      "Epoch 809/1000\n",
      " - 4s - loss: 0.2516 - acc: 0.8884 - val_loss: 0.7350 - val_acc: 0.7438\n",
      "Epoch 810/1000\n",
      " - 4s - loss: 0.1776 - acc: 0.9277 - val_loss: 0.7305 - val_acc: 0.7190\n",
      "Epoch 811/1000\n",
      " - 4s - loss: 0.2302 - acc: 0.9070 - val_loss: 0.5374 - val_acc: 0.8182\n",
      "Epoch 812/1000\n",
      " - 4s - loss: 0.2188 - acc: 0.9029 - val_loss: 0.4758 - val_acc: 0.8347\n",
      "Epoch 813/1000\n",
      " - 4s - loss: 0.2349 - acc: 0.9050 - val_loss: 0.4033 - val_acc: 0.8512\n",
      "Epoch 814/1000\n",
      " - 4s - loss: 0.2467 - acc: 0.8946 - val_loss: 0.4112 - val_acc: 0.8430\n",
      "Epoch 815/1000\n",
      " - 4s - loss: 0.2376 - acc: 0.9091 - val_loss: 0.4557 - val_acc: 0.8264\n",
      "Epoch 816/1000\n",
      " - 4s - loss: 0.1951 - acc: 0.9256 - val_loss: 0.4405 - val_acc: 0.8926\n",
      "Epoch 817/1000\n",
      " - 4s - loss: 0.2313 - acc: 0.9029 - val_loss: 0.3936 - val_acc: 0.8760\n",
      "Epoch 818/1000\n",
      " - 4s - loss: 0.2048 - acc: 0.9050 - val_loss: 0.7239 - val_acc: 0.7438\n",
      "Epoch 819/1000\n",
      " - 4s - loss: 0.2080 - acc: 0.9194 - val_loss: 0.7916 - val_acc: 0.7273\n",
      "Epoch 820/1000\n",
      " - 4s - loss: 0.2208 - acc: 0.9050 - val_loss: 1.4856 - val_acc: 0.6033\n",
      "Epoch 821/1000\n",
      " - 4s - loss: 0.2187 - acc: 0.9091 - val_loss: 0.6675 - val_acc: 0.7438\n",
      "Epoch 822/1000\n",
      " - 4s - loss: 0.2118 - acc: 0.9050 - val_loss: 0.7979 - val_acc: 0.7355\n",
      "Epoch 823/1000\n",
      " - 4s - loss: 0.2041 - acc: 0.9174 - val_loss: 0.3733 - val_acc: 0.8843\n",
      "Epoch 824/1000\n",
      " - 4s - loss: 0.2025 - acc: 0.9215 - val_loss: 0.6034 - val_acc: 0.8182\n",
      "Epoch 825/1000\n",
      " - 4s - loss: 0.1778 - acc: 0.9256 - val_loss: 0.3675 - val_acc: 0.9174\n",
      "Epoch 826/1000\n",
      " - 4s - loss: 0.2404 - acc: 0.8926 - val_loss: 1.1089 - val_acc: 0.6612\n",
      "Epoch 827/1000\n",
      " - 4s - loss: 0.2295 - acc: 0.9070 - val_loss: 0.4602 - val_acc: 0.8099\n",
      "Epoch 828/1000\n",
      " - 4s - loss: 0.2239 - acc: 0.8967 - val_loss: 0.6493 - val_acc: 0.8017\n",
      "Epoch 829/1000\n",
      " - 4s - loss: 0.2137 - acc: 0.9174 - val_loss: 0.5468 - val_acc: 0.7769\n",
      "Epoch 830/1000\n",
      " - 4s - loss: 0.2177 - acc: 0.9008 - val_loss: 0.7014 - val_acc: 0.7355\n",
      "Epoch 831/1000\n",
      " - 4s - loss: 0.2028 - acc: 0.9215 - val_loss: 0.4195 - val_acc: 0.9008\n",
      "Epoch 832/1000\n",
      " - 4s - loss: 0.1850 - acc: 0.9256 - val_loss: 1.2891 - val_acc: 0.6364\n",
      "Epoch 833/1000\n",
      " - 4s - loss: 0.2168 - acc: 0.9174 - val_loss: 0.3984 - val_acc: 0.8760\n",
      "Epoch 834/1000\n",
      " - 4s - loss: 0.2017 - acc: 0.9236 - val_loss: 0.4776 - val_acc: 0.8595\n",
      "Epoch 835/1000\n",
      " - 4s - loss: 0.2272 - acc: 0.9029 - val_loss: 1.1711 - val_acc: 0.6612\n",
      "Epoch 836/1000\n",
      " - 4s - loss: 0.2486 - acc: 0.8988 - val_loss: 0.4520 - val_acc: 0.8595\n",
      "Epoch 837/1000\n",
      " - 4s - loss: 0.2040 - acc: 0.9132 - val_loss: 0.4545 - val_acc: 0.8678\n",
      "Epoch 838/1000\n",
      " - 4s - loss: 0.1885 - acc: 0.9215 - val_loss: 0.5080 - val_acc: 0.8347\n",
      "Epoch 839/1000\n",
      " - 4s - loss: 0.1969 - acc: 0.9215 - val_loss: 0.4507 - val_acc: 0.8678\n",
      "Epoch 840/1000\n",
      " - 4s - loss: 0.1952 - acc: 0.9236 - val_loss: 0.3907 - val_acc: 0.8760\n",
      "Epoch 841/1000\n",
      " - 4s - loss: 0.2465 - acc: 0.8967 - val_loss: 0.4266 - val_acc: 0.9008\n",
      "Epoch 842/1000\n",
      " - 4s - loss: 0.2362 - acc: 0.8926 - val_loss: 0.4194 - val_acc: 0.8926\n",
      "Epoch 843/1000\n",
      " - 4s - loss: 0.2054 - acc: 0.9256 - val_loss: 1.1496 - val_acc: 0.6860\n",
      "Epoch 844/1000\n",
      " - 4s - loss: 0.1971 - acc: 0.9070 - val_loss: 1.3865 - val_acc: 0.6198\n",
      "Epoch 845/1000\n",
      " - 4s - loss: 0.2091 - acc: 0.9112 - val_loss: 0.4929 - val_acc: 0.8347\n",
      "Epoch 846/1000\n",
      " - 4s - loss: 0.2369 - acc: 0.9070 - val_loss: 0.3794 - val_acc: 0.8347\n",
      "Epoch 847/1000\n",
      " - 4s - loss: 0.1905 - acc: 0.9318 - val_loss: 0.4143 - val_acc: 0.8678\n",
      "Epoch 848/1000\n",
      " - 4s - loss: 0.2054 - acc: 0.9236 - val_loss: 0.3899 - val_acc: 0.8760\n",
      "Epoch 849/1000\n",
      " - 4s - loss: 0.1751 - acc: 0.9132 - val_loss: 0.5326 - val_acc: 0.7934\n",
      "Epoch 850/1000\n",
      " - 4s - loss: 0.2400 - acc: 0.8988 - val_loss: 0.6140 - val_acc: 0.8017\n",
      "Epoch 851/1000\n",
      " - 4s - loss: 0.2357 - acc: 0.9112 - val_loss: 0.4421 - val_acc: 0.8347\n",
      "Epoch 852/1000\n",
      " - 4s - loss: 0.2004 - acc: 0.9194 - val_loss: 0.6568 - val_acc: 0.7438\n",
      "Epoch 853/1000\n",
      " - 4s - loss: 0.2242 - acc: 0.9070 - val_loss: 0.4436 - val_acc: 0.8843\n",
      "Epoch 854/1000\n",
      " - 4s - loss: 0.1892 - acc: 0.9298 - val_loss: 1.2959 - val_acc: 0.6281\n",
      "Epoch 855/1000\n",
      " - 4s - loss: 0.2095 - acc: 0.9215 - val_loss: 0.7455 - val_acc: 0.7438\n",
      "Epoch 856/1000\n",
      " - 4s - loss: 0.1866 - acc: 0.9215 - val_loss: 0.4315 - val_acc: 0.9008\n",
      "Epoch 857/1000\n",
      " - 4s - loss: 0.2062 - acc: 0.9215 - val_loss: 1.9853 - val_acc: 0.5868\n",
      "Epoch 858/1000\n",
      " - 4s - loss: 0.1839 - acc: 0.9339 - val_loss: 1.1442 - val_acc: 0.6694\n",
      "Epoch 859/1000\n",
      " - 4s - loss: 0.1932 - acc: 0.9277 - val_loss: 0.8174 - val_acc: 0.7190\n",
      "Epoch 860/1000\n",
      " - 4s - loss: 0.2083 - acc: 0.9174 - val_loss: 0.4687 - val_acc: 0.8347\n",
      "Epoch 861/1000\n",
      " - 4s - loss: 0.1880 - acc: 0.9256 - val_loss: 0.6230 - val_acc: 0.8017\n",
      "Epoch 862/1000\n",
      " - 4s - loss: 0.2075 - acc: 0.9215 - val_loss: 0.3996 - val_acc: 0.8843\n",
      "Epoch 863/1000\n",
      " - 4s - loss: 0.2044 - acc: 0.9174 - val_loss: 0.4160 - val_acc: 0.8678\n",
      "Epoch 864/1000\n",
      " - 4s - loss: 0.1981 - acc: 0.9112 - val_loss: 0.5183 - val_acc: 0.8430\n",
      "Epoch 865/1000\n",
      " - 4s - loss: 0.2021 - acc: 0.9008 - val_loss: 0.5026 - val_acc: 0.8512\n",
      "Epoch 866/1000\n",
      " - 4s - loss: 0.2279 - acc: 0.9008 - val_loss: 0.4247 - val_acc: 0.8760\n",
      "Epoch 867/1000\n",
      " - 4s - loss: 0.1557 - acc: 0.9504 - val_loss: 0.7405 - val_acc: 0.7603\n",
      "Epoch 868/1000\n",
      " - 4s - loss: 0.2004 - acc: 0.9298 - val_loss: 0.6146 - val_acc: 0.8017\n",
      "Epoch 869/1000\n",
      " - 4s - loss: 0.1777 - acc: 0.9194 - val_loss: 0.5638 - val_acc: 0.8264\n",
      "Epoch 870/1000\n",
      " - 4s - loss: 0.2070 - acc: 0.9050 - val_loss: 2.9002 - val_acc: 0.5289\n",
      "Epoch 871/1000\n",
      " - 4s - loss: 0.2031 - acc: 0.9215 - val_loss: 0.4102 - val_acc: 0.9091\n",
      "Epoch 872/1000\n",
      " - 4s - loss: 0.1846 - acc: 0.9298 - val_loss: 0.8251 - val_acc: 0.7273\n",
      "Epoch 873/1000\n",
      " - 4s - loss: 0.1833 - acc: 0.9256 - val_loss: 0.7338 - val_acc: 0.7851\n",
      "Epoch 874/1000\n",
      " - 4s - loss: 0.1572 - acc: 0.9339 - val_loss: 0.9087 - val_acc: 0.6942\n",
      "Epoch 875/1000\n",
      " - 4s - loss: 0.1881 - acc: 0.9236 - val_loss: 0.4159 - val_acc: 0.8926\n",
      "Epoch 876/1000\n",
      " - 4s - loss: 0.2002 - acc: 0.9091 - val_loss: 0.4861 - val_acc: 0.8430\n",
      "Epoch 877/1000\n",
      " - 4s - loss: 0.2080 - acc: 0.9174 - val_loss: 0.3866 - val_acc: 0.8843\n",
      "Epoch 878/1000\n",
      " - 4s - loss: 0.1648 - acc: 0.9380 - val_loss: 0.9239 - val_acc: 0.6860\n",
      "Epoch 879/1000\n",
      " - 4s - loss: 0.1881 - acc: 0.9236 - val_loss: 1.8881 - val_acc: 0.5950\n",
      "Epoch 880/1000\n",
      " - 4s - loss: 0.1997 - acc: 0.9132 - val_loss: 0.6778 - val_acc: 0.8099\n",
      "Epoch 881/1000\n",
      " - 4s - loss: 0.2067 - acc: 0.9256 - val_loss: 0.4880 - val_acc: 0.8430\n",
      "Epoch 882/1000\n",
      " - 4s - loss: 0.1624 - acc: 0.9442 - val_loss: 0.4542 - val_acc: 0.8347\n",
      "Epoch 883/1000\n",
      " - 4s - loss: 0.2092 - acc: 0.9112 - val_loss: 1.1678 - val_acc: 0.6777\n",
      "Epoch 884/1000\n",
      " - 4s - loss: 0.1973 - acc: 0.9174 - val_loss: 0.3819 - val_acc: 0.9008\n",
      "Epoch 885/1000\n",
      " - 4s - loss: 0.2367 - acc: 0.9008 - val_loss: 0.5797 - val_acc: 0.8347\n",
      "Epoch 886/1000\n",
      " - 4s - loss: 0.2137 - acc: 0.9050 - val_loss: 0.4448 - val_acc: 0.8843\n",
      "Epoch 887/1000\n",
      " - 4s - loss: 0.1937 - acc: 0.9277 - val_loss: 0.3829 - val_acc: 0.9008\n",
      "Epoch 888/1000\n",
      " - 4s - loss: 0.1884 - acc: 0.9194 - val_loss: 0.4014 - val_acc: 0.8430\n",
      "Epoch 889/1000\n",
      " - 4s - loss: 0.1906 - acc: 0.9194 - val_loss: 0.9222 - val_acc: 0.7273\n",
      "Epoch 890/1000\n",
      " - 4s - loss: 0.1846 - acc: 0.9132 - val_loss: 0.4378 - val_acc: 0.8347\n",
      "Epoch 891/1000\n",
      " - 4s - loss: 0.1814 - acc: 0.9339 - val_loss: 0.4448 - val_acc: 0.8512\n",
      "Epoch 892/1000\n",
      " - 4s - loss: 0.1688 - acc: 0.9277 - val_loss: 0.6699 - val_acc: 0.7686\n",
      "Epoch 893/1000\n",
      " - 4s - loss: 0.2153 - acc: 0.8988 - val_loss: 0.8787 - val_acc: 0.7025\n",
      "Epoch 894/1000\n",
      " - 4s - loss: 0.1791 - acc: 0.9236 - val_loss: 0.3762 - val_acc: 0.8926\n",
      "Epoch 895/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.1785 - acc: 0.9236 - val_loss: 1.0264 - val_acc: 0.7107\n",
      "Epoch 896/1000\n",
      " - 4s - loss: 0.2319 - acc: 0.9174 - val_loss: 0.5220 - val_acc: 0.8182\n",
      "Epoch 897/1000\n",
      " - 4s - loss: 0.1584 - acc: 0.9421 - val_loss: 1.0911 - val_acc: 0.6777\n",
      "Epoch 898/1000\n",
      " - 4s - loss: 0.1714 - acc: 0.9256 - val_loss: 1.2502 - val_acc: 0.6612\n",
      "Epoch 899/1000\n",
      " - 4s - loss: 0.1985 - acc: 0.9194 - val_loss: 0.8706 - val_acc: 0.7107\n",
      "Epoch 900/1000\n",
      " - 4s - loss: 0.1950 - acc: 0.9112 - val_loss: 0.7457 - val_acc: 0.7355\n",
      "Epoch 901/1000\n",
      " - 4s - loss: 0.1614 - acc: 0.9421 - val_loss: 1.3950 - val_acc: 0.6694\n",
      "Epoch 902/1000\n",
      " - 4s - loss: 0.2153 - acc: 0.9091 - val_loss: 0.4757 - val_acc: 0.8430\n",
      "Epoch 903/1000\n",
      " - 4s - loss: 0.1636 - acc: 0.9339 - val_loss: 0.4786 - val_acc: 0.8678\n",
      "Epoch 904/1000\n",
      " - 4s - loss: 0.1969 - acc: 0.9298 - val_loss: 0.4270 - val_acc: 0.8843\n",
      "Epoch 905/1000\n",
      " - 4s - loss: 0.2162 - acc: 0.9236 - val_loss: 0.4242 - val_acc: 0.8926\n",
      "Epoch 906/1000\n",
      " - 4s - loss: 0.1872 - acc: 0.9215 - val_loss: 0.4211 - val_acc: 0.8595\n",
      "Epoch 907/1000\n",
      " - 4s - loss: 0.2253 - acc: 0.9008 - val_loss: 0.5622 - val_acc: 0.8099\n",
      "Epoch 908/1000\n",
      " - 4s - loss: 0.1955 - acc: 0.9339 - val_loss: 1.8065 - val_acc: 0.5868\n",
      "Epoch 909/1000\n",
      " - 4s - loss: 0.1837 - acc: 0.9380 - val_loss: 1.4182 - val_acc: 0.6116\n",
      "Epoch 910/1000\n",
      " - 4s - loss: 0.2222 - acc: 0.9029 - val_loss: 0.4801 - val_acc: 0.8347\n",
      "Epoch 911/1000\n",
      " - 4s - loss: 0.1863 - acc: 0.9132 - val_loss: 0.4073 - val_acc: 0.8760\n",
      "Epoch 912/1000\n",
      " - 4s - loss: 0.1940 - acc: 0.9236 - val_loss: 0.6442 - val_acc: 0.7934\n",
      "Epoch 913/1000\n",
      " - 4s - loss: 0.1896 - acc: 0.9256 - val_loss: 0.6324 - val_acc: 0.7686\n",
      "Epoch 914/1000\n",
      " - 4s - loss: 0.1831 - acc: 0.9174 - val_loss: 0.4710 - val_acc: 0.8843\n",
      "Epoch 915/1000\n",
      " - 4s - loss: 0.1954 - acc: 0.9174 - val_loss: 0.4686 - val_acc: 0.8678\n",
      "Epoch 916/1000\n",
      " - 4s - loss: 0.2264 - acc: 0.9112 - val_loss: 2.4279 - val_acc: 0.5372\n",
      "Epoch 917/1000\n",
      " - 4s - loss: 0.1651 - acc: 0.9421 - val_loss: 0.4399 - val_acc: 0.8595\n",
      "Epoch 918/1000\n",
      " - 4s - loss: 0.1933 - acc: 0.9194 - val_loss: 0.6011 - val_acc: 0.7603\n",
      "Epoch 919/1000\n",
      " - 4s - loss: 0.1975 - acc: 0.9132 - val_loss: 0.4660 - val_acc: 0.8595\n",
      "Epoch 920/1000\n",
      " - 4s - loss: 0.2126 - acc: 0.9256 - val_loss: 0.9503 - val_acc: 0.7355\n",
      "Epoch 921/1000\n",
      " - 4s - loss: 0.1609 - acc: 0.9380 - val_loss: 0.4452 - val_acc: 0.8760\n",
      "Epoch 922/1000\n",
      " - 4s - loss: 0.1431 - acc: 0.9463 - val_loss: 1.4150 - val_acc: 0.6281\n",
      "Epoch 923/1000\n",
      " - 4s - loss: 0.2155 - acc: 0.9153 - val_loss: 0.4746 - val_acc: 0.8843\n",
      "Epoch 924/1000\n",
      " - 4s - loss: 0.1751 - acc: 0.9277 - val_loss: 2.3024 - val_acc: 0.5372\n",
      "Epoch 925/1000\n",
      " - 4s - loss: 0.1703 - acc: 0.9277 - val_loss: 0.5271 - val_acc: 0.8182\n",
      "Epoch 926/1000\n",
      " - 4s - loss: 0.2115 - acc: 0.9091 - val_loss: 0.6881 - val_acc: 0.7438\n",
      "Epoch 927/1000\n",
      " - 4s - loss: 0.1543 - acc: 0.9339 - val_loss: 0.5305 - val_acc: 0.8099\n",
      "Epoch 928/1000\n",
      " - 4s - loss: 0.1967 - acc: 0.9256 - val_loss: 0.4257 - val_acc: 0.8926\n",
      "Epoch 929/1000\n",
      " - 4s - loss: 0.1950 - acc: 0.9174 - val_loss: 0.6741 - val_acc: 0.7851\n",
      "Epoch 930/1000\n",
      " - 4s - loss: 0.1838 - acc: 0.9174 - val_loss: 0.4424 - val_acc: 0.8430\n",
      "Epoch 931/1000\n",
      " - 4s - loss: 0.2145 - acc: 0.9132 - val_loss: 0.5296 - val_acc: 0.7934\n",
      "Epoch 932/1000\n",
      " - 4s - loss: 0.1546 - acc: 0.9421 - val_loss: 0.6455 - val_acc: 0.8182\n",
      "Epoch 933/1000\n",
      " - 4s - loss: 0.1722 - acc: 0.9318 - val_loss: 0.4484 - val_acc: 0.8926\n",
      "Epoch 934/1000\n",
      " - 4s - loss: 0.1977 - acc: 0.9236 - val_loss: 0.7434 - val_acc: 0.7603\n",
      "Epoch 935/1000\n",
      " - 4s - loss: 0.1666 - acc: 0.9401 - val_loss: 0.6106 - val_acc: 0.7851\n",
      "Epoch 936/1000\n",
      " - 4s - loss: 0.2086 - acc: 0.9091 - val_loss: 0.5261 - val_acc: 0.8512\n",
      "Epoch 937/1000\n",
      " - 4s - loss: 0.1896 - acc: 0.9256 - val_loss: 0.7919 - val_acc: 0.7355\n",
      "Epoch 938/1000\n",
      " - 4s - loss: 0.1994 - acc: 0.9174 - val_loss: 0.5344 - val_acc: 0.8099\n",
      "Epoch 939/1000\n",
      " - 4s - loss: 0.1881 - acc: 0.9298 - val_loss: 0.4111 - val_acc: 0.8843\n",
      "Epoch 940/1000\n",
      " - 4s - loss: 0.1842 - acc: 0.9256 - val_loss: 0.5357 - val_acc: 0.8347\n",
      "Epoch 941/1000\n",
      " - 4s - loss: 0.1889 - acc: 0.9153 - val_loss: 0.8135 - val_acc: 0.7190\n",
      "Epoch 942/1000\n",
      " - 4s - loss: 0.1754 - acc: 0.9236 - val_loss: 0.3793 - val_acc: 0.9008\n",
      "Epoch 943/1000\n",
      " - 4s - loss: 0.1957 - acc: 0.9132 - val_loss: 0.4015 - val_acc: 0.8843\n",
      "Epoch 944/1000\n",
      " - 4s - loss: 0.1319 - acc: 0.9504 - val_loss: 0.4171 - val_acc: 0.8760\n",
      "Epoch 945/1000\n",
      " - 4s - loss: 0.1992 - acc: 0.9236 - val_loss: 0.7601 - val_acc: 0.7355\n",
      "Epoch 946/1000\n",
      " - 4s - loss: 0.1696 - acc: 0.9380 - val_loss: 0.5211 - val_acc: 0.8264\n",
      "Epoch 947/1000\n",
      " - 4s - loss: 0.1784 - acc: 0.9194 - val_loss: 0.5730 - val_acc: 0.8017\n",
      "Epoch 948/1000\n",
      " - 4s - loss: 0.1655 - acc: 0.9318 - val_loss: 0.5024 - val_acc: 0.8430\n",
      "Epoch 949/1000\n",
      " - 4s - loss: 0.1961 - acc: 0.9112 - val_loss: 0.3825 - val_acc: 0.9174\n",
      "Epoch 950/1000\n",
      " - 4s - loss: 0.1443 - acc: 0.9545 - val_loss: 0.4875 - val_acc: 0.8678\n",
      "Epoch 951/1000\n",
      " - 4s - loss: 0.1899 - acc: 0.9070 - val_loss: 0.6156 - val_acc: 0.8099\n",
      "Epoch 952/1000\n",
      " - 4s - loss: 0.1585 - acc: 0.9277 - val_loss: 1.2250 - val_acc: 0.6942\n",
      "Epoch 953/1000\n",
      " - 4s - loss: 0.1622 - acc: 0.9380 - val_loss: 0.5593 - val_acc: 0.8595\n",
      "Epoch 954/1000\n",
      " - 4s - loss: 0.2412 - acc: 0.8967 - val_loss: 2.0606 - val_acc: 0.5620\n",
      "Epoch 955/1000\n",
      " - 4s - loss: 0.1463 - acc: 0.9545 - val_loss: 0.6624 - val_acc: 0.7769\n",
      "Epoch 956/1000\n",
      " - 4s - loss: 0.1838 - acc: 0.9236 - val_loss: 0.4220 - val_acc: 0.8926\n",
      "Epoch 957/1000\n",
      " - 4s - loss: 0.1866 - acc: 0.9236 - val_loss: 0.7532 - val_acc: 0.7603\n",
      "Epoch 958/1000\n",
      " - 4s - loss: 0.1831 - acc: 0.9277 - val_loss: 0.5020 - val_acc: 0.8595\n",
      "Epoch 959/1000\n",
      " - 4s - loss: 0.1886 - acc: 0.9256 - val_loss: 0.4306 - val_acc: 0.8843\n",
      "Epoch 960/1000\n",
      " - 4s - loss: 0.1959 - acc: 0.9256 - val_loss: 1.2912 - val_acc: 0.6694\n",
      "Epoch 961/1000\n",
      " - 4s - loss: 0.1718 - acc: 0.9360 - val_loss: 0.4307 - val_acc: 0.8760\n",
      "Epoch 962/1000\n",
      " - 4s - loss: 0.1737 - acc: 0.9256 - val_loss: 0.7596 - val_acc: 0.7521\n",
      "Epoch 963/1000\n",
      " - 4s - loss: 0.1637 - acc: 0.9360 - val_loss: 0.4270 - val_acc: 0.8843\n",
      "Epoch 964/1000\n",
      " - 4s - loss: 0.1793 - acc: 0.9277 - val_loss: 0.3777 - val_acc: 0.9174\n",
      "Epoch 965/1000\n",
      " - 4s - loss: 0.1900 - acc: 0.9277 - val_loss: 0.4684 - val_acc: 0.8678\n",
      "Epoch 966/1000\n",
      " - 4s - loss: 0.1822 - acc: 0.9194 - val_loss: 0.5190 - val_acc: 0.8512\n",
      "Epoch 967/1000\n",
      " - 4s - loss: 0.1407 - acc: 0.9504 - val_loss: 0.4806 - val_acc: 0.8264\n",
      "Epoch 968/1000\n",
      " - 4s - loss: 0.1699 - acc: 0.9277 - val_loss: 0.6491 - val_acc: 0.7851\n",
      "Epoch 969/1000\n",
      " - 4s - loss: 0.1918 - acc: 0.9153 - val_loss: 0.5169 - val_acc: 0.8347\n",
      "Epoch 970/1000\n",
      " - 4s - loss: 0.1572 - acc: 0.9421 - val_loss: 0.8482 - val_acc: 0.7355\n",
      "Epoch 971/1000\n",
      " - 4s - loss: 0.2129 - acc: 0.9091 - val_loss: 0.9983 - val_acc: 0.7025\n",
      "Epoch 972/1000\n",
      " - 4s - loss: 0.1731 - acc: 0.9380 - val_loss: 0.4248 - val_acc: 0.8926\n",
      "Epoch 973/1000\n",
      " - 4s - loss: 0.1745 - acc: 0.9256 - val_loss: 0.7760 - val_acc: 0.7769\n",
      "Epoch 974/1000\n",
      " - 4s - loss: 0.1875 - acc: 0.9236 - val_loss: 0.9416 - val_acc: 0.7025\n",
      "Epoch 975/1000\n",
      " - 4s - loss: 0.1987 - acc: 0.9277 - val_loss: 0.4474 - val_acc: 0.8678\n",
      "Epoch 976/1000\n",
      " - 4s - loss: 0.1822 - acc: 0.9298 - val_loss: 0.4220 - val_acc: 0.8843\n",
      "Epoch 977/1000\n",
      " - 4s - loss: 0.1968 - acc: 0.9132 - val_loss: 0.6278 - val_acc: 0.7769\n",
      "Epoch 978/1000\n",
      " - 4s - loss: 0.2140 - acc: 0.9070 - val_loss: 0.4732 - val_acc: 0.8512\n",
      "Epoch 979/1000\n",
      " - 4s - loss: 0.2099 - acc: 0.9153 - val_loss: 0.5048 - val_acc: 0.8678\n",
      "Epoch 980/1000\n",
      " - 4s - loss: 0.1604 - acc: 0.9318 - val_loss: 0.6486 - val_acc: 0.7851\n",
      "Epoch 981/1000\n",
      " - 4s - loss: 0.1353 - acc: 0.9483 - val_loss: 0.4490 - val_acc: 0.8347\n",
      "Epoch 982/1000\n",
      " - 4s - loss: 0.1620 - acc: 0.9339 - val_loss: 0.4719 - val_acc: 0.8595\n",
      "Epoch 983/1000\n",
      " - 4s - loss: 0.1907 - acc: 0.9298 - val_loss: 0.4268 - val_acc: 0.8926\n",
      "Epoch 984/1000\n",
      " - 4s - loss: 0.1721 - acc: 0.9401 - val_loss: 0.6699 - val_acc: 0.7769\n",
      "Epoch 985/1000\n",
      " - 4s - loss: 0.2124 - acc: 0.9318 - val_loss: 0.4340 - val_acc: 0.8678\n",
      "Epoch 986/1000\n",
      " - 4s - loss: 0.1470 - acc: 0.9504 - val_loss: 0.5676 - val_acc: 0.8347\n",
      "Epoch 987/1000\n",
      " - 4s - loss: 0.1412 - acc: 0.9380 - val_loss: 0.4922 - val_acc: 0.8182\n",
      "Epoch 988/1000\n",
      " - 4s - loss: 0.1707 - acc: 0.9380 - val_loss: 0.4456 - val_acc: 0.8595\n",
      "Epoch 989/1000\n",
      " - 4s - loss: 0.1658 - acc: 0.9277 - val_loss: 0.4346 - val_acc: 0.8843\n",
      "Epoch 990/1000\n",
      " - 4s - loss: 0.1703 - acc: 0.9339 - val_loss: 0.8086 - val_acc: 0.7438\n",
      "Epoch 991/1000\n",
      " - 4s - loss: 0.1753 - acc: 0.9504 - val_loss: 1.6372 - val_acc: 0.6364\n",
      "Epoch 992/1000\n",
      " - 4s - loss: 0.1903 - acc: 0.9277 - val_loss: 0.6412 - val_acc: 0.8182\n",
      "Epoch 993/1000\n",
      " - 4s - loss: 0.1691 - acc: 0.9442 - val_loss: 0.4867 - val_acc: 0.8512\n",
      "Epoch 994/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.1847 - acc: 0.9236 - val_loss: 1.1518 - val_acc: 0.6860\n",
      "Epoch 995/1000\n",
      " - 4s - loss: 0.1634 - acc: 0.9421 - val_loss: 0.4862 - val_acc: 0.8512\n",
      "Epoch 996/1000\n",
      " - 4s - loss: 0.1993 - acc: 0.9112 - val_loss: 0.4155 - val_acc: 0.8843\n",
      "Epoch 997/1000\n",
      " - 4s - loss: 0.2106 - acc: 0.9112 - val_loss: 0.8527 - val_acc: 0.7025\n",
      "Epoch 998/1000\n",
      " - 4s - loss: 0.1282 - acc: 0.9525 - val_loss: 1.3437 - val_acc: 0.6446\n",
      "Epoch 999/1000\n",
      " - 4s - loss: 0.1694 - acc: 0.9318 - val_loss: 0.5908 - val_acc: 0.8347\n",
      "Epoch 1000/1000\n",
      " - 4s - loss: 0.1612 - acc: 0.9380 - val_loss: 0.4871 - val_acc: 0.8264\n",
      "CPU times: user 41min 16s, sys: 22min 23s, total: 1h 3min 40s\n",
      "Wall time: 1h 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# defining learning hyperparameters\n",
    "batch_size = 4\n",
    "epochs = 1000\n",
    "learning_rate = 1.e-4\n",
    "width_shift_range=0.2\n",
    "height_shift_range=0.2\n",
    "zoom_range=0.2\n",
    "horizontal_flip=True\n",
    "fill_mode='nearest'\n",
    "\n",
    "sgd = optimizers.SGD(lr=learning_rate)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "reset_weights(model)\n",
    "datagen = ImageDataGenerator(width_shift_range=width_shift_range, \n",
    "                             height_shift_range=height_shift_range, \n",
    "                             zoom_range=zoom_range, \n",
    "                             horizontal_flip=horizontal_flip, \n",
    "                             fill_mode=fill_mode)\n",
    "\n",
    "datagen.fit(images_train)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='bestfit.hdf5',\n",
    "                               save_best_only=True, \n",
    "                               monitor='val_acc')\n",
    "\n",
    "hist = model.fit_generator(datagen.flow(images_train,labels_train,batch_size), \n",
    "                           steps_per_epoch=int(dsize/batch_size), \n",
    "                           epochs=epochs, \n",
    "                           validation_data=(images_dev,labels_dev),\n",
    "                           verbose=2, \n",
    "                           use_multiprocessing=True, \n",
    "                           callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1527814040470,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "KK8wA0hXSpYV",
    "outputId": "2b887b1b-152f-44ac-b780-562ac7459dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 824\n",
      " - loss: 0.1778 - acc: 0.9256 - val_loss: 0.3675 - val_acc: 0.9174\n"
     ]
    }
   ],
   "source": [
    "best_epoch = np.argmax(hist.history['val_acc'])\n",
    "cost_train = hist.history['loss'][best_epoch]\n",
    "cost_dev = hist.history['val_loss'][best_epoch]\n",
    "acc_train = hist.history['acc'][best_epoch]\n",
    "acc_dev = hist.history['val_acc'][best_epoch]\n",
    "print(\"Epoch {}\".format(best_epoch))\n",
    "print(\" - loss: {:6.4f} - acc: {:6.4f} - val_loss: {:6.4f} - val_acc: {:6.4f}\".format(cost_train,acc_train,cost_dev,acc_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1527813443925,
     "user": {
      "displayName": "Hermes Ribeiro Sant' Anna",
      "photoUrl": "//lh4.googleusercontent.com/-koFV274CtUI/AAAAAAAAAAI/AAAAAAAABZQ/cj9uOoWg1lQ/s50-c-k-no/photo.jpg",
      "userId": "107730963675482581712"
     },
     "user_tz": 180
    },
    "id": "VBzfciw7B6uO",
    "outputId": "b37ac8fa-6fad-4d58-b3af-c7baa953c56f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBlQqgIiIAfU/denorw8a/bsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20116f7a20>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_curve, = plt.plot(hist.history['loss'], label = 'Train error')\n",
    "test_curve,  = plt.plot(hist.history['val_loss'], label = 'Dev error')\n",
    "plt.legend(handles=[train_curve,test_curve])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean log loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08LaIRMEPRJW"
   },
   "source": [
    "The results speak for themselves. The accuracy increased dramatically to about 90% (89% - 92% depending on the validation metric you are looking at) and the dev error decreased to about 0.35, a 40% error decrease. \n",
    "\n",
    "The learning curves oscillate heavily due to two facts combined. One is the random nature of data augmentation. Especulatively speaking, every epoch presents a \"different\" dataset to backprop, and the gradient directions change with respect to each augmented dataset. The other factor is the higher learning rate used. We have been using learning rates in the scale of $10^{-6}$ to $10^{-5}$ for the sake of a smoother learning. However, we had to increase it to $10^{-4}$ for the sake of running time. Therefore, this learning rate amplified the erratic nature of the randomly augmented gradients. \n",
    "\n",
    "If you draw a line at the bottom of the dev error curve, it will approximate convex curve. This is tipical to a learning curve, meaning, the model is fitting untill the minimum in the dev error, after that, the model starts to overfit and the dev error starts to increase. This is a good result showing that 1000 epochs were enough to fit the dataset, overfitting around 600-800 epochs. Indeed, the minimum dev error occured on epoch 608, while the maximum dev accuracy occured on epoch 824. Tha\n",
    "\n",
    "All factors - convolution, pooling, dataset augmentation, 8 layers deep model and batch normalization - contributed to this increase in 20% in absolute difference accuracy. This only demonstrates how and why this was the first widely acclaimed deep learning architecture.\n",
    "\n",
    "We end this notebook here for the sake of your patience, but we will continue to explore the latest deep learning methodologies in the next notebook.\n",
    "\n",
    "We hope that, by the end of the next notebook we can already have an idea of where to go in terms of architecture. So later, we will address our second biggest issue, low resolution, using an even larger dataset in terms of samples and of pixel density. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "3 - DeepSnakes - CNN-AlexNet.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
